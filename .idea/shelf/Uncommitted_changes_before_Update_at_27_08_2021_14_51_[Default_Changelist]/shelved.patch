Index: venv/Lib/site-packages/whitenoise-5.3.0.dist-info/top_level.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/whitenoise-5.3.0.dist-info/top_level.txt b/venv/Lib/site-packages/whitenoise-5.3.0.dist-info/top_level.txt
new file mode 100644
--- /dev/null	(date 1630065393876)
+++ b/venv/Lib/site-packages/whitenoise-5.3.0.dist-info/top_level.txt	(date 1630065393876)
@@ -0,0 +1,1 @@
+whitenoise
Index: venv/Lib/site-packages/whitenoise-5.3.0.dist-info/WHEEL
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/whitenoise-5.3.0.dist-info/WHEEL b/venv/Lib/site-packages/whitenoise-5.3.0.dist-info/WHEEL
new file mode 100644
--- /dev/null	(date 1630065393873)
+++ b/venv/Lib/site-packages/whitenoise-5.3.0.dist-info/WHEEL	(date 1630065393873)
@@ -0,0 +1,6 @@
+Wheel-Version: 1.0
+Generator: bdist_wheel (0.36.2)
+Root-Is-Purelib: true
+Tag: py2-none-any
+Tag: py3-none-any
+
Index: venv/Lib/site-packages/whitenoise-5.3.0.dist-info/METADATA
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/whitenoise-5.3.0.dist-info/METADATA b/venv/Lib/site-packages/whitenoise-5.3.0.dist-info/METADATA
new file mode 100644
--- /dev/null	(date 1630065393870)
+++ b/venv/Lib/site-packages/whitenoise-5.3.0.dist-info/METADATA	(date 1630065393870)
@@ -0,0 +1,86 @@
+Metadata-Version: 2.1
+Name: whitenoise
+Version: 5.3.0
+Summary: Radically simplified static file serving for WSGI applications
+Home-page: https://whitenoise.evans.io
+Author: David Evans
+Author-email: d@evans.io
+License: MIT
+Platform: UNKNOWN
+Classifier: Development Status :: 5 - Production/Stable
+Classifier: Topic :: Internet :: WWW/HTTP :: WSGI :: Middleware
+Classifier: Framework :: Django
+Classifier: Framework :: Django :: 1.11
+Classifier: Framework :: Django :: 2.0
+Classifier: Framework :: Django :: 2.1
+Classifier: Framework :: Django :: 2.2
+Classifier: Framework :: Django :: 3.0
+Classifier: Framework :: Django :: 3.1
+Classifier: Framework :: Django :: 3.2
+Classifier: Intended Audience :: Developers
+Classifier: License :: OSI Approved :: MIT License
+Classifier: Operating System :: OS Independent
+Classifier: Programming Language :: Python :: 3
+Classifier: Programming Language :: Python :: 3.5
+Classifier: Programming Language :: Python :: 3.6
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: Implementation :: CPython
+Classifier: Programming Language :: Python :: Implementation :: PyPy
+Requires-Python: >=3.5, <4
+License-File: LICENSE
+Provides-Extra: brotli
+Requires-Dist: Brotli ; extra == 'brotli'
+
+WhiteNoise
+==========
+
+.. image:: https://img.shields.io/travis/evansd/whitenoise.svg
+   :target:  https://travis-ci.org/evansd/whitenoise
+   :alt: Build Status (Linux)
+
+.. image:: https://img.shields.io/appveyor/ci/evansd/whitenoise.svg
+   :target:  https://ci.appveyor.com/project/evansd/whitenoise
+   :alt: Build Status (Windows)
+
+.. image:: https://img.shields.io/pypi/v/whitenoise.svg
+    :target: https://pypi.python.org/pypi/whitenoise
+    :alt: Latest PyPI version
+
+.. image:: https://img.shields.io/pypi/dm/whitenoise.svg
+    :target: https://pypistats.org/packages/whitenoise
+    :alt: Monthly PyPI downloads
+
+.. image:: https://img.shields.io/github/stars/evansd/whitenoise.svg?style=social&label=Star
+    :target: https://github.com/evansd/whitenoise
+    :alt: GitHub project
+
+**Radically simplified static file serving for Python web apps**
+
+With a couple of lines of config WhiteNoise allows your web app to serve its
+own static files, making it a self-contained unit that can be deployed anywhere
+without relying on nginx, Amazon S3 or any other external service. (Especially
+useful on Heroku, OpenShift and other PaaS providers.)
+
+It's designed to work nicely with a CDN for high-traffic sites so you don't have to
+sacrifice performance to benefit from simplicity.
+
+WhiteNoise works with any WSGI-compatible app but has some special auto-configuration
+features for Django.
+
+WhiteNoise takes care of best-practices for you, for instance:
+
+* Serving compressed content (gzip and Brotli formats, handling Accept-Encoding
+  and Vary headers correctly)
+* Setting far-future cache headers on content which won't change
+
+Worried that serving static files with Python is horribly inefficient?
+Still think you should be using Amazon S3? Have a look at the `Infrequently
+Asked Questions`_.
+
+To get started, see the documentation_.
+
+.. _Infrequently Asked Questions: https://whitenoise.evans.io/en/stable/#infrequently-asked-questions
+.. _documentation: https://whitenoise.evans.io/en/stable/
+
+
Index: venv/Lib/site-packages/whitenoise-5.3.0.dist-info/RECORD
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/whitenoise-5.3.0.dist-info/RECORD b/venv/Lib/site-packages/whitenoise-5.3.0.dist-info/RECORD
new file mode 100644
--- /dev/null	(date 1630065394225)
+++ b/venv/Lib/site-packages/whitenoise-5.3.0.dist-info/RECORD	(date 1630065394225)
@@ -0,0 +1,32 @@
+whitenoise-5.3.0.dist-info/INSTALLER,sha256=zuuue4knoyJ-UwPPXg8fezS7VCrXJQrAP7zeNuwvFQg,4
+whitenoise-5.3.0.dist-info/LICENSE,sha256=6_1Gm0-2ta2tpUd0fh6NpyXs8gWV1UrO0EMnXU9KNgA,1078
+whitenoise-5.3.0.dist-info/METADATA,sha256=13Br0vB69hRGPilRgLhONgezMh-talRIoR5aIUFZvcI,3313
+whitenoise-5.3.0.dist-info/RECORD,,
+whitenoise-5.3.0.dist-info/WHEEL,sha256=Z-nyYpwrcSqxfdux5Mbn_DQ525iP7J2DG3JgGvOYyTQ,110
+whitenoise-5.3.0.dist-info/top_level.txt,sha256=B9W4KO2HU7q-2WgAvuqFtjp3cG4z2RZ5JDZB0Wuni6Q,11
+whitenoise/__init__.py,sha256=5q3AAD_1fKn7VY5bl0YRcUevDISpd-zy0i3_cn33q4A,78
+whitenoise/__pycache__/__init__.cpython-39.pyc,,
+whitenoise/__pycache__/base.cpython-39.pyc,,
+whitenoise/__pycache__/compress.cpython-39.pyc,,
+whitenoise/__pycache__/django.cpython-39.pyc,,
+whitenoise/__pycache__/media_types.cpython-39.pyc,,
+whitenoise/__pycache__/middleware.cpython-39.pyc,,
+whitenoise/__pycache__/responders.cpython-39.pyc,,
+whitenoise/__pycache__/storage.cpython-39.pyc,,
+whitenoise/__pycache__/string_utils.cpython-39.pyc,,
+whitenoise/base.py,sha256=zODOgMnH23Bc4M3XY6slSmKXf0P0aLKaSvMWZArNY-k,10390
+whitenoise/compress.py,sha256=5IngsxgsgoGwAbnNVMisa5khgJpMIUhpRbqBl-SL5as,5099
+whitenoise/django.py,sha256=yvILW87Huhs0RCkBL355NFaSMR57EbdqJFPyMnPSyHc,250
+whitenoise/media_types.py,sha256=LHR2Y44Be_FIH-87vu99vaVTHZNcAgN8VngwTaP1vg4,4897
+whitenoise/middleware.py,sha256=SSjnK3u7QQMsQ7ZKlGze0gLFhAMoP48yLk2-PWfwOSY,6460
+whitenoise/responders.py,sha256=j8_FucCaif-NxRipocDJqP1ZFmpweah9KF1B_JxZsv0,8940
+whitenoise/runserver_nostatic/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+whitenoise/runserver_nostatic/__pycache__/__init__.cpython-39.pyc,,
+whitenoise/runserver_nostatic/management/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+whitenoise/runserver_nostatic/management/__pycache__/__init__.cpython-39.pyc,,
+whitenoise/runserver_nostatic/management/commands/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+whitenoise/runserver_nostatic/management/commands/__pycache__/__init__.cpython-39.pyc,,
+whitenoise/runserver_nostatic/management/commands/__pycache__/runserver.cpython-39.pyc,,
+whitenoise/runserver_nostatic/management/commands/runserver.py,sha256=Y3jToYAb0q6sBdLJyFMFpp_58MP8ZLb5JIKIOF9iG1w,1652
+whitenoise/storage.py,sha256=Yn91oQNiHorV7NxHsangB7Ukxr4wCgajBORr9DAc3Gc,7515
+whitenoise/string_utils.py,sha256=Ae0-Tom5JcYfKVtFuFPxtuNQL7iARCzraQSy3v7MsRc,653
Index: venv/Lib/site-packages/whitenoise-5.3.0.dist-info/INSTALLER
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/whitenoise-5.3.0.dist-info/INSTALLER b/venv/Lib/site-packages/whitenoise-5.3.0.dist-info/INSTALLER
new file mode 100644
--- /dev/null	(date 1630065394218)
+++ b/venv/Lib/site-packages/whitenoise-5.3.0.dist-info/INSTALLER	(date 1630065394218)
@@ -0,0 +1,1 @@
+pip
Index: venv/Lib/site-packages/whitenoise-5.3.0.dist-info/LICENSE
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/whitenoise-5.3.0.dist-info/LICENSE b/venv/Lib/site-packages/whitenoise-5.3.0.dist-info/LICENSE
new file mode 100644
--- /dev/null	(date 1630065393866)
+++ b/venv/Lib/site-packages/whitenoise-5.3.0.dist-info/LICENSE	(date 1630065393866)
@@ -0,0 +1,20 @@
+The MIT License (MIT)
+
+Copyright (c) 2013 David Evans
+
+Permission is hereby granted, free of charge, to any person obtaining a copy of
+this software and associated documentation files (the "Software"), to deal in
+the Software without restriction, including without limitation the rights to
+use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of
+the Software, and to permit persons to whom the Software is furnished to do so,
+subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
+FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
+COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
+IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
Index: venv/Lib/site-packages/whitenoise/runserver_nostatic/management/commands/runserver.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/whitenoise/runserver_nostatic/management/commands/runserver.py b/venv/Lib/site-packages/whitenoise/runserver_nostatic/management/commands/runserver.py
new file mode 100644
--- /dev/null	(date 1630065393861)
+++ b/venv/Lib/site-packages/whitenoise/runserver_nostatic/management/commands/runserver.py	(date 1630065393861)
@@ -0,0 +1,51 @@
+"""
+Subclass the existing 'runserver' command and change the default options
+to disable static file serving, allowing WhiteNoise to handle static files.
+
+There is some unpleasant hackery here because we don't know which command class
+to subclass until runtime as it depends on which INSTALLED_APPS we have, so we
+have to determine this dynamically.
+"""
+from importlib import import_module
+
+from django.apps import apps
+
+
+def get_next_runserver_command():
+    """
+    Return the next highest priority "runserver" command class
+    """
+    for app_name in get_lower_priority_apps():
+        module_path = "%s.management.commands.runserver" % app_name
+        try:
+            return import_module(module_path).Command
+        except (ImportError, AttributeError):
+            pass
+
+
+def get_lower_priority_apps():
+    """
+    Yield all app module names below the current app in the INSTALLED_APPS list
+    """
+    self_app_name = ".".join(__name__.split(".")[:-3])
+    reached_self = False
+    for app_config in apps.get_app_configs():
+        if app_config.name == self_app_name:
+            reached_self = True
+        elif reached_self:
+            yield app_config.name
+    yield "django.core"
+
+
+RunserverCommand = get_next_runserver_command()
+
+
+class Command(RunserverCommand):
+    def add_arguments(self, parser):
+        super(Command, self).add_arguments(parser)
+        if parser.get_default("use_static_handler") is True:
+            parser.set_defaults(use_static_handler=False)
+            parser.description += (
+                "\n(Wrapped by 'whitenoise.runserver_nostatic' to always"
+                " enable '--nostatic')"
+            )
Index: venv/Lib/site-packages/gunicorn-20.1.0.dist-info/INSTALLER
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn-20.1.0.dist-info/INSTALLER b/venv/Lib/site-packages/gunicorn-20.1.0.dist-info/INSTALLER
new file mode 100644
--- /dev/null	(date 1630065627658)
+++ b/venv/Lib/site-packages/gunicorn-20.1.0.dist-info/INSTALLER	(date 1630065627658)
@@ -0,0 +1,1 @@
+pip
Index: venv/Lib/site-packages/gunicorn-20.1.0.dist-info/top_level.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn-20.1.0.dist-info/top_level.txt b/venv/Lib/site-packages/gunicorn-20.1.0.dist-info/top_level.txt
new file mode 100644
--- /dev/null	(date 1630065626460)
+++ b/venv/Lib/site-packages/gunicorn-20.1.0.dist-info/top_level.txt	(date 1630065626460)
@@ -0,0 +1,1 @@
+gunicorn
Index: venv/Lib/site-packages/gunicorn-20.1.0.dist-info/WHEEL
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn-20.1.0.dist-info/WHEEL b/venv/Lib/site-packages/gunicorn-20.1.0.dist-info/WHEEL
new file mode 100644
--- /dev/null	(date 1630065626451)
+++ b/venv/Lib/site-packages/gunicorn-20.1.0.dist-info/WHEEL	(date 1630065626451)
@@ -0,0 +1,5 @@
+Wheel-Version: 1.0
+Generator: bdist_wheel (0.36.2)
+Root-Is-Purelib: true
+Tag: py3-none-any
+
Index: venv/Lib/site-packages/gunicorn-20.1.0.dist-info/METADATA
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn-20.1.0.dist-info/METADATA b/venv/Lib/site-packages/gunicorn-20.1.0.dist-info/METADATA
new file mode 100644
--- /dev/null	(date 1630065626448)
+++ b/venv/Lib/site-packages/gunicorn-20.1.0.dist-info/METADATA	(date 1630065626448)
@@ -0,0 +1,120 @@
+Metadata-Version: 2.1
+Name: gunicorn
+Version: 20.1.0
+Summary: WSGI HTTP Server for UNIX
+Home-page: https://gunicorn.org
+Author: Benoit Chesneau
+Author-email: benoitc@e-engura.com
+License: MIT
+Project-URL: Documentation, https://docs.gunicorn.org
+Project-URL: Homepage, https://gunicorn.org
+Project-URL: Issue tracker, https://github.com/benoitc/gunicorn/issues
+Project-URL: Source code, https://github.com/benoitc/gunicorn
+Platform: UNKNOWN
+Classifier: Development Status :: 5 - Production/Stable
+Classifier: Environment :: Other Environment
+Classifier: Intended Audience :: Developers
+Classifier: License :: OSI Approved :: MIT License
+Classifier: Operating System :: MacOS :: MacOS X
+Classifier: Operating System :: POSIX
+Classifier: Programming Language :: Python
+Classifier: Programming Language :: Python :: 3
+Classifier: Programming Language :: Python :: 3.5
+Classifier: Programming Language :: Python :: 3.6
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3 :: Only
+Classifier: Programming Language :: Python :: Implementation :: CPython
+Classifier: Programming Language :: Python :: Implementation :: PyPy
+Classifier: Topic :: Internet
+Classifier: Topic :: Utilities
+Classifier: Topic :: Software Development :: Libraries :: Python Modules
+Classifier: Topic :: Internet :: WWW/HTTP
+Classifier: Topic :: Internet :: WWW/HTTP :: WSGI
+Classifier: Topic :: Internet :: WWW/HTTP :: WSGI :: Server
+Classifier: Topic :: Internet :: WWW/HTTP :: Dynamic Content
+Requires-Python: >=3.5
+Requires-Dist: setuptools (>=3.0)
+Provides-Extra: eventlet
+Requires-Dist: eventlet (>=0.24.1) ; extra == 'eventlet'
+Provides-Extra: gevent
+Requires-Dist: gevent (>=1.4.0) ; extra == 'gevent'
+Provides-Extra: gthread
+Provides-Extra: setproctitle
+Requires-Dist: setproctitle ; extra == 'setproctitle'
+Provides-Extra: tornado
+Requires-Dist: tornado (>=0.2) ; extra == 'tornado'
+
+Gunicorn
+--------
+
+.. image:: https://img.shields.io/pypi/v/gunicorn.svg?style=flat
+    :alt: PyPI version
+    :target: https://pypi.python.org/pypi/gunicorn
+
+.. image:: https://img.shields.io/pypi/pyversions/gunicorn.svg
+    :alt: Supported Python versions
+    :target: https://pypi.python.org/pypi/gunicorn
+
+.. image:: https://travis-ci.org/benoitc/gunicorn.svg?branch=master
+    :alt: Build Status
+    :target: https://travis-ci.org/benoitc/gunicorn
+
+Gunicorn 'Green Unicorn' is a Python WSGI HTTP Server for UNIX. It's a pre-fork
+worker model ported from Ruby's Unicorn_ project. The Gunicorn server is broadly
+compatible with various web frameworks, simply implemented, light on server
+resource usage, and fairly speedy.
+
+Feel free to join us in `#gunicorn`_ on Freenode_.
+
+Documentation
+-------------
+
+The documentation is hosted at https://docs.gunicorn.org.
+
+Installation
+------------
+
+Gunicorn requires **Python 3.x >= 3.5**.
+
+Install from PyPI::
+
+    $ pip install gunicorn
+
+
+Usage
+-----
+
+Basic usage::
+
+    $ gunicorn [OPTIONS] APP_MODULE
+
+Where ``APP_MODULE`` is of the pattern ``$(MODULE_NAME):$(VARIABLE_NAME)``. The
+module name can be a full dotted path. The variable name refers to a WSGI
+callable that should be found in the specified module.
+
+Example with test app::
+
+    $ cd examples
+    $ gunicorn --workers=2 test:app
+
+
+Contributing
+------------
+
+See `our complete contributor's guide <CONTRIBUTING.md>`_ for more details.
+
+
+License
+-------
+
+Gunicorn is released under the MIT License. See the LICENSE_ file for more
+details.
+
+.. _Unicorn: https://bogomips.org/unicorn/
+.. _`#gunicorn`: https://webchat.freenode.net/?channels=gunicorn
+.. _Freenode: https://freenode.net/
+.. _LICENSE: https://github.com/benoitc/gunicorn/blob/master/LICENSE
+
+
Index: venv/Lib/site-packages/gunicorn-20.1.0.dist-info/RECORD
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn-20.1.0.dist-info/RECORD b/venv/Lib/site-packages/gunicorn-20.1.0.dist-info/RECORD
new file mode 100644
--- /dev/null	(date 1630065627817)
+++ b/venv/Lib/site-packages/gunicorn-20.1.0.dist-info/RECORD	(date 1630065627817)
@@ -0,0 +1,77 @@
+../../Scripts/gunicorn.exe,sha256=NKs7cYX9uQE6F_R5LiCCkIZ4R9NJrk7rslcG81rDys4,106355
+gunicorn-20.1.0.dist-info/INSTALLER,sha256=zuuue4knoyJ-UwPPXg8fezS7VCrXJQrAP7zeNuwvFQg,4
+gunicorn-20.1.0.dist-info/LICENSE,sha256=eJ_hG5Lhyr-890S1_MOSyb1cZ5hgOk6J-SW2M3mE0d8,1136
+gunicorn-20.1.0.dist-info/METADATA,sha256=-0kZuLv3CwPyNDUH40lI3VZN4CbFt3YCalVUprINtfs,3771
+gunicorn-20.1.0.dist-info/RECORD,,
+gunicorn-20.1.0.dist-info/REQUESTED,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+gunicorn-20.1.0.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
+gunicorn-20.1.0.dist-info/entry_points.txt,sha256=iKqKVTg4RzByDFxtGUiHcSAFATmYSYMO0f7S5RPLK6o,130
+gunicorn-20.1.0.dist-info/top_level.txt,sha256=cdMaa2yhxb8do-WioY9qRHUCfwf55YztjwQCncaInoE,9
+gunicorn/__init__.py,sha256=cMSZ4dqEfhR1eghYtGfJ9Fw4Xw9RN6U7BzH3tRpT4fQ,279
+gunicorn/__main__.py,sha256=kv-LQeOm8rXRw_NQTj8Tg3l3jv9eKMAm6jyKzYY0Hs8,171
+gunicorn/__pycache__/__init__.cpython-39.pyc,,
+gunicorn/__pycache__/__main__.cpython-39.pyc,,
+gunicorn/__pycache__/arbiter.cpython-39.pyc,,
+gunicorn/__pycache__/config.cpython-39.pyc,,
+gunicorn/__pycache__/debug.cpython-39.pyc,,
+gunicorn/__pycache__/errors.cpython-39.pyc,,
+gunicorn/__pycache__/glogging.cpython-39.pyc,,
+gunicorn/__pycache__/pidfile.cpython-39.pyc,,
+gunicorn/__pycache__/reloader.cpython-39.pyc,,
+gunicorn/__pycache__/sock.cpython-39.pyc,,
+gunicorn/__pycache__/systemd.cpython-39.pyc,,
+gunicorn/__pycache__/util.cpython-39.pyc,,
+gunicorn/app/__init__.py,sha256=GuqstqdkizeV4HRbd8aGMBn0Q8IDOyRU1wMMNqNe5GY,127
+gunicorn/app/__pycache__/__init__.cpython-39.pyc,,
+gunicorn/app/__pycache__/base.cpython-39.pyc,,
+gunicorn/app/__pycache__/pasterapp.cpython-39.pyc,,
+gunicorn/app/__pycache__/wsgiapp.cpython-39.pyc,,
+gunicorn/app/base.py,sha256=wIDHzndfzyTcKySUMJmW_mscgLVj_K9w7UCOsUNcVFo,7150
+gunicorn/app/pasterapp.py,sha256=Bb0JwQNqZxmZ-gvvZUGWAEc9RX2BdhdhfhJ2a12Xafo,2038
+gunicorn/app/wsgiapp.py,sha256=Ktb5z0GPkCpDqQ0zS8zccYCvqJi8Su4zOekwKJulwBA,1926
+gunicorn/arbiter.py,sha256=0U6C550IKETMLzTXe1scCcNfayXPKo0ZB0nJkvRWxVA,20521
+gunicorn/config.py,sha256=IxV1P9X41D2_1tTkuOR093SUKxXf5tbYVUYpfymaygU,61423
+gunicorn/debug.py,sha256=UUw-eteLEm_OQ98D6K3XtDjx4Dya2H35zdiu8z7F7uc,2289
+gunicorn/errors.py,sha256=JlDBjag90gMiRwLHG3xzEJzDOntSl1iM32R277-U6j0,919
+gunicorn/glogging.py,sha256=k_bt1mrTczN0El0rWq9FE1pwi5cTYFJeg9xHBj_d-ZE,14913
+gunicorn/http/__init__.py,sha256=b4TF3x5F0VYOPTOeNYwRGR1EYHBaPMhZRMoNeuD5-n0,277
+gunicorn/http/__pycache__/__init__.cpython-39.pyc,,
+gunicorn/http/__pycache__/body.cpython-39.pyc,,
+gunicorn/http/__pycache__/errors.cpython-39.pyc,,
+gunicorn/http/__pycache__/message.cpython-39.pyc,,
+gunicorn/http/__pycache__/parser.cpython-39.pyc,,
+gunicorn/http/__pycache__/unreader.cpython-39.pyc,,
+gunicorn/http/__pycache__/wsgi.cpython-39.pyc,,
+gunicorn/http/body.py,sha256=X1vbGcTSM3-2UI2ubtavuTS4yOd0fpTyfeFaQZ_x92o,7297
+gunicorn/http/errors.py,sha256=sNjF2lm4m2qyZ9l95_U33FRxPXpxXzjnZyYqWS-hxd4,2850
+gunicorn/http/message.py,sha256=hmSmf8DOHkRNstcYYkhuw0eg065pTDL8BeybtPftTVc,11759
+gunicorn/http/parser.py,sha256=6eNGDUMEURYqzCXsftv3a4hYuD_fBvttZxOJuRbdKNg,1364
+gunicorn/http/unreader.py,sha256=pXVde3fNCUIO2FLOSJ0iNtEEpA0m8GH6_R2Sl-cB-J8,1943
+gunicorn/http/wsgi.py,sha256=25Q6VZlBFpt-Wqmsxwt6FLw7-ckk1dU5XBkeY7i5mmc,12328
+gunicorn/instrument/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+gunicorn/instrument/__pycache__/__init__.cpython-39.pyc,,
+gunicorn/instrument/__pycache__/statsd.cpython-39.pyc,,
+gunicorn/instrument/statsd.py,sha256=-_DKM8T-3CHaq3pyxhdS6UoV70oD21tTXyJ4bVTrVFc,4633
+gunicorn/pidfile.py,sha256=U3TpoE5_05wQxonGS4pV-aLkq8BMSvql142XJnE2olw,2367
+gunicorn/reloader.py,sha256=jDxzT3Mn2NdcKD9Jiex6HNh-XSjAlK-iw7L4R36h-L0,3777
+gunicorn/sock.py,sha256=dIpBDeH2X-pzMB94VRmc-MbMrbE_ZFTAhuasOo7QebM,6110
+gunicorn/systemd.py,sha256=k2qJb6wAEv9Vk-k8zuTr9OyHJW6K2GkqWrSNoR3zTrs,2511
+gunicorn/util.py,sha256=supyIhToKSH4QONMMqTwChHwFHovRSInR92xxURSPQg,18516
+gunicorn/workers/__init__.py,sha256=Gv_JJXKofikyiPbRAUQ0IXIchKxgt0Gu-8y-nYRN9vY,594
+gunicorn/workers/__pycache__/__init__.cpython-39.pyc,,
+gunicorn/workers/__pycache__/base.cpython-39.pyc,,
+gunicorn/workers/__pycache__/base_async.cpython-39.pyc,,
+gunicorn/workers/__pycache__/geventlet.cpython-39.pyc,,
+gunicorn/workers/__pycache__/ggevent.cpython-39.pyc,,
+gunicorn/workers/__pycache__/gthread.cpython-39.pyc,,
+gunicorn/workers/__pycache__/gtornado.cpython-39.pyc,,
+gunicorn/workers/__pycache__/sync.cpython-39.pyc,,
+gunicorn/workers/__pycache__/workertmp.cpython-39.pyc,,
+gunicorn/workers/base.py,sha256=jNF8BnkHhaFNEmvfKrH0DI2-LiOs9UbKFGAPOXoFH30,9103
+gunicorn/workers/base_async.py,sha256=Eyb-zHt6bhaVfsCVygauVGVbw6WrX0KKvk5kIK-2yZ4,5693
+gunicorn/workers/geventlet.py,sha256=DDlj1MGimp-dpovHVOJB0eEvqD45_O0xcFG06v5vnEg,5713
+gunicorn/workers/ggevent.py,sha256=qgrz1Lsfcnjh8pthi6FW8BQrbT5KqLZhGxMjC1fNtEc,5733
+gunicorn/workers/gthread.py,sha256=bteTEQkeEKJMgJbtf3GcP2oCRV8HNGx4_lfzaWblTjE,12194
+gunicorn/workers/gtornado.py,sha256=0d_MoAXbLsy1LnbKc3C2joo0AwQLYnmpCJCQupl-n-Q,5988
+gunicorn/workers/sync.py,sha256=HvyNnCDlAFH3o2Ynm6W_F3IXnYQLBAT1oSn_uQ3LhCA,7327
+gunicorn/workers/workertmp.py,sha256=4sygTmNodn5vZ5qUnSSB0dUwtfetgAxrTrhhYxgEObY,1649
Index: venv/Lib/site-packages/gunicorn-20.1.0.dist-info/entry_points.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn-20.1.0.dist-info/entry_points.txt b/venv/Lib/site-packages/gunicorn-20.1.0.dist-info/entry_points.txt
new file mode 100644
--- /dev/null	(date 1630065626456)
+++ b/venv/Lib/site-packages/gunicorn-20.1.0.dist-info/entry_points.txt	(date 1630065626456)
@@ -0,0 +1,7 @@
+
+    [console_scripts]
+    gunicorn=gunicorn.app.wsgiapp:run
+
+    [paste.server_runner]
+    main=gunicorn.app.pasterapp:serve
+    
\ No newline at end of file
Index: venv/Lib/site-packages/gunicorn-20.1.0.dist-info/LICENSE
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn-20.1.0.dist-info/LICENSE b/venv/Lib/site-packages/gunicorn-20.1.0.dist-info/LICENSE
new file mode 100644
--- /dev/null	(date 1630065626444)
+++ b/venv/Lib/site-packages/gunicorn-20.1.0.dist-info/LICENSE	(date 1630065626444)
@@ -0,0 +1,23 @@
+2009-2018 (c) Benoît Chesneau <benoitc@e-engura.org>
+2009-2015 (c) Paul J. Davis <paul.joseph.davis@gmail.com>
+
+Permission is hereby granted, free of charge, to any person
+obtaining a copy of this software and associated documentation
+files (the "Software"), to deal in the Software without
+restriction, including without limitation the rights to use,
+copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the
+Software is furnished to do so, subject to the following
+conditions:
+
+The above copyright notice and this permission notice shall be
+included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
+OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT
+HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
+WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+OTHER DEALINGS IN THE SOFTWARE.
Index: venv/Lib/site-packages/whitenoise/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/whitenoise/__init__.py b/venv/Lib/site-packages/whitenoise/__init__.py
new file mode 100644
--- /dev/null	(date 1630065393821)
+++ b/venv/Lib/site-packages/whitenoise/__init__.py	(date 1630065393821)
@@ -0,0 +1,5 @@
+from .base import WhiteNoise
+
+__version__ = "5.3.0"
+
+__all__ = ["WhiteNoise"]
Index: venv/Lib/site-packages/whitenoise/storage.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/whitenoise/storage.py b/venv/Lib/site-packages/whitenoise/storage.py
new file mode 100644
--- /dev/null	(date 1630065393844)
+++ b/venv/Lib/site-packages/whitenoise/storage.py	(date 1630065393844)
@@ -0,0 +1,203 @@
+import errno
+import os
+import re
+import textwrap
+
+from django.conf import settings
+from django.contrib.staticfiles.storage import (
+    ManifestStaticFilesStorage,
+    StaticFilesStorage,
+)
+
+from .compress import Compressor
+
+
+class CompressedStaticFilesMixin(object):
+    """
+    Wraps a StaticFilesStorage instance to compress output files
+    """
+
+    def post_process(self, *args, **kwargs):
+        super_post_process = getattr(
+            super(CompressedStaticFilesMixin, self),
+            "post_process",
+            self.fallback_post_process,
+        )
+        files = super_post_process(*args, **kwargs)
+        if not kwargs.get("dry_run"):
+            files = self.post_process_with_compression(files)
+        return files
+
+    # Only used if the class we're wrapping doesn't implement its own
+    # `post_process` method
+    def fallback_post_process(self, paths, dry_run=False, **options):
+        if not dry_run:
+            for path in paths:
+                yield path, None, False
+
+    def create_compressor(self, **kwargs):
+        return Compressor(**kwargs)
+
+    def post_process_with_compression(self, files):
+        extensions = getattr(settings, "WHITENOISE_SKIP_COMPRESS_EXTENSIONS", None)
+        compressor = self.create_compressor(extensions=extensions, quiet=True)
+        for name, hashed_name, processed in files:
+            yield name, hashed_name, processed
+            if isinstance(processed, Exception):
+                continue
+            unique_names = set(filter(None, [name, hashed_name]))
+            for name in unique_names:
+                if compressor.should_compress(name):
+                    path = self.path(name)
+                    prefix_len = len(path) - len(name)
+                    for compressed_path in compressor.compress(path):
+                        compressed_name = compressed_path[prefix_len:]
+                        yield name, compressed_name, True
+
+
+class CompressedStaticFilesStorage(CompressedStaticFilesMixin, StaticFilesStorage):
+    pass
+
+
+class HelpfulExceptionMixin(object):
+    """
+    If a CSS file contains references to images, fonts etc that can't be found
+    then Django's `post_process` blows up with a not particularly helpful
+    ValueError that leads people to think WhiteNoise is broken.
+
+    Here we attempt to intercept such errors and reformat them to be more
+    helpful in revealing the source of the problem.
+    """
+
+    ERROR_MSG_RE = re.compile("^The file '(.+)' could not be found")
+
+    ERROR_MSG = textwrap.dedent(
+        u"""\
+        {orig_message}
+
+        The {ext} file '{filename}' references a file which could not be found:
+          {missing}
+
+        Please check the URL references in this {ext} file, particularly any
+        relative paths which might be pointing to the wrong location.
+        """
+    )
+
+    def post_process(self, *args, **kwargs):
+        files = super(HelpfulExceptionMixin, self).post_process(*args, **kwargs)
+        for name, hashed_name, processed in files:
+            if isinstance(processed, Exception):
+                processed = self.make_helpful_exception(processed, name)
+            yield name, hashed_name, processed
+
+    def make_helpful_exception(self, exception, name):
+        if isinstance(exception, ValueError):
+            message = exception.args[0] if len(exception.args) else ""
+            # Stringly typed exceptions. Yay!
+            match = self.ERROR_MSG_RE.search(message)
+            if match:
+                extension = os.path.splitext(name)[1].lstrip(".").upper()
+                message = self.ERROR_MSG.format(
+                    orig_message=message,
+                    filename=name,
+                    missing=match.group(1),
+                    ext=extension,
+                )
+                exception = MissingFileError(message)
+        return exception
+
+
+class MissingFileError(ValueError):
+    pass
+
+
+class CompressedManifestStaticFilesStorage(
+    HelpfulExceptionMixin, ManifestStaticFilesStorage
+):
+    """
+    Extends ManifestStaticFilesStorage instance to create compressed versions
+    of its output files and, optionally, to delete the non-hashed files (i.e.
+    those without the hash in their name)
+    """
+
+    _new_files = None
+
+    def __init__(self, *args, **kwargs):
+        manifest_strict = getattr(settings, "WHITENOISE_MANIFEST_STRICT", None)
+        if manifest_strict is not None:
+            self.manifest_strict = manifest_strict
+        super().__init__(*args, **kwargs)
+
+    def post_process(self, *args, **kwargs):
+        files = super(CompressedManifestStaticFilesStorage, self).post_process(
+            *args, **kwargs
+        )
+        if not kwargs.get("dry_run"):
+            files = self.post_process_with_compression(files)
+        return files
+
+    def post_process_with_compression(self, files):
+        # Files may get hashed multiple times, we want to keep track of all the
+        # intermediate files generated during the process and which of these
+        # are the final names used for each file. As not every intermediate
+        # file is yielded we have to hook in to the `hashed_name` method to
+        # keep track of them all.
+        hashed_names = {}
+        new_files = set()
+        self.start_tracking_new_files(new_files)
+        for name, hashed_name, processed in files:
+            if hashed_name and not isinstance(processed, Exception):
+                hashed_names[self.clean_name(name)] = hashed_name
+            yield name, hashed_name, processed
+        self.stop_tracking_new_files()
+        original_files = set(hashed_names.keys())
+        hashed_files = set(hashed_names.values())
+        if self.keep_only_hashed_files:
+            files_to_delete = (original_files | new_files) - hashed_files
+            files_to_compress = hashed_files
+        else:
+            files_to_delete = set()
+            files_to_compress = original_files | hashed_files
+        self.delete_files(files_to_delete)
+        for name, compressed_name in self.compress_files(files_to_compress):
+            yield name, compressed_name, True
+
+    def hashed_name(self, *args, **kwargs):
+        name = super(CompressedManifestStaticFilesStorage, self).hashed_name(
+            *args, **kwargs
+        )
+        if self._new_files is not None:
+            self._new_files.add(self.clean_name(name))
+        return name
+
+    def start_tracking_new_files(self, new_files):
+        self._new_files = new_files
+
+    def stop_tracking_new_files(self):
+        self._new_files = None
+
+    @property
+    def keep_only_hashed_files(self):
+        return getattr(settings, "WHITENOISE_KEEP_ONLY_HASHED_FILES", False)
+
+    def delete_files(self, files_to_delete):
+        for name in files_to_delete:
+            try:
+                os.unlink(self.path(name))
+            except OSError as e:
+                if e.errno != errno.ENOENT:
+                    raise
+
+    def create_compressor(self, **kwargs):
+        return Compressor(**kwargs)
+
+    def compress_files(self, names):
+        extensions = getattr(settings, "WHITENOISE_SKIP_COMPRESS_EXTENSIONS", None)
+        compressor = self.create_compressor(extensions=extensions, quiet=True)
+        for name in names:
+            if compressor.should_compress(name):
+                path = self.path(name)
+                prefix_len = len(path) - len(name)
+                for compressed_path in compressor.compress(path):
+                    compressed_name = compressed_path[prefix_len:]
+                    yield name, compressed_name
Index: venv/Lib/site-packages/whitenoise/string_utils.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/whitenoise/string_utils.py b/venv/Lib/site-packages/whitenoise/string_utils.py
new file mode 100644
--- /dev/null	(date 1630065393847)
+++ b/venv/Lib/site-packages/whitenoise/string_utils.py	(date 1630065393847)
@@ -0,0 +1,18 @@
+def decode_if_byte_string(s, force_text=False):
+    if isinstance(s, bytes):
+        s = s.decode("utf-8")
+    if force_text and not isinstance(s, str):
+        s = str(s)
+    return s
+
+
+# Follow Django in treating URLs as UTF-8 encoded (which requires undoing the
+# implicit ISO-8859-1 decoding applied in Python 3). Strictly speaking, URLs
+# should only be ASCII anyway, but UTF-8 can be found in the wild.
+def decode_path_info(path_info):
+    return path_info.encode("iso-8859-1", "replace").decode("utf-8", "replace")
+
+
+def ensure_leading_trailing_slash(path):
+    path = (path or u"").strip(u"/")
+    return u"/{0}/".format(path) if path else u"/"
Index: venv/Lib/site-packages/whitenoise/responders.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/whitenoise/responders.py b/venv/Lib/site-packages/whitenoise/responders.py
new file mode 100644
--- /dev/null	(date 1630065393841)
+++ b/venv/Lib/site-packages/whitenoise/responders.py	(date 1630065393841)
@@ -0,0 +1,246 @@
+from collections import namedtuple
+from email.utils import formatdate, parsedate
+import errno
+
+from http import HTTPStatus
+import os
+import re
+import stat
+from time import mktime
+from urllib.parse import quote
+
+from wsgiref.headers import Headers
+
+
+Response = namedtuple("Response", ("status", "headers", "file"))
+
+NOT_ALLOWED_RESPONSE = Response(
+    status=HTTPStatus.METHOD_NOT_ALLOWED, headers=[("Allow", "GET, HEAD")], file=None
+)
+
+# Headers which should be returned with a 304 Not Modified response as
+# specified here: https://tools.ietf.org/html/rfc7232#section-4.1
+NOT_MODIFIED_HEADERS = (
+    "Cache-Control",
+    "Content-Location",
+    "Date",
+    "ETag",
+    "Expires",
+    "Vary",
+)
+
+
+class StaticFile(object):
+    def __init__(self, path, headers, encodings=None, stat_cache=None):
+        files = self.get_file_stats(path, encodings, stat_cache)
+        headers = self.get_headers(headers, files)
+        self.last_modified = parsedate(headers["Last-Modified"])
+        self.etag = headers["ETag"]
+        self.not_modified_response = self.get_not_modified_response(headers)
+        self.alternatives = self.get_alternatives(headers, files)
+
+    def get_response(self, method, request_headers):
+        if method not in ("GET", "HEAD"):
+            return NOT_ALLOWED_RESPONSE
+        if self.is_not_modified(request_headers):
+            return self.not_modified_response
+        path, headers = self.get_path_and_headers(request_headers)
+        if method != "HEAD":
+            file_handle = open(path, "rb")
+        else:
+            file_handle = None
+        range_header = request_headers.get("HTTP_RANGE")
+        if range_header:
+            try:
+                return self.get_range_response(range_header, headers, file_handle)
+            except ValueError:
+                # If we can't interpret the Range request for any reason then
+                # just ignore it and return the standard response (this
+                # behaviour is allowed by the spec)
+                pass
+        return Response(HTTPStatus.OK, headers, file_handle)
+
+    def get_range_response(self, range_header, base_headers, file_handle):
+        headers = []
+        for item in base_headers:
+            if item[0] == "Content-Length":
+                size = int(item[1])
+            else:
+                headers.append(item)
+        start, end = self.get_byte_range(range_header, size)
+        if start >= end:
+            return self.get_range_not_satisfiable_response(file_handle, size)
+        if file_handle is not None and start != 0:
+            file_handle.seek(start)
+        headers.append(("Content-Range", "bytes {}-{}/{}".format(start, end, size)))
+        headers.append(("Content-Length", str(end - start + 1)))
+        return Response(HTTPStatus.PARTIAL_CONTENT, headers, file_handle)
+
+    def get_byte_range(self, range_header, size):
+        start, end = self.parse_byte_range(range_header)
+        if start < 0:
+            start = max(start + size, 0)
+        if end is None:
+            end = size - 1
+        else:
+            end = min(end, size - 1)
+        return start, end
+
+    @staticmethod
+    def parse_byte_range(range_header):
+        units, _, range_spec = range_header.strip().partition("=")
+        if units != "bytes":
+            raise ValueError()
+        # Only handle a single range spec. Multiple ranges will trigger a
+        # ValueError below which will result in the Range header being ignored
+        start_str, sep, end_str = range_spec.strip().partition("-")
+        if not sep:
+            raise ValueError()
+        if not start_str:
+            start = -int(end_str)
+            end = None
+        else:
+            start = int(start_str)
+            end = int(end_str) if end_str else None
+        return start, end
+
+    @staticmethod
+    def get_range_not_satisfiable_response(file_handle, size):
+        if file_handle is not None:
+            file_handle.close()
+        return Response(
+            HTTPStatus.REQUESTED_RANGE_NOT_SATISFIABLE,
+            [("Content-Range", "bytes */{}".format(size))],
+            None,
+        )
+
+    @staticmethod
+    def get_file_stats(path, encodings, stat_cache):
+        # Primary file has an encoding of None
+        files = {None: FileEntry(path, stat_cache)}
+        if encodings:
+            for encoding, alt_path in encodings.items():
+                try:
+                    files[encoding] = FileEntry(alt_path, stat_cache)
+                except MissingFileError:
+                    continue
+        return files
+
+    def get_headers(self, headers_list, files):
+        headers = Headers(headers_list)
+        main_file = files[None]
+        if len(files) > 1:
+            headers["Vary"] = "Accept-Encoding"
+        if "Last-Modified" not in headers:
+            mtime = main_file.stat.st_mtime
+            # Not all filesystems report mtimes, and sometimes they report an
+            # mtime of 0 which we know is incorrect
+            if mtime:
+                headers["Last-Modified"] = formatdate(mtime, usegmt=True)
+        if "ETag" not in headers:
+            last_modified = parsedate(headers["Last-Modified"])
+            if last_modified:
+                timestamp = int(mktime(last_modified))
+                headers["ETag"] = '"{:x}-{:x}"'.format(
+                    timestamp, main_file.stat.st_size
+                )
+        return headers
+
+    @staticmethod
+    def get_not_modified_response(headers):
+        not_modified_headers = []
+        for key in NOT_MODIFIED_HEADERS:
+            if key in headers:
+                not_modified_headers.append((key, headers[key]))
+        return Response(
+            status=HTTPStatus.NOT_MODIFIED, headers=not_modified_headers, file=None
+        )
+
+    @staticmethod
+    def get_alternatives(base_headers, files):
+        # Sort by size so that the smallest compressed alternative matches first
+        alternatives = []
+        files_by_size = sorted(files.items(), key=lambda i: i[1].stat.st_size)
+        for encoding, file_entry in files_by_size:
+            headers = Headers(base_headers.items())
+            headers["Content-Length"] = str(file_entry.stat.st_size)
+            if encoding:
+                headers["Content-Encoding"] = encoding
+                encoding_re = re.compile(r"\b%s\b" % encoding)
+            else:
+                encoding_re = re.compile("")
+            alternatives.append((encoding_re, file_entry.path, headers.items()))
+        return alternatives
+
+    def is_not_modified(self, request_headers):
+        previous_etag = request_headers.get("HTTP_IF_NONE_MATCH")
+        if previous_etag is not None:
+            return previous_etag == self.etag
+        if self.last_modified is None:
+            return False
+        try:
+            last_requested = request_headers["HTTP_IF_MODIFIED_SINCE"]
+        except KeyError:
+            return False
+        last_requested_ts = parsedate(last_requested)
+        if last_requested_ts is not None:
+            return parsedate(last_requested) >= self.last_modified
+        return False
+
+    def get_path_and_headers(self, request_headers):
+        accept_encoding = request_headers.get("HTTP_ACCEPT_ENCODING", "")
+        # These are sorted by size so first match is the best
+        for encoding_re, path, headers in self.alternatives:
+            if encoding_re.search(accept_encoding):
+                return path, headers
+
+
+class Redirect(object):
+    def __init__(self, location, headers=None):
+        headers = list(headers.items()) if headers else []
+        headers.append(("Location", quote(location.encode("utf8"))))
+        self.response = Response(HTTPStatus.FOUND, headers, None)
+
+    def get_response(self, method, request_headers):
+        return self.response
+
+
+class NotARegularFileError(Exception):
+    pass
+
+
+class MissingFileError(NotARegularFileError):
+    pass
+
+
+class IsDirectoryError(MissingFileError):
+    pass
+
+
+class FileEntry(object):
+    def __init__(self, path, stat_cache=None):
+        stat_function = os.stat if stat_cache is None else stat_cache.__getitem__
+        self.stat = self.stat_regular_file(path, stat_function)
+        self.path = path
+
+    @staticmethod
+    def stat_regular_file(path, stat_function):
+        """
+        Wrap `stat_function` to raise appropriate errors if `path` is not a
+        regular file
+        """
+        try:
+            stat_result = stat_function(path)
+        except KeyError:
+            raise MissingFileError(path)
+        except OSError as e:
+            if e.errno in (errno.ENOENT, errno.ENAMETOOLONG):
+                raise MissingFileError(path)
+            else:
+                raise
+        if not stat.S_ISREG(stat_result.st_mode):
+            if stat.S_ISDIR(stat_result.st_mode):
+                raise IsDirectoryError(u"Path is a directory: {0}".format(path))
+            else:
+                raise NotARegularFileError(u"Not a regular file: {0}".format(path))
+        return stat_result
Index: venv/Lib/site-packages/whitenoise/media_types.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/whitenoise/media_types.py b/venv/Lib/site-packages/whitenoise/media_types.py
new file mode 100644
--- /dev/null	(date 1630065393834)
+++ b/venv/Lib/site-packages/whitenoise/media_types.py	(date 1630065393834)
@@ -0,0 +1,130 @@
+import os
+
+
+class MediaTypes(object):
+    def __init__(self, default="application/octet-stream", extra_types=None):
+        self.types_map = default_types()
+        self.default = default
+        if extra_types:
+            self.types_map.update(extra_types)
+
+    def get_type(self, path):
+        name = os.path.basename(path).lower()
+        media_type = self.types_map.get(name)
+        if media_type is not None:
+            return media_type
+        extension = os.path.splitext(name)[1]
+        return self.types_map.get(extension, self.default)
+
+
+def default_types():
+    """
+    We use our own set of default media types rather than the system-supplied
+    ones. This ensures consistent media type behaviour across varied
+    environments.  The defaults are based on those shipped with nginx, with
+    some custom additions.
+    """
+
+    return {
+        ".3gp": "video/3gpp",
+        ".3gpp": "video/3gpp",
+        ".7z": "application/x-7z-compressed",
+        ".ai": "application/postscript",
+        ".asf": "video/x-ms-asf",
+        ".asx": "video/x-ms-asf",
+        ".atom": "application/atom+xml",
+        ".avi": "video/x-msvideo",
+        ".bmp": "image/x-ms-bmp",
+        ".cco": "application/x-cocoa",
+        ".crt": "application/x-x509-ca-cert",
+        ".css": "text/css",
+        ".der": "application/x-x509-ca-cert",
+        ".doc": "application/msword",
+        ".docx": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
+        ".ear": "application/java-archive",
+        ".eot": "application/vnd.ms-fontobject",
+        ".eps": "application/postscript",
+        ".flv": "video/x-flv",
+        ".gif": "image/gif",
+        ".hqx": "application/mac-binhex40",
+        ".htc": "text/x-component",
+        ".htm": "text/html",
+        ".html": "text/html",
+        ".ico": "image/x-icon",
+        ".jad": "text/vnd.sun.j2me.app-descriptor",
+        ".jar": "application/java-archive",
+        ".jardiff": "application/x-java-archive-diff",
+        ".jng": "image/x-jng",
+        ".jnlp": "application/x-java-jnlp-file",
+        ".jpeg": "image/jpeg",
+        ".jpg": "image/jpeg",
+        ".js": "text/javascript",
+        ".json": "application/json",
+        ".kar": "audio/midi",
+        ".kml": "application/vnd.google-earth.kml+xml",
+        ".kmz": "application/vnd.google-earth.kmz",
+        ".m3u8": "application/vnd.apple.mpegurl",
+        ".m4a": "audio/x-m4a",
+        ".m4v": "video/x-m4v",
+        ".md": "text/markdown",
+        ".mid": "audio/midi",
+        ".midi": "audio/midi",
+        ".mjs": "text/javascript",
+        ".mml": "text/mathml",
+        ".mng": "video/x-mng",
+        ".mov": "video/quicktime",
+        ".mp3": "audio/mpeg",
+        ".mp4": "video/mp4",
+        ".mpeg": "video/mpeg",
+        ".mpg": "video/mpeg",
+        ".ogg": "audio/ogg",
+        ".pdb": "application/x-pilot",
+        ".pdf": "application/pdf",
+        ".pem": "application/x-x509-ca-cert",
+        ".pl": "application/x-perl",
+        ".pm": "application/x-perl",
+        ".png": "image/png",
+        ".ppt": "application/vnd.ms-powerpoint",
+        ".pptx": "application/vnd.openxmlformats-officedocument.presentationml.presentation",
+        ".prc": "application/x-pilot",
+        ".ps": "application/postscript",
+        ".ra": "audio/x-realaudio",
+        ".rar": "application/x-rar-compressed",
+        ".rpm": "application/x-redhat-package-manager",
+        ".rss": "application/rss+xml",
+        ".rtf": "application/rtf",
+        ".run": "application/x-makeself",
+        ".sea": "application/x-sea",
+        ".shtml": "text/html",
+        ".sit": "application/x-stuffit",
+        ".svg": "image/svg+xml",
+        ".svgz": "image/svg+xml",
+        ".swf": "application/x-shockwave-flash",
+        ".tcl": "application/x-tcl",
+        ".tif": "image/tiff",
+        ".tiff": "image/tiff",
+        ".tk": "application/x-tcl",
+        ".ts": "video/mp2t",
+        ".txt": "text/plain",
+        ".wasm": "application/wasm",
+        ".war": "application/java-archive",
+        ".wbmp": "image/vnd.wap.wbmp",
+        ".webm": "video/webm",
+        ".webp": "image/webp",
+        ".wml": "text/vnd.wap.wml",
+        ".wmlc": "application/vnd.wap.wmlc",
+        ".wmv": "video/x-ms-wmv",
+        ".woff": "application/font-woff",
+        ".woff2": "font/woff2",
+        ".xhtml": "application/xhtml+xml",
+        ".xls": "application/vnd.ms-excel",
+        ".xlsx": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
+        ".xml": "text/xml",
+        ".xpi": "application/x-xpinstall",
+        ".xspf": "application/xspf+xml",
+        ".zip": "application/zip",
+        "apple-app-site-association": "application/pkc7-mime",
+        # Adobe Products - see:
+        # https://www.adobe.com/devnet-docs/acrobatetk/tools/AppSec/xdomain.html#policy-file-host-basics
+        "crossdomain.xml": "text/x-cross-domain-policy",
+    }
Index: venv/Lib/site-packages/whitenoise/middleware.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/whitenoise/middleware.py b/venv/Lib/site-packages/whitenoise/middleware.py
new file mode 100644
--- /dev/null	(date 1630065393837)
+++ b/venv/Lib/site-packages/whitenoise/middleware.py	(date 1630065393837)
@@ -0,0 +1,170 @@
+import os
+from posixpath import basename
+from urllib.parse import urlparse
+
+from django.conf import settings
+from django.contrib.staticfiles.storage import staticfiles_storage
+from django.contrib.staticfiles import finders
+from django.http import FileResponse
+from django.urls import get_script_prefix
+
+from .base import WhiteNoise
+from .string_utils import decode_if_byte_string, ensure_leading_trailing_slash
+
+
+__all__ = ["WhiteNoiseMiddleware"]
+
+
+class WhiteNoiseFileResponse(FileResponse):
+    """
+    Wrap Django's FileResponse to prevent setting any default headers. For the
+    most part these just duplicate work already done by WhiteNoise but in some
+    cases (e.g. the content-disposition header introduced in Django 3.0) they
+    are actively harmful.
+    """
+
+    def set_headers(self, *args, **kwargs):
+        pass
+
+
+class WhiteNoiseMiddleware(WhiteNoise):
+    """
+    Wrap WhiteNoise to allow it to function as Django middleware, rather
+    than WSGI middleware
+
+    This functions as both old- and new-style middleware, so can be included in
+    either MIDDLEWARE or MIDDLEWARE_CLASSES.
+    """
+
+    config_attrs = WhiteNoise.config_attrs + ("root", "use_finders", "static_prefix")
+    root = None
+    use_finders = False
+    static_prefix = None
+
+    def __init__(self, get_response=None, settings=settings):
+        self.get_response = get_response
+        self.configure_from_settings(settings)
+        # Pass None for `application`
+        super(WhiteNoiseMiddleware, self).__init__(None)
+        if self.static_root:
+            self.add_files(self.static_root, prefix=self.static_prefix)
+        if self.root:
+            self.add_files(self.root)
+        if self.use_finders and not self.autorefresh:
+            self.add_files_from_finders()
+
+    def __call__(self, request):
+        response = self.process_request(request)
+        if response is None:
+            response = self.get_response(request)
+        return response
+
+    def process_request(self, request):
+        if self.autorefresh:
+            static_file = self.find_file(request.path_info)
+        else:
+            static_file = self.files.get(request.path_info)
+        if static_file is not None:
+            return self.serve(static_file, request)
+
+    @staticmethod
+    def serve(static_file, request):
+        response = static_file.get_response(request.method, request.META)
+        status = int(response.status)
+        http_response = WhiteNoiseFileResponse(response.file or (), status=status)
+        # Remove default content-type
+        del http_response["content-type"]
+        for key, value in response.headers:
+            http_response[key] = value
+        return http_response
+
+    def configure_from_settings(self, settings):
+        # Default configuration
+        self.autorefresh = settings.DEBUG
+        self.use_finders = settings.DEBUG
+        self.static_prefix = urlparse(settings.STATIC_URL or "").path
+        script_prefix = get_script_prefix().rstrip("/")
+        if script_prefix:
+            if self.static_prefix.startswith(script_prefix):
+                self.static_prefix = self.static_prefix[len(script_prefix) :]
+        if settings.DEBUG:
+            self.max_age = 0
+        # Allow settings to override default attributes
+        for attr in self.config_attrs:
+            settings_key = "WHITENOISE_{0}".format(attr.upper())
+            try:
+                value = getattr(settings, settings_key)
+            except AttributeError:
+                pass
+            else:
+                value = decode_if_byte_string(value)
+                setattr(self, attr, value)
+        self.static_prefix = ensure_leading_trailing_slash(self.static_prefix)
+        self.static_root = decode_if_byte_string(settings.STATIC_ROOT)
+
+    def add_files_from_finders(self):
+        files = {}
+        for finder in finders.get_finders():
+            for path, storage in finder.list(None):
+                prefix = (getattr(storage, "prefix", None) or "").strip("/")
+                url = u"".join(
+                    (
+                        self.static_prefix,
+                        prefix,
+                        "/" if prefix else "",
+                        path.replace("\\", "/"),
+                    )
+                )
+                # Use setdefault as only first matching file should be used
+                files.setdefault(url, storage.path(path))
+        stat_cache = {path: os.stat(path) for path in files.values()}
+        for url, path in files.items():
+            self.add_file_to_dictionary(url, path, stat_cache=stat_cache)
+
+    def candidate_paths_for_url(self, url):
+        if self.use_finders and url.startswith(self.static_prefix):
+            path = finders.find(url[len(self.static_prefix) :])
+            if path:
+                yield path
+        paths = super(WhiteNoiseMiddleware, self).candidate_paths_for_url(url)
+        for path in paths:
+            yield path
+
+    def immutable_file_test(self, path, url):
+        """
+        Determine whether given URL represents an immutable file (i.e. a
+        file with a hash of its contents as part of its name) which can
+        therefore be cached forever
+        """
+        if not url.startswith(self.static_prefix):
+            return False
+        name = url[len(self.static_prefix) :]
+        name_without_hash = self.get_name_without_hash(name)
+        if name == name_without_hash:
+            return False
+        static_url = self.get_static_url(name_without_hash)
+        # If the static_url function maps the name without hash
+        # back to the original name, then we know we've got a
+        # versioned filename
+        if static_url and basename(static_url) == basename(url):
+            return True
+        return False
+
+    def get_name_without_hash(self, filename):
+        """
+        Removes the version hash from a filename e.g, transforms
+        'css/application.f3ea4bcc2.css' into 'css/application.css'
+
+        Note: this is specific to the naming scheme used by Django's
+        CachedStaticFilesStorage. You may have to override this if
+        you are using a different static files versioning system
+        """
+        name_with_hash, ext = os.path.splitext(filename)
+        name = os.path.splitext(name_with_hash)[0]
+        return name + ext
+
+    def get_static_url(self, name):
+        try:
+            return decode_if_byte_string(staticfiles_storage.url(name))
+        except ValueError:
+            return None
Index: venv/Lib/site-packages/whitenoise/compress.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/whitenoise/compress.py b/venv/Lib/site-packages/whitenoise/compress.py
new file mode 100644
--- /dev/null	(date 1630065393828)
+++ b/venv/Lib/site-packages/whitenoise/compress.py	(date 1630065393828)
@@ -0,0 +1,171 @@
+import gzip
+import os
+import re
+
+from io import BytesIO
+
+try:
+    import brotli
+
+    brotli_installed = True
+except ImportError:
+    brotli_installed = False
+
+
+class Compressor(object):
+
+    # Extensions that it's not worth trying to compress
+    SKIP_COMPRESS_EXTENSIONS = (
+        # Images
+        "jpg",
+        "jpeg",
+        "png",
+        "gif",
+        "webp",
+        # Compressed files
+        "zip",
+        "gz",
+        "tgz",
+        "bz2",
+        "tbz",
+        "xz",
+        "br",
+        # Flash
+        "swf",
+        "flv",
+        # Fonts
+        "woff",
+        "woff2",
+    )
+
+    def __init__(
+        self, extensions=None, use_gzip=True, use_brotli=True, log=print, quiet=False
+    ):
+        if extensions is None:
+            extensions = self.SKIP_COMPRESS_EXTENSIONS
+        self.extension_re = self.get_extension_re(extensions)
+        self.use_gzip = use_gzip
+        self.use_brotli = use_brotli and brotli_installed
+        if not quiet:
+            self.log = log
+
+    @staticmethod
+    def get_extension_re(extensions):
+        if not extensions:
+            return re.compile("^$")
+        else:
+            return re.compile(
+                r"\.({0})$".format("|".join(map(re.escape, extensions))), re.IGNORECASE
+            )
+
+    def should_compress(self, filename):
+        return not self.extension_re.search(filename)
+
+    def log(self, message):
+        pass
+
+    def compress(self, path):
+        with open(path, "rb") as f:
+            stat_result = os.fstat(f.fileno())
+            data = f.read()
+        size = len(data)
+        if self.use_brotli:
+            compressed = self.compress_brotli(data)
+            if self.is_compressed_effectively("Brotli", path, size, compressed):
+                yield self.write_data(path, compressed, ".br", stat_result)
+            else:
+                # If Brotli compression wasn't effective gzip won't be either
+                return
+        if self.use_gzip:
+            compressed = self.compress_gzip(data)
+            if self.is_compressed_effectively("Gzip", path, size, compressed):
+                yield self.write_data(path, compressed, ".gz", stat_result)
+
+    @staticmethod
+    def compress_gzip(data):
+        output = BytesIO()
+        # Explicitly set mtime to 0 so gzip content is fully determined
+        # by file content (0 = "no timestamp" according to gzip spec)
+        with gzip.GzipFile(
+            filename="", mode="wb", fileobj=output, compresslevel=9, mtime=0
+        ) as gz_file:
+            gz_file.write(data)
+        return output.getvalue()
+
+    @staticmethod
+    def compress_brotli(data):
+        return brotli.compress(data)
+
+    def is_compressed_effectively(self, encoding_name, path, orig_size, data):
+        compressed_size = len(data)
+        if orig_size == 0:
+            is_effective = False
+        else:
+            ratio = compressed_size / orig_size
+            is_effective = ratio <= 0.95
+        if is_effective:
+            self.log(
+                "{0} compressed {1} ({2}K -> {3}K)".format(
+                    encoding_name, path, orig_size // 1024, compressed_size // 1024
+                )
+            )
+        else:
+            self.log(
+                "Skipping {0} ({1} compression not effective)".format(
+                    path, encoding_name
+                )
+            )
+        return is_effective
+
+    def write_data(self, path, data, suffix, stat_result):
+        filename = path + suffix
+        with open(filename, "wb") as f:
+            f.write(data)
+        os.utime(filename, (stat_result.st_atime, stat_result.st_mtime))
+        return filename
+
+
+def main(root, **kwargs):
+    compressor = Compressor(**kwargs)
+    for dirpath, dirs, files in os.walk(root):
+        for filename in files:
+            if compressor.should_compress(filename):
+                path = os.path.join(dirpath, filename)
+                for compressed in compressor.compress(path):
+                    pass
+
+
+if __name__ == "__main__":
+    import argparse
+
+    parser = argparse.ArgumentParser(
+        description="Search for all files inside <root> *not* matching "
+        "<extensions> and produce compressed versions with "
+        "'.gz' and '.br' suffixes (as long as this results in a "
+        "smaller file)"
+    )
+    parser.add_argument(
+        "-q", "--quiet", help="Don't produce log output", action="store_true"
+    )
+    parser.add_argument(
+        "--no-gzip",
+        help="Don't produce gzip '.gz' files",
+        action="store_false",
+        dest="use_gzip",
+    )
+    parser.add_argument(
+        "--no-brotli",
+        help="Don't produce brotli '.br' files",
+        action="store_false",
+        dest="use_brotli",
+    )
+    parser.add_argument("root", help="Path root from which to search for files")
+    parser.add_argument(
+        "extensions",
+        nargs="*",
+        help="File extensions to exclude from compression "
+        "(default: {})".format(", ".join(Compressor.SKIP_COMPRESS_EXTENSIONS)),
+        default=Compressor.SKIP_COMPRESS_EXTENSIONS,
+    )
+    args = parser.parse_args()
+    main(**vars(args))
Index: venv/Lib/site-packages/whitenoise/django.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/whitenoise/django.py b/venv/Lib/site-packages/whitenoise/django.py
new file mode 100644
--- /dev/null	(date 1630065393831)
+++ b/venv/Lib/site-packages/whitenoise/django.py	(date 1630065393831)
@@ -0,0 +1,7 @@
+raise ImportError(
+    "\n\n"
+    "Your WhiteNoise configuration is incompatible with WhiteNoise v4.0\n"
+    "This can be fixed by following the upgrade instructions at:\n"
+    "https://whitenoise.evans.io/en/stable/changelog.html#v4-0\n"
+    "\n"
+)
Index: venv/Lib/site-packages/django_heroku-0.3.1.dist-info/WHEEL
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/django_heroku-0.3.1.dist-info/WHEEL b/venv/Lib/site-packages/django_heroku-0.3.1.dist-info/WHEEL
new file mode 100644
--- /dev/null	(date 1630065394639)
+++ b/venv/Lib/site-packages/django_heroku-0.3.1.dist-info/WHEEL	(date 1630065394639)
@@ -0,0 +1,6 @@
+Wheel-Version: 1.0
+Generator: bdist_wheel (0.30.0)
+Root-Is-Purelib: true
+Tag: py2-none-any
+Tag: py3-none-any
+
Index: venv/Lib/site-packages/whitenoise/base.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/whitenoise/base.py b/venv/Lib/site-packages/whitenoise/base.py
new file mode 100644
--- /dev/null	(date 1630065393824)
+++ b/venv/Lib/site-packages/whitenoise/base.py	(date 1630065393824)
@@ -0,0 +1,273 @@
+import os
+from posixpath import normpath
+import re
+import warnings
+from wsgiref.headers import Headers
+from wsgiref.util import FileWrapper
+
+from .media_types import MediaTypes
+from .responders import StaticFile, MissingFileError, IsDirectoryError, Redirect
+from .string_utils import (
+    decode_if_byte_string,
+    decode_path_info,
+    ensure_leading_trailing_slash,
+)
+
+
+class WhiteNoise(object):
+
+    # Ten years is what nginx sets a max age if you use 'expires max;'
+    # so we'll follow its lead
+    FOREVER = 10 * 365 * 24 * 60 * 60
+
+    # Attributes that can be set by keyword args in the constructor
+    config_attrs = (
+        "autorefresh",
+        "max_age",
+        "allow_all_origins",
+        "charset",
+        "mimetypes",
+        "add_headers_function",
+        "index_file",
+        "immutable_file_test",
+    )
+    # Re-check the filesystem on every request so that any changes are
+    # automatically picked up. NOTE: For use in development only, not supported
+    # in production
+    autorefresh = False
+    max_age = 60
+    # Set 'Access-Control-Allow-Orign: *' header on all files.
+    # As these are all public static files this is safe (See
+    # https://www.w3.org/TR/cors/#security) and ensures that things (e.g
+    # webfonts in Firefox) still work as expected when your static files are
+    # served from a CDN, rather than your primary domain.
+    allow_all_origins = True
+    charset = "utf-8"
+    # Custom mime types
+    mimetypes = None
+    # Callback for adding custom logic when setting headers
+    add_headers_function = None
+    # Name of index file (None to disable index support)
+    index_file = None
+
+    def __init__(self, application, root=None, prefix=None, **kwargs):
+        for attr in self.config_attrs:
+            try:
+                value = kwargs.pop(attr)
+            except KeyError:
+                pass
+            else:
+                value = decode_if_byte_string(value)
+                setattr(self, attr, value)
+        if kwargs:
+            raise TypeError(
+                "Unexpected keyword argument '{0}'".format(list(kwargs.keys())[0])
+            )
+        self.media_types = MediaTypes(extra_types=self.mimetypes)
+        self.application = application
+        self.files = {}
+        self.directories = []
+        if self.index_file is True:
+            self.index_file = "index.html"
+        if not callable(self.immutable_file_test):
+            regex = re.compile(self.immutable_file_test)
+            self.immutable_file_test = lambda path, url: bool(regex.search(url))
+        if root is not None:
+            self.add_files(root, prefix)
+
+    def __call__(self, environ, start_response):
+        path = decode_path_info(environ.get("PATH_INFO", ""))
+        if self.autorefresh:
+            static_file = self.find_file(path)
+        else:
+            static_file = self.files.get(path)
+        if static_file is None:
+            return self.application(environ, start_response)
+        else:
+            return self.serve(static_file, environ, start_response)
+
+    @staticmethod
+    def serve(static_file, environ, start_response):
+        response = static_file.get_response(environ["REQUEST_METHOD"], environ)
+        status_line = "{} {}".format(response.status, response.status.phrase)
+        start_response(status_line, list(response.headers))
+        if response.file is not None:
+            file_wrapper = environ.get("wsgi.file_wrapper", FileWrapper)
+            return file_wrapper(response.file)
+        else:
+            return []
+
+    def add_files(self, root, prefix=None):
+        root = decode_if_byte_string(root, force_text=True)
+        root = os.path.abspath(root)
+        root = root.rstrip(os.path.sep) + os.path.sep
+        prefix = decode_if_byte_string(prefix)
+        prefix = ensure_leading_trailing_slash(prefix)
+        if self.autorefresh:
+            # Later calls to `add_files` overwrite earlier ones, hence we need
+            # to store the list of directories in reverse order so later ones
+            # match first when they're checked in "autorefresh" mode
+            self.directories.insert(0, (root, prefix))
+        else:
+            if os.path.isdir(root):
+                self.update_files_dictionary(root, prefix)
+            else:
+                warnings.warn(u"No directory at: {}".format(root))
+
+    def update_files_dictionary(self, root, prefix):
+        # Build a mapping from paths to the results of `os.stat` calls
+        # so we only have to touch the filesystem once
+        stat_cache = dict(scantree(root))
+        for path in stat_cache.keys():
+            relative_path = path[len(root) :]
+            relative_url = relative_path.replace("\\", "/")
+            url = prefix + relative_url
+            self.add_file_to_dictionary(url, path, stat_cache=stat_cache)
+
+    def add_file_to_dictionary(self, url, path, stat_cache=None):
+        if self.is_compressed_variant(path, stat_cache=stat_cache):
+            return
+        if self.index_file and url.endswith("/" + self.index_file):
+            index_url = url[: -len(self.index_file)]
+            index_no_slash = index_url.rstrip("/")
+            self.files[url] = self.redirect(url, index_url)
+            self.files[index_no_slash] = self.redirect(index_no_slash, index_url)
+            url = index_url
+        static_file = self.get_static_file(path, url, stat_cache=stat_cache)
+        self.files[url] = static_file
+
+    def find_file(self, url):
+        # Optimization: bail early if the URL can never match a file
+        if not self.index_file and url.endswith("/"):
+            return
+        if not self.url_is_canonical(url):
+            return
+        for path in self.candidate_paths_for_url(url):
+            try:
+                return self.find_file_at_path(path, url)
+            except MissingFileError:
+                pass
+
+    def candidate_paths_for_url(self, url):
+        for root, prefix in self.directories:
+            if url.startswith(prefix):
+                path = os.path.join(root, url[len(prefix) :])
+                if os.path.commonprefix((root, path)) == root:
+                    yield path
+
+    def find_file_at_path(self, path, url):
+        if self.is_compressed_variant(path):
+            raise MissingFileError(path)
+        if self.index_file:
+            return self.find_file_at_path_with_indexes(path, url)
+        else:
+            return self.get_static_file(path, url)
+
+    def find_file_at_path_with_indexes(self, path, url):
+        if url.endswith("/"):
+            path = os.path.join(path, self.index_file)
+            return self.get_static_file(path, url)
+        elif url.endswith("/" + self.index_file):
+            if os.path.isfile(path):
+                return self.redirect(url, url[: -len(self.index_file)])
+        else:
+            try:
+                return self.get_static_file(path, url)
+            except IsDirectoryError:
+                if os.path.isfile(os.path.join(path, self.index_file)):
+                    return self.redirect(url, url + "/")
+        raise MissingFileError(path)
+
+    @staticmethod
+    def url_is_canonical(url):
+        """
+        Check that the URL path is in canonical format i.e. has normalised
+        slashes and no path traversal elements
+        """
+        if "\\" in url:
+            return False
+        normalised = normpath(url)
+        if url.endswith("/") and url != "/":
+            normalised += "/"
+        return normalised == url
+
+    @staticmethod
+    def is_compressed_variant(path, stat_cache=None):
+        if path[-3:] in (".gz", ".br"):
+            uncompressed_path = path[:-3]
+            if stat_cache is None:
+                return os.path.isfile(uncompressed_path)
+            else:
+                return uncompressed_path in stat_cache
+        return False
+
+    def get_static_file(self, path, url, stat_cache=None):
+        # Optimization: bail early if file does not exist
+        if stat_cache is None and not os.path.exists(path):
+            raise MissingFileError(path)
+        headers = Headers([])
+        self.add_mime_headers(headers, path, url)
+        self.add_cache_headers(headers, path, url)
+        if self.allow_all_origins:
+            headers["Access-Control-Allow-Origin"] = "*"
+        if self.add_headers_function:
+            self.add_headers_function(headers, path, url)
+        return StaticFile(
+            path,
+            headers.items(),
+            stat_cache=stat_cache,
+            encodings={"gzip": path + ".gz", "br": path + ".br"},
+        )
+
+    def add_mime_headers(self, headers, path, url):
+        media_type = self.media_types.get_type(path)
+        if media_type.startswith("text/"):
+            params = {"charset": str(self.charset)}
+        else:
+            params = {}
+        headers.add_header("Content-Type", str(media_type), **params)
+
+    def add_cache_headers(self, headers, path, url):
+        if self.immutable_file_test(path, url):
+            headers["Cache-Control"] = "max-age={0}, public, immutable".format(
+                self.FOREVER
+            )
+        elif self.max_age is not None:
+            headers["Cache-Control"] = "max-age={0}, public".format(self.max_age)
+
+    def immutable_file_test(self, path, url):
+        """
+        This should be implemented by sub-classes (see e.g. WhiteNoiseMiddleware)
+        or by setting the `immutable_file_test` config option
+        """
+        return False
+
+    def redirect(self, from_url, to_url):
+        """
+        Return a relative 302 redirect
+
+        We use relative redirects as we don't know the absolute URL the app is
+        being hosted under
+        """
+        if to_url == from_url + "/":
+            relative_url = from_url.split("/")[-1] + "/"
+        elif from_url == to_url + self.index_file:
+            relative_url = "./"
+        else:
+            raise ValueError("Cannot handle redirect: {} > {}".format(from_url, to_url))
+        if self.max_age is not None:
+            headers = {"Cache-Control": "max-age={0}, public".format(self.max_age)}
+        else:
+            headers = {}
+        return Redirect(relative_url, headers=headers)
+
+
+def scantree(root):
+    """
+    Recurse the given directory yielding (pathname, os.stat(pathname)) pairs
+    """
+    for entry in os.scandir(root):
+        if entry.is_dir():
+            yield from scantree(entry.path)
+        else:
+            yield entry.path, entry.stat()
Index: venv/Lib/site-packages/django_heroku-0.3.1.dist-info/top_level.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/django_heroku-0.3.1.dist-info/top_level.txt b/venv/Lib/site-packages/django_heroku-0.3.1.dist-info/top_level.txt
new file mode 100644
--- /dev/null	(date 1630065394637)
+++ b/venv/Lib/site-packages/django_heroku-0.3.1.dist-info/top_level.txt	(date 1630065394637)
@@ -0,0 +1,1 @@
+django_heroku
Index: venv/Lib/site-packages/django_heroku-0.3.1.dist-info/metadata.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/django_heroku-0.3.1.dist-info/metadata.json b/venv/Lib/site-packages/django_heroku-0.3.1.dist-info/metadata.json
new file mode 100644
--- /dev/null	(date 1630065394634)
+++ b/venv/Lib/site-packages/django_heroku-0.3.1.dist-info/metadata.json	(date 1630065394634)
@@ -0,0 +1,1 @@
+{"classifiers": ["License :: OSI Approved :: MIT License", "Programming Language :: Python", "Programming Language :: Python :: 2.6", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: Implementation :: CPython", "Programming Language :: Python :: Implementation :: PyPy"], "description_content_type": "UNKNOWN", "extensions": {"python.details": {"contacts": [{"email": "kreitz@salesforce.com", "name": "Kenneth Reitz", "role": "author"}], "document_names": {"description": "DESCRIPTION.rst"}, "project_urls": {"Home": "https://github.com/heroku/django-heroku"}}}, "extras": [], "generator": "bdist_wheel (0.30.0)", "license": "MIT", "metadata_version": "2.0", "name": "django-heroku", "run_requires": [{"requires": ["dj-database-url (>=0.5.0)", "django", "psycopg2", "whitenoise"]}], "summary": "This is a Django library for Heroku apps.", "version": "0.3.1"}
\ No newline at end of file
Index: venv/Lib/site-packages/django_heroku-0.3.1.dist-info/RECORD
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/django_heroku-0.3.1.dist-info/RECORD b/venv/Lib/site-packages/django_heroku-0.3.1.dist-info/RECORD
new file mode 100644
--- /dev/null	(date 1630065394705)
+++ b/venv/Lib/site-packages/django_heroku-0.3.1.dist-info/RECORD	(date 1630065394705)
@@ -0,0 +1,14 @@
+django_heroku-0.3.1.dist-info/DESCRIPTION.rst,sha256=VvfFmiOV4t7Fr1gRBrss4uxD7VlLn0_RKrj8JREGoi8,1664
+django_heroku-0.3.1.dist-info/INSTALLER,sha256=zuuue4knoyJ-UwPPXg8fezS7VCrXJQrAP7zeNuwvFQg,4
+django_heroku-0.3.1.dist-info/METADATA,sha256=yoEI2aHJeMHHX-dl-QjeWkTyQ9PT56ta2j1u3Df62Ls,2643
+django_heroku-0.3.1.dist-info/RECORD,,
+django_heroku-0.3.1.dist-info/REQUESTED,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
+django_heroku-0.3.1.dist-info/WHEEL,sha256=kdsN-5OJAZIiHN-iO4Rhl82KyS0bDWf4uBwMbkNafr8,110
+django_heroku-0.3.1.dist-info/metadata.json,sha256=U6oXQDXK3jPg_3s3lPfFbzprkAc-k3tuSo8yWt2xLMQ,1088
+django_heroku-0.3.1.dist-info/top_level.txt,sha256=vGvZOYk0UioCJl_6belP5Yy4Y-Hx3BdpeUvJOFFUE4M,14
+django_heroku/__init__.py,sha256=79Ih1151rfcqZdr7F8HSZSTs_iT2SKd1xCkehMsXeXs,19
+django_heroku/__pycache__/__init__.cpython-39.pyc,,
+django_heroku/__pycache__/__version__.cpython-39.pyc,,
+django_heroku/__pycache__/core.cpython-39.pyc,,
+django_heroku/__version__.py,sha256=2KwowXhmiT6-Bln7VPq9d9sRpAzJq9qLyclhp2KWmjA,21
+django_heroku/core.py,sha256=vxV3saoGkw7OYdb9g-GlWSTSgaDuVpIE4y08vkNPPN4,5750
Index: venv/Lib/site-packages/django_heroku-0.3.1.dist-info/INSTALLER
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/django_heroku-0.3.1.dist-info/INSTALLER b/venv/Lib/site-packages/django_heroku-0.3.1.dist-info/INSTALLER
new file mode 100644
--- /dev/null	(date 1630065394693)
+++ b/venv/Lib/site-packages/django_heroku-0.3.1.dist-info/INSTALLER	(date 1630065394693)
@@ -0,0 +1,1 @@
+pip
Index: venv/Lib/site-packages/django_heroku-0.3.1.dist-info/METADATA
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/django_heroku-0.3.1.dist-info/METADATA b/venv/Lib/site-packages/django_heroku-0.3.1.dist-info/METADATA
new file mode 100644
--- /dev/null	(date 1630065394642)
+++ b/venv/Lib/site-packages/django_heroku-0.3.1.dist-info/METADATA	(date 1630065394642)
@@ -0,0 +1,81 @@
+Metadata-Version: 2.0
+Name: django-heroku
+Version: 0.3.1
+Summary: This is a Django library for Heroku apps.
+Home-page: https://github.com/heroku/django-heroku
+Author: Kenneth Reitz
+Author-email: kreitz@salesforce.com
+License: MIT
+Description-Content-Type: UNKNOWN
+Platform: UNKNOWN
+Classifier: License :: OSI Approved :: MIT License
+Classifier: Programming Language :: Python
+Classifier: Programming Language :: Python :: 2.6
+Classifier: Programming Language :: Python :: 2.7
+Classifier: Programming Language :: Python :: 3
+Classifier: Programming Language :: Python :: 3.3
+Classifier: Programming Language :: Python :: 3.4
+Classifier: Programming Language :: Python :: 3.5
+Classifier: Programming Language :: Python :: 3.6
+Classifier: Programming Language :: Python :: Implementation :: CPython
+Classifier: Programming Language :: Python :: Implementation :: PyPy
+Requires-Dist: dj-database-url (>=0.5.0)
+Requires-Dist: whitenoise
+Requires-Dist: psycopg2
+Requires-Dist: django
+
+
+Django-Heroku (Python Library)
+==============================
+
+.. image:: https://travis-ci.org/heroku/django-heroku.svg?branch=master
+    :target: https://travis-ci.org/heroku/django-heroku
+
+This is a Django library for Heroku applications that ensures a seamless deployment and development experience.
+
+This library provides:
+
+-  Settings configuration (Static files / WhiteNoise).
+-  Logging configuration.
+-  Test runner (important for `Heroku CI <https://www.heroku.com/continuous-integration>`_).
+
+--------------
+
+Django 2.0 is targeted, but older versions of Django should be compatible. Only Python 3 is supported.
+
+Usage of Django-Heroku
+----------------------
+
+In ``settings.py``, at the very bottom::
+
+    …
+    # Configure Django App for Heroku.
+    import django_heroku
+    django_heroku.settings(locals())
+
+This will automatically configure ``DATABASE_URL``, ``ALLOWED_HOSTS``, WhiteNoise (for static assets), Logging, and Heroku CI for your application.
+
+**Bonus points!** If you set the ``SECRET_KEY`` environment variable, it will automatically be used in your Django settings, too!
+
+Disabling Functionality
+///////////////////////
+
+``settings()`` also accepts keyword arguments that can be passed ``False`` as a value, which will disable automatic configuration for their specific areas of responsibility:
+
+- ``databases``
+- ``test_runner``
+- ``staticfiles``
+- ``allowed_hosts``
+- ``logging``
+- ``secret_key``
+
+-----------------------
+
+You can also just use this library to provide a test runner for your Django application, for use on Heroku CI::
+
+    import django_heroku
+    TEST_RUNNER = 'django_heroku.HerokuDiscoverRunner'
+
+✨🍰✨
+
+
Index: venv/Lib/site-packages/psycopg2-2.9.1.dist-info/WHEEL
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/psycopg2-2.9.1.dist-info/WHEEL b/venv/Lib/site-packages/psycopg2-2.9.1.dist-info/WHEEL
new file mode 100644
--- /dev/null	(date 1630065394310)
+++ b/venv/Lib/site-packages/psycopg2-2.9.1.dist-info/WHEEL	(date 1630065394310)
@@ -0,0 +1,5 @@
+Wheel-Version: 1.0
+Generator: bdist_wheel (0.36.2)
+Root-Is-Purelib: false
+Tag: cp39-cp39-win_amd64
+
Index: venv/Lib/site-packages/django_heroku-0.3.1.dist-info/DESCRIPTION.rst
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/django_heroku-0.3.1.dist-info/DESCRIPTION.rst b/venv/Lib/site-packages/django_heroku-0.3.1.dist-info/DESCRIPTION.rst
new file mode 100644
--- /dev/null	(date 1630065394631)
+++ b/venv/Lib/site-packages/django_heroku-0.3.1.dist-info/DESCRIPTION.rst	(date 1630065394631)
@@ -0,0 +1,55 @@
+
+Django-Heroku (Python Library)
+==============================
+
+.. image:: https://travis-ci.org/heroku/django-heroku.svg?branch=master
+    :target: https://travis-ci.org/heroku/django-heroku
+
+This is a Django library for Heroku applications that ensures a seamless deployment and development experience.
+
+This library provides:
+
+-  Settings configuration (Static files / WhiteNoise).
+-  Logging configuration.
+-  Test runner (important for `Heroku CI <https://www.heroku.com/continuous-integration>`_).
+
+--------------
+
+Django 2.0 is targeted, but older versions of Django should be compatible. Only Python 3 is supported.
+
+Usage of Django-Heroku
+----------------------
+
+In ``settings.py``, at the very bottom::
+
+    …
+    # Configure Django App for Heroku.
+    import django_heroku
+    django_heroku.settings(locals())
+
+This will automatically configure ``DATABASE_URL``, ``ALLOWED_HOSTS``, WhiteNoise (for static assets), Logging, and Heroku CI for your application.
+
+**Bonus points!** If you set the ``SECRET_KEY`` environment variable, it will automatically be used in your Django settings, too!
+
+Disabling Functionality
+///////////////////////
+
+``settings()`` also accepts keyword arguments that can be passed ``False`` as a value, which will disable automatic configuration for their specific areas of responsibility:
+
+- ``databases``
+- ``test_runner``
+- ``staticfiles``
+- ``allowed_hosts``
+- ``logging``
+- ``secret_key``
+
+-----------------------
+
+You can also just use this library to provide a test runner for your Django application, for use on Heroku CI::
+
+    import django_heroku
+    TEST_RUNNER = 'django_heroku.HerokuDiscoverRunner'
+
+✨🍰✨
+
+
Index: venv/Lib/site-packages/psycopg2-2.9.1.dist-info/RECORD
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/psycopg2-2.9.1.dist-info/RECORD b/venv/Lib/site-packages/psycopg2-2.9.1.dist-info/RECORD
new file mode 100644
--- /dev/null	(date 1630065394564)
+++ b/venv/Lib/site-packages/psycopg2-2.9.1.dist-info/RECORD	(date 1630065394564)
@@ -0,0 +1,29 @@
+psycopg2-2.9.1.dist-info/INSTALLER,sha256=zuuue4knoyJ-UwPPXg8fezS7VCrXJQrAP7zeNuwvFQg,4
+psycopg2-2.9.1.dist-info/LICENSE,sha256=lhS4XfyacsWyyjMUTB1-HtOxwpdFnZ-yimpXYsLo1xs,2238
+psycopg2-2.9.1.dist-info/METADATA,sha256=VcLa10PWTJ99CXoB2zLkwY1RZmcKF1TcOrnOicey7gM,4315
+psycopg2-2.9.1.dist-info/RECORD,,
+psycopg2-2.9.1.dist-info/WHEEL,sha256=jr7ubY0Lkz_yXH9FfFe9PTtLhGOsf62dZkNvTYrJINE,100
+psycopg2-2.9.1.dist-info/top_level.txt,sha256=7dHGpLqQ3w-vGmGEVn-7uK90qU9fyrGdWWi7S-gTcnM,9
+psycopg2/__init__.py,sha256=9mo5Qd0uWHiEBx2CdogGos2kNqtlNNGzbtYlGC0hWS8,4768
+psycopg2/__pycache__/__init__.cpython-39.pyc,,
+psycopg2/__pycache__/_ipaddress.cpython-39.pyc,,
+psycopg2/__pycache__/_json.cpython-39.pyc,,
+psycopg2/__pycache__/_range.cpython-39.pyc,,
+psycopg2/__pycache__/errorcodes.cpython-39.pyc,,
+psycopg2/__pycache__/errors.cpython-39.pyc,,
+psycopg2/__pycache__/extensions.cpython-39.pyc,,
+psycopg2/__pycache__/extras.cpython-39.pyc,,
+psycopg2/__pycache__/pool.cpython-39.pyc,,
+psycopg2/__pycache__/sql.cpython-39.pyc,,
+psycopg2/__pycache__/tz.cpython-39.pyc,,
+psycopg2/_ipaddress.py,sha256=jkuyhLgqUGRBcLNWDM8QJysV6q1Npc_RYH4_kE7JZPU,2922
+psycopg2/_json.py,sha256=XPn4PnzbTg1Dcqz7n1JMv5dKhB5VFV6834GEtxSawt0,7153
+psycopg2/_psycopg.cp39-win_amd64.pyd,sha256=b2SX6e_KTHkqOnoOw7igRTa8cKlqGnfjXnvzlTawb_Y,2442240
+psycopg2/_range.py,sha256=79xD6i5_aIVYTj_q6lrqfQEz00y3V16nJ5kbScLqy6U,17608
+psycopg2/errorcodes.py,sha256=Z5gbq6FF4nAucL4eWxNwa_UQC7FmA-fz0nr_Ly4KToA,14277
+psycopg2/errors.py,sha256=aAS4dJyTg1bsDzJDCRQAMB_s7zv-Q4yB6Yvih26I-0M,1425
+psycopg2/extensions.py,sha256=CG0kG5vL8Ot503UGlDXXJJFdFWLg4HE2_c1-lLOLc8M,6797
+psycopg2/extras.py,sha256=XOQ6YkyaLeypg2_POPXt8c_MUbuSthdMYywGn9rMNfo,42863
+psycopg2/pool.py,sha256=UGEt8IdP3xNc2PGYNlG4sQvg8nhf4aeCnz39hTR0H8I,6316
+psycopg2/sql.py,sha256=OcFEAmpe2aMfrx0MEk4Lx00XvXXJCmvllaOVbJY-yoE,14779
+psycopg2/tz.py,sha256=r95kK7eGSpOYr_luCyYsznHMzjl52sLjsnSPXkXLzRI,4870
Index: venv/Lib/site-packages/psycopg2-2.9.1.dist-info/top_level.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/psycopg2-2.9.1.dist-info/top_level.txt b/venv/Lib/site-packages/psycopg2-2.9.1.dist-info/top_level.txt
new file mode 100644
--- /dev/null	(date 1630065394312)
+++ b/venv/Lib/site-packages/psycopg2-2.9.1.dist-info/top_level.txt	(date 1630065394312)
@@ -0,0 +1,1 @@
+psycopg2
Index: venv/Lib/site-packages/psycopg2-2.9.1.dist-info/LICENSE
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/psycopg2-2.9.1.dist-info/LICENSE b/venv/Lib/site-packages/psycopg2-2.9.1.dist-info/LICENSE
new file mode 100644
--- /dev/null	(date 1630065394306)
+++ b/venv/Lib/site-packages/psycopg2-2.9.1.dist-info/LICENSE	(date 1630065394306)
@@ -0,0 +1,49 @@
+psycopg2 and the LGPL
+---------------------
+
+psycopg2 is free software: you can redistribute it and/or modify it
+under the terms of the GNU Lesser General Public License as published
+by the Free Software Foundation, either version 3 of the License, or
+(at your option) any later version.
+
+psycopg2 is distributed in the hope that it will be useful, but WITHOUT
+ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public
+License for more details.
+
+In addition, as a special exception, the copyright holders give
+permission to link this program with the OpenSSL library (or with
+modified versions of OpenSSL that use the same license as OpenSSL),
+and distribute linked combinations including the two.
+
+You must obey the GNU Lesser General Public License in all respects for
+all of the code used other than OpenSSL. If you modify file(s) with this
+exception, you may extend this exception to your version of the file(s),
+but you are not obligated to do so. If you do not wish to do so, delete
+this exception statement from your version. If you delete this exception
+statement from all source files in the program, then also delete it here.
+
+You should have received a copy of the GNU Lesser General Public License
+along with psycopg2 (see the doc/ directory.)
+If not, see <https://www.gnu.org/licenses/>.
+
+
+Alternative licenses
+--------------------
+
+The following BSD-like license applies (at your option) to the files following
+the pattern ``psycopg/adapter*.{h,c}`` and ``psycopg/microprotocol*.{h,c}``:
+
+ Permission is granted to anyone to use this software for any purpose,
+ including commercial applications, and to alter it and redistribute it
+ freely, subject to the following restrictions:
+
+ 1. The origin of this software must not be misrepresented; you must not
+    claim that you wrote the original software. If you use this
+    software in a product, an acknowledgment in the product documentation
+    would be appreciated but is not required.
+
+ 2. Altered source versions must be plainly marked as such, and must not
+    be misrepresented as being the original software.
+
+ 3. This notice may not be removed or altered from any source distribution.
Index: venv/Lib/site-packages/gunicorn/workers/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/workers/__init__.py b/venv/Lib/site-packages/gunicorn/workers/__init__.py
new file mode 100644
--- /dev/null	(date 1630065626411)
+++ b/venv/Lib/site-packages/gunicorn/workers/__init__.py	(date 1630065626411)
@@ -0,0 +1,15 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+# supported gunicorn workers.
+SUPPORTED_WORKERS = {
+    "sync": "gunicorn.workers.sync.SyncWorker",
+    "eventlet": "gunicorn.workers.geventlet.EventletWorker",
+    "gevent": "gunicorn.workers.ggevent.GeventWorker",
+    "gevent_wsgi": "gunicorn.workers.ggevent.GeventPyWSGIWorker",
+    "gevent_pywsgi": "gunicorn.workers.ggevent.GeventPyWSGIWorker",
+    "tornado": "gunicorn.workers.gtornado.TornadoWorker",
+    "gthread": "gunicorn.workers.gthread.ThreadWorker",
+}
Index: venv/Lib/site-packages/psycopg2-2.9.1.dist-info/METADATA
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/psycopg2-2.9.1.dist-info/METADATA b/venv/Lib/site-packages/psycopg2-2.9.1.dist-info/METADATA
new file mode 100644
--- /dev/null	(date 1630065394308)
+++ b/venv/Lib/site-packages/psycopg2-2.9.1.dist-info/METADATA	(date 1630065394308)
@@ -0,0 +1,109 @@
+Metadata-Version: 2.1
+Name: psycopg2
+Version: 2.9.1
+Summary: psycopg2 - Python-PostgreSQL Database Adapter
+Home-page: https://psycopg.org/
+Author: Federico Di Gregorio
+Author-email: fog@initd.org
+Maintainer: Daniele Varrazzo
+Maintainer-email: daniele.varrazzo@gmail.org
+License: LGPL with exceptions
+Project-URL: Homepage, https://psycopg.org/
+Project-URL: Documentation, https://www.psycopg.org/docs/
+Project-URL: Code, https://github.com/psycopg/psycopg2
+Project-URL: Issue Tracker, https://github.com/psycopg/psycopg2/issues
+Project-URL: Download, https://pypi.org/project/psycopg2/
+Platform: any
+Classifier: Development Status :: 5 - Production/Stable
+Classifier: Intended Audience :: Developers
+Classifier: License :: OSI Approved :: GNU Library or Lesser General Public License (LGPL)
+Classifier: Programming Language :: Python
+Classifier: Programming Language :: Python :: 3
+Classifier: Programming Language :: Python :: 3.6
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3 :: Only
+Classifier: Programming Language :: Python :: Implementation :: CPython
+Classifier: Programming Language :: C
+Classifier: Programming Language :: SQL
+Classifier: Topic :: Database
+Classifier: Topic :: Database :: Front-Ends
+Classifier: Topic :: Software Development
+Classifier: Topic :: Software Development :: Libraries :: Python Modules
+Classifier: Operating System :: Microsoft :: Windows
+Classifier: Operating System :: Unix
+Requires-Python: >=3.6
+
+Psycopg is the most popular PostgreSQL database adapter for the Python
+programming language.  Its main features are the complete implementation of
+the Python DB API 2.0 specification and the thread safety (several threads can
+share the same connection).  It was designed for heavily multi-threaded
+applications that create and destroy lots of cursors and make a large number
+of concurrent "INSERT"s or "UPDATE"s.
+
+Psycopg 2 is mostly implemented in C as a libpq wrapper, resulting in being
+both efficient and secure.  It features client-side and server-side cursors,
+asynchronous communication and notifications, "COPY TO/COPY FROM" support.
+Many Python types are supported out-of-the-box and adapted to matching
+PostgreSQL data types; adaptation can be extended and customized thanks to a
+flexible objects adaptation system.
+
+Psycopg 2 is both Unicode and Python 3 friendly.
+
+
+Documentation
+-------------
+
+Documentation is included in the ``doc`` directory and is `available online`__.
+
+.. __: https://www.psycopg.org/docs/
+
+For any other resource (source code repository, bug tracker, mailing list)
+please check the `project homepage`__.
+
+.. __: https://psycopg.org/
+
+
+Installation
+------------
+
+Building Psycopg requires a few prerequisites (a C compiler, some development
+packages): please check the install_ and the faq_ documents in the ``doc`` dir
+or online for the details.
+
+If prerequisites are met, you can install psycopg like any other Python
+package, using ``pip`` to download it from PyPI_::
+
+    $ pip install psycopg2
+
+or using ``setup.py`` if you have downloaded the source package locally::
+
+    $ python setup.py build
+    $ sudo python setup.py install
+
+You can also obtain a stand-alone package, not requiring a compiler or
+external libraries, by installing the `psycopg2-binary`_ package from PyPI::
+
+    $ pip install psycopg2-binary
+
+The binary package is a practical choice for development and testing but in
+production it is advised to use the package built from sources.
+
+.. _PyPI: https://pypi.org/project/psycopg2/
+.. _psycopg2-binary: https://pypi.org/project/psycopg2-binary/
+.. _install: https://www.psycopg.org/docs/install.html#install-from-source
+.. _faq: https://www.psycopg.org/docs/faq.html#faq-compile
+
+:Linux/OSX: |gh-actions|
+:Windows: |appveyor|
+
+.. |gh-actions| image:: https://github.com/psycopg/psycopg2/actions/workflows/tests.yml/badge.svg
+    :target: https://github.com/psycopg/psycopg2/actions/workflows/tests.yml
+    :alt: Linux and OSX build status
+
+.. |appveyor| image:: https://ci.appveyor.com/api/projects/status/github/psycopg/psycopg2?branch=master&svg=true
+    :target: https://ci.appveyor.com/project/psycopg/psycopg2/branch/master
+    :alt: Windows build status
+
+
Index: venv/Lib/site-packages/gunicorn/instrument/statsd.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/instrument/statsd.py b/venv/Lib/site-packages/gunicorn/instrument/statsd.py
new file mode 100644
--- /dev/null	(date 1630065626406)
+++ b/venv/Lib/site-packages/gunicorn/instrument/statsd.py	(date 1630065626406)
@@ -0,0 +1,130 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+"Bare-bones implementation of statsD's protocol, client-side"
+
+import logging
+import socket
+from re import sub
+
+from gunicorn.glogging import Logger
+
+# Instrumentation constants
+METRIC_VAR = "metric"
+VALUE_VAR = "value"
+MTYPE_VAR = "mtype"
+GAUGE_TYPE = "gauge"
+COUNTER_TYPE = "counter"
+HISTOGRAM_TYPE = "histogram"
+
+
+class Statsd(Logger):
+    """statsD-based instrumentation, that passes as a logger
+    """
+    def __init__(self, cfg):
+        """host, port: statsD server
+        """
+        Logger.__init__(self, cfg)
+        self.prefix = sub(r"^(.+[^.]+)\.*$", "\\g<1>.", cfg.statsd_prefix)
+        try:
+            host, port = cfg.statsd_host
+            self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
+            self.sock.connect((host, int(port)))
+        except Exception:
+            self.sock = None
+
+        self.dogstatsd_tags = cfg.dogstatsd_tags
+
+    # Log errors and warnings
+    def critical(self, msg, *args, **kwargs):
+        Logger.critical(self, msg, *args, **kwargs)
+        self.increment("gunicorn.log.critical", 1)
+
+    def error(self, msg, *args, **kwargs):
+        Logger.error(self, msg, *args, **kwargs)
+        self.increment("gunicorn.log.error", 1)
+
+    def warning(self, msg, *args, **kwargs):
+        Logger.warning(self, msg, *args, **kwargs)
+        self.increment("gunicorn.log.warning", 1)
+
+    def exception(self, msg, *args, **kwargs):
+        Logger.exception(self, msg, *args, **kwargs)
+        self.increment("gunicorn.log.exception", 1)
+
+    # Special treatment for info, the most common log level
+    def info(self, msg, *args, **kwargs):
+        self.log(logging.INFO, msg, *args, **kwargs)
+
+    # skip the run-of-the-mill logs
+    def debug(self, msg, *args, **kwargs):
+        self.log(logging.DEBUG, msg, *args, **kwargs)
+
+    def log(self, lvl, msg, *args, **kwargs):
+        """Log a given statistic if metric, value and type are present
+        """
+        try:
+            extra = kwargs.get("extra", None)
+            if extra is not None:
+                metric = extra.get(METRIC_VAR, None)
+                value = extra.get(VALUE_VAR, None)
+                typ = extra.get(MTYPE_VAR, None)
+                if metric and value and typ:
+                    if typ == GAUGE_TYPE:
+                        self.gauge(metric, value)
+                    elif typ == COUNTER_TYPE:
+                        self.increment(metric, value)
+                    elif typ == HISTOGRAM_TYPE:
+                        self.histogram(metric, value)
+                    else:
+                        pass
+
+            # Log to parent logger only if there is something to say
+            if msg:
+                Logger.log(self, lvl, msg, *args, **kwargs)
+        except Exception:
+            Logger.warning(self, "Failed to log to statsd", exc_info=True)
+
+    # access logging
+    def access(self, resp, req, environ, request_time):
+        """Measure request duration
+        request_time is a datetime.timedelta
+        """
+        Logger.access(self, resp, req, environ, request_time)
+        duration_in_ms = request_time.seconds * 1000 + float(request_time.microseconds) / 10 ** 3
+        status = resp.status
+        if isinstance(status, str):
+            status = int(status.split(None, 1)[0])
+        self.histogram("gunicorn.request.duration", duration_in_ms)
+        self.increment("gunicorn.requests", 1)
+        self.increment("gunicorn.request.status.%d" % status, 1)
+
+    # statsD methods
+    # you can use those directly if you want
+    def gauge(self, name, value):
+        self._sock_send("{0}{1}:{2}|g".format(self.prefix, name, value))
+
+    def increment(self, name, value, sampling_rate=1.0):
+        self._sock_send("{0}{1}:{2}|c|@{3}".format(self.prefix, name, value, sampling_rate))
+
+    def decrement(self, name, value, sampling_rate=1.0):
+        self._sock_send("{0}{1}:-{2}|c|@{3}".format(self.prefix, name, value, sampling_rate))
+
+    def histogram(self, name, value):
+        self._sock_send("{0}{1}:{2}|ms".format(self.prefix, name, value))
+
+    def _sock_send(self, msg):
+        try:
+            if isinstance(msg, str):
+                msg = msg.encode("ascii")
+
+            # http://docs.datadoghq.com/guides/dogstatsd/#datagram-format
+            if self.dogstatsd_tags:
+                msg = msg + b"|#" + self.dogstatsd_tags.encode('ascii')
+
+            if self.sock:
+                self.sock.send(msg)
+        except Exception:
+            Logger.warning(self, "Error sending message to statsd", exc_info=True)
Index: venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/WHEEL
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/WHEEL b/venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/WHEEL
new file mode 100644
--- /dev/null	(date 1630065394586)
+++ b/venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/WHEEL	(date 1630065394586)
@@ -0,0 +1,6 @@
+Wheel-Version: 1.0
+Generator: bdist_wheel (0.30.0)
+Root-Is-Purelib: true
+Tag: py2-none-any
+Tag: py3-none-any
+
Index: venv/Lib/site-packages/gunicorn/workers/sync.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/workers/sync.py b/venv/Lib/site-packages/gunicorn/workers/sync.py
new file mode 100644
--- /dev/null	(date 1630065626437)
+++ b/venv/Lib/site-packages/gunicorn/workers/sync.py	(date 1630065626437)
@@ -0,0 +1,211 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+#
+
+from datetime import datetime
+import errno
+import os
+import select
+import socket
+import ssl
+import sys
+
+import gunicorn.http as http
+import gunicorn.http.wsgi as wsgi
+import gunicorn.util as util
+import gunicorn.workers.base as base
+
+
+class StopWaiting(Exception):
+    """ exception raised to stop waiting for a connection """
+
+
+class SyncWorker(base.Worker):
+
+    def accept(self, listener):
+        client, addr = listener.accept()
+        client.setblocking(1)
+        util.close_on_exec(client)
+        self.handle(listener, client, addr)
+
+    def wait(self, timeout):
+        try:
+            self.notify()
+            ret = select.select(self.wait_fds, [], [], timeout)
+            if ret[0]:
+                if self.PIPE[0] in ret[0]:
+                    os.read(self.PIPE[0], 1)
+                return ret[0]
+
+        except select.error as e:
+            if e.args[0] == errno.EINTR:
+                return self.sockets
+            if e.args[0] == errno.EBADF:
+                if self.nr < 0:
+                    return self.sockets
+                else:
+                    raise StopWaiting
+            raise
+
+    def is_parent_alive(self):
+        # If our parent changed then we shut down.
+        if self.ppid != os.getppid():
+            self.log.info("Parent changed, shutting down: %s", self)
+            return False
+        return True
+
+    def run_for_one(self, timeout):
+        listener = self.sockets[0]
+        while self.alive:
+            self.notify()
+
+            # Accept a connection. If we get an error telling us
+            # that no connection is waiting we fall down to the
+            # select which is where we'll wait for a bit for new
+            # workers to come give us some love.
+            try:
+                self.accept(listener)
+                # Keep processing clients until no one is waiting. This
+                # prevents the need to select() for every client that we
+                # process.
+                continue
+
+            except EnvironmentError as e:
+                if e.errno not in (errno.EAGAIN, errno.ECONNABORTED,
+                                   errno.EWOULDBLOCK):
+                    raise
+
+            if not self.is_parent_alive():
+                return
+
+            try:
+                self.wait(timeout)
+            except StopWaiting:
+                return
+
+    def run_for_multiple(self, timeout):
+        while self.alive:
+            self.notify()
+
+            try:
+                ready = self.wait(timeout)
+            except StopWaiting:
+                return
+
+            if ready is not None:
+                for listener in ready:
+                    if listener == self.PIPE[0]:
+                        continue
+
+                    try:
+                        self.accept(listener)
+                    except EnvironmentError as e:
+                        if e.errno not in (errno.EAGAIN, errno.ECONNABORTED,
+                                           errno.EWOULDBLOCK):
+                            raise
+
+            if not self.is_parent_alive():
+                return
+
+    def run(self):
+        # if no timeout is given the worker will never wait and will
+        # use the CPU for nothing. This minimal timeout prevent it.
+        timeout = self.timeout or 0.5
+
+        # self.socket appears to lose its blocking status after
+        # we fork in the arbiter. Reset it here.
+        for s in self.sockets:
+            s.setblocking(0)
+
+        if len(self.sockets) > 1:
+            self.run_for_multiple(timeout)
+        else:
+            self.run_for_one(timeout)
+
+    def handle(self, listener, client, addr):
+        req = None
+        try:
+            if self.cfg.is_ssl:
+                client = ssl.wrap_socket(client, server_side=True,
+                                         **self.cfg.ssl_options)
+
+            parser = http.RequestParser(self.cfg, client, addr)
+            req = next(parser)
+            self.handle_request(listener, req, client, addr)
+        except http.errors.NoMoreData as e:
+            self.log.debug("Ignored premature client disconnection. %s", e)
+        except StopIteration as e:
+            self.log.debug("Closing connection. %s", e)
+        except ssl.SSLError as e:
+            if e.args[0] == ssl.SSL_ERROR_EOF:
+                self.log.debug("ssl connection closed")
+                client.close()
+            else:
+                self.log.debug("Error processing SSL request.")
+                self.handle_error(req, client, addr, e)
+        except EnvironmentError as e:
+            if e.errno not in (errno.EPIPE, errno.ECONNRESET, errno.ENOTCONN):
+                self.log.exception("Socket error processing request.")
+            else:
+                if e.errno == errno.ECONNRESET:
+                    self.log.debug("Ignoring connection reset")
+                elif e.errno == errno.ENOTCONN:
+                    self.log.debug("Ignoring socket not connected")
+                else:
+                    self.log.debug("Ignoring EPIPE")
+        except Exception as e:
+            self.handle_error(req, client, addr, e)
+        finally:
+            util.close(client)
+
+    def handle_request(self, listener, req, client, addr):
+        environ = {}
+        resp = None
+        try:
+            self.cfg.pre_request(self, req)
+            request_start = datetime.now()
+            resp, environ = wsgi.create(req, client, addr,
+                                        listener.getsockname(), self.cfg)
+            # Force the connection closed until someone shows
+            # a buffering proxy that supports Keep-Alive to
+            # the backend.
+            resp.force_close()
+            self.nr += 1
+            if self.nr >= self.max_requests:
+                self.log.info("Autorestarting worker after current request.")
+                self.alive = False
+            respiter = self.wsgi(environ, resp.start_response)
+            try:
+                if isinstance(respiter, environ['wsgi.file_wrapper']):
+                    resp.write_file(respiter)
+                else:
+                    for item in respiter:
+                        resp.write(item)
+                resp.close()
+                request_time = datetime.now() - request_start
+                self.log.access(resp, req, environ, request_time)
+            finally:
+                if hasattr(respiter, "close"):
+                    respiter.close()
+        except EnvironmentError:
+            # pass to next try-except level
+            util.reraise(*sys.exc_info())
+        except Exception:
+            if resp and resp.headers_sent:
+                # If the requests have already been sent, we should close the
+                # connection to indicate the error.
+                self.log.exception("Error handling request")
+                try:
+                    client.shutdown(socket.SHUT_RDWR)
+                    client.close()
+                except EnvironmentError:
+                    pass
+                raise StopIteration()
+            raise
+        finally:
+            try:
+                self.cfg.post_request(self, req, environ, resp)
+            except Exception:
+                self.log.exception("Exception in post_request hook")
Index: venv/Lib/site-packages/psycopg2-2.9.1.dist-info/INSTALLER
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/psycopg2-2.9.1.dist-info/INSTALLER b/venv/Lib/site-packages/psycopg2-2.9.1.dist-info/INSTALLER
new file mode 100644
--- /dev/null	(date 1630065394557)
+++ b/venv/Lib/site-packages/psycopg2-2.9.1.dist-info/INSTALLER	(date 1630065394557)
@@ -0,0 +1,1 @@
+pip
Index: venv/Lib/site-packages/gunicorn/workers/workertmp.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/workers/workertmp.py b/venv/Lib/site-packages/gunicorn/workers/workertmp.py
new file mode 100644
--- /dev/null	(date 1630065626439)
+++ b/venv/Lib/site-packages/gunicorn/workers/workertmp.py	(date 1630065626439)
@@ -0,0 +1,55 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+import os
+import platform
+import tempfile
+
+from gunicorn import util
+
+PLATFORM = platform.system()
+IS_CYGWIN = PLATFORM.startswith('CYGWIN')
+
+
+class WorkerTmp(object):
+
+    def __init__(self, cfg):
+        old_umask = os.umask(cfg.umask)
+        fdir = cfg.worker_tmp_dir
+        if fdir and not os.path.isdir(fdir):
+            raise RuntimeError("%s doesn't exist. Can't create workertmp." % fdir)
+        fd, name = tempfile.mkstemp(prefix="wgunicorn-", dir=fdir)
+        os.umask(old_umask)
+
+        # change the owner and group of the file if the worker will run as
+        # a different user or group, so that the worker can modify the file
+        if cfg.uid != os.geteuid() or cfg.gid != os.getegid():
+            util.chown(name, cfg.uid, cfg.gid)
+
+        # unlink the file so we don't leak tempory files
+        try:
+            if not IS_CYGWIN:
+                util.unlink(name)
+            # In Python 3.8, open() emits RuntimeWarning if buffering=1 for binary mode.
+            # Because we never write to this file, pass 0 to switch buffering off.
+            self._tmp = os.fdopen(fd, 'w+b', 0)
+        except Exception:
+            os.close(fd)
+            raise
+
+        self.spinner = 0
+
+    def notify(self):
+        self.spinner = (self.spinner + 1) % 2
+        os.fchmod(self._tmp.fileno(), self.spinner)
+
+    def last_update(self):
+        return os.fstat(self._tmp.fileno()).st_ctime
+
+    def fileno(self):
+        return self._tmp.fileno()
+
+    def close(self):
+        return self._tmp.close()
Index: venv/Lib/site-packages/gunicorn/workers/gthread.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/workers/gthread.py b/venv/Lib/site-packages/gunicorn/workers/gthread.py
new file mode 100644
--- /dev/null	(date 1630065626433)
+++ b/venv/Lib/site-packages/gunicorn/workers/gthread.py	(date 1630065626433)
@@ -0,0 +1,362 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+# design:
+# A threaded worker accepts connections in the main loop, accepted
+# connections are added to the thread pool as a connection job.
+# Keepalive connections are put back in the loop waiting for an event.
+# If no event happen after the keep alive timeout, the connection is
+# closed.
+# pylint: disable=no-else-break
+
+import concurrent.futures as futures
+import errno
+import os
+import selectors
+import socket
+import ssl
+import sys
+import time
+from collections import deque
+from datetime import datetime
+from functools import partial
+from threading import RLock
+
+from . import base
+from .. import http
+from .. import util
+from ..http import wsgi
+
+
+class TConn(object):
+
+    def __init__(self, cfg, sock, client, server):
+        self.cfg = cfg
+        self.sock = sock
+        self.client = client
+        self.server = server
+
+        self.timeout = None
+        self.parser = None
+
+        # set the socket to non blocking
+        self.sock.setblocking(False)
+
+    def init(self):
+        self.sock.setblocking(True)
+        if self.parser is None:
+            # wrap the socket if needed
+            if self.cfg.is_ssl:
+                self.sock = ssl.wrap_socket(self.sock, server_side=True,
+                                            **self.cfg.ssl_options)
+
+            # initialize the parser
+            self.parser = http.RequestParser(self.cfg, self.sock, self.client)
+
+    def set_timeout(self):
+        # set the timeout
+        self.timeout = time.time() + self.cfg.keepalive
+
+    def close(self):
+        util.close(self.sock)
+
+
+class ThreadWorker(base.Worker):
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.worker_connections = self.cfg.worker_connections
+        self.max_keepalived = self.cfg.worker_connections - self.cfg.threads
+        # initialise the pool
+        self.tpool = None
+        self.poller = None
+        self._lock = None
+        self.futures = deque()
+        self._keep = deque()
+        self.nr_conns = 0
+
+    @classmethod
+    def check_config(cls, cfg, log):
+        max_keepalived = cfg.worker_connections - cfg.threads
+
+        if max_keepalived <= 0 and cfg.keepalive:
+            log.warning("No keepalived connections can be handled. " +
+                        "Check the number of worker connections and threads.")
+
+    def init_process(self):
+        self.tpool = self.get_thread_pool()
+        self.poller = selectors.DefaultSelector()
+        self._lock = RLock()
+        super().init_process()
+
+    def get_thread_pool(self):
+        """Override this method to customize how the thread pool is created"""
+        return futures.ThreadPoolExecutor(max_workers=self.cfg.threads)
+
+    def handle_quit(self, sig, frame):
+        self.alive = False
+        # worker_int callback
+        self.cfg.worker_int(self)
+        self.tpool.shutdown(False)
+        time.sleep(0.1)
+        sys.exit(0)
+
+    def _wrap_future(self, fs, conn):
+        fs.conn = conn
+        self.futures.append(fs)
+        fs.add_done_callback(self.finish_request)
+
+    def enqueue_req(self, conn):
+        conn.init()
+        # submit the connection to a worker
+        fs = self.tpool.submit(self.handle, conn)
+        self._wrap_future(fs, conn)
+
+    def accept(self, server, listener):
+        try:
+            sock, client = listener.accept()
+            # initialize the connection object
+            conn = TConn(self.cfg, sock, client, server)
+            self.nr_conns += 1
+            # enqueue the job
+            self.enqueue_req(conn)
+        except EnvironmentError as e:
+            if e.errno not in (errno.EAGAIN, errno.ECONNABORTED,
+                               errno.EWOULDBLOCK):
+                raise
+
+    def reuse_connection(self, conn, client):
+        with self._lock:
+            # unregister the client from the poller
+            self.poller.unregister(client)
+            # remove the connection from keepalive
+            try:
+                self._keep.remove(conn)
+            except ValueError:
+                # race condition
+                return
+
+        # submit the connection to a worker
+        self.enqueue_req(conn)
+
+    def murder_keepalived(self):
+        now = time.time()
+        while True:
+            with self._lock:
+                try:
+                    # remove the connection from the queue
+                    conn = self._keep.popleft()
+                except IndexError:
+                    break
+
+            delta = conn.timeout - now
+            if delta > 0:
+                # add the connection back to the queue
+                with self._lock:
+                    self._keep.appendleft(conn)
+                break
+            else:
+                self.nr_conns -= 1
+                # remove the socket from the poller
+                with self._lock:
+                    try:
+                        self.poller.unregister(conn.sock)
+                    except EnvironmentError as e:
+                        if e.errno != errno.EBADF:
+                            raise
+                    except KeyError:
+                        # already removed by the system, continue
+                        pass
+
+                # close the socket
+                conn.close()
+
+    def is_parent_alive(self):
+        # If our parent changed then we shut down.
+        if self.ppid != os.getppid():
+            self.log.info("Parent changed, shutting down: %s", self)
+            return False
+        return True
+
+    def run(self):
+        # init listeners, add them to the event loop
+        for sock in self.sockets:
+            sock.setblocking(False)
+            # a race condition during graceful shutdown may make the listener
+            # name unavailable in the request handler so capture it once here
+            server = sock.getsockname()
+            acceptor = partial(self.accept, server)
+            self.poller.register(sock, selectors.EVENT_READ, acceptor)
+
+        while self.alive:
+            # notify the arbiter we are alive
+            self.notify()
+
+            # can we accept more connections?
+            if self.nr_conns < self.worker_connections:
+                # wait for an event
+                events = self.poller.select(1.0)
+                for key, _ in events:
+                    callback = key.data
+                    callback(key.fileobj)
+
+                # check (but do not wait) for finished requests
+                result = futures.wait(self.futures, timeout=0,
+                                      return_when=futures.FIRST_COMPLETED)
+            else:
+                # wait for a request to finish
+                result = futures.wait(self.futures, timeout=1.0,
+                                      return_when=futures.FIRST_COMPLETED)
+
+            # clean up finished requests
+            for fut in result.done:
+                self.futures.remove(fut)
+
+            if not self.is_parent_alive():
+                break
+
+            # handle keepalive timeouts
+            self.murder_keepalived()
+
+        self.tpool.shutdown(False)
+        self.poller.close()
+
+        for s in self.sockets:
+            s.close()
+
+        futures.wait(self.futures, timeout=self.cfg.graceful_timeout)
+
+    def finish_request(self, fs):
+        if fs.cancelled():
+            self.nr_conns -= 1
+            fs.conn.close()
+            return
+
+        try:
+            (keepalive, conn) = fs.result()
+            # if the connection should be kept alived add it
+            # to the eventloop and record it
+            if keepalive and self.alive:
+                # flag the socket as non blocked
+                conn.sock.setblocking(False)
+
+                # register the connection
+                conn.set_timeout()
+                with self._lock:
+                    self._keep.append(conn)
+
+                    # add the socket to the event loop
+                    self.poller.register(conn.sock, selectors.EVENT_READ,
+                                         partial(self.reuse_connection, conn))
+            else:
+                self.nr_conns -= 1
+                conn.close()
+        except Exception:
+            # an exception happened, make sure to close the
+            # socket.
+            self.nr_conns -= 1
+            fs.conn.close()
+
+    def handle(self, conn):
+        keepalive = False
+        req = None
+        try:
+            req = next(conn.parser)
+            if not req:
+                return (False, conn)
+
+            # handle the request
+            keepalive = self.handle_request(req, conn)
+            if keepalive:
+                return (keepalive, conn)
+        except http.errors.NoMoreData as e:
+            self.log.debug("Ignored premature client disconnection. %s", e)
+
+        except StopIteration as e:
+            self.log.debug("Closing connection. %s", e)
+        except ssl.SSLError as e:
+            if e.args[0] == ssl.SSL_ERROR_EOF:
+                self.log.debug("ssl connection closed")
+                conn.sock.close()
+            else:
+                self.log.debug("Error processing SSL request.")
+                self.handle_error(req, conn.sock, conn.client, e)
+
+        except EnvironmentError as e:
+            if e.errno not in (errno.EPIPE, errno.ECONNRESET, errno.ENOTCONN):
+                self.log.exception("Socket error processing request.")
+            else:
+                if e.errno == errno.ECONNRESET:
+                    self.log.debug("Ignoring connection reset")
+                elif e.errno == errno.ENOTCONN:
+                    self.log.debug("Ignoring socket not connected")
+                else:
+                    self.log.debug("Ignoring connection epipe")
+        except Exception as e:
+            self.handle_error(req, conn.sock, conn.client, e)
+
+        return (False, conn)
+
+    def handle_request(self, req, conn):
+        environ = {}
+        resp = None
+        try:
+            self.cfg.pre_request(self, req)
+            request_start = datetime.now()
+            resp, environ = wsgi.create(req, conn.sock, conn.client,
+                                        conn.server, self.cfg)
+            environ["wsgi.multithread"] = True
+            self.nr += 1
+            if self.nr >= self.max_requests:
+                if self.alive:
+                    self.log.info("Autorestarting worker after current request.")
+                    self.alive = False
+                resp.force_close()
+
+            if not self.alive or not self.cfg.keepalive:
+                resp.force_close()
+            elif len(self._keep) >= self.max_keepalived:
+                resp.force_close()
+
+            respiter = self.wsgi(environ, resp.start_response)
+            try:
+                if isinstance(respiter, environ['wsgi.file_wrapper']):
+                    resp.write_file(respiter)
+                else:
+                    for item in respiter:
+                        resp.write(item)
+
+                resp.close()
+                request_time = datetime.now() - request_start
+                self.log.access(resp, req, environ, request_time)
+            finally:
+                if hasattr(respiter, "close"):
+                    respiter.close()
+
+            if resp.should_close():
+                self.log.debug("Closing connection.")
+                return False
+        except EnvironmentError:
+            # pass to next try-except level
+            util.reraise(*sys.exc_info())
+        except Exception:
+            if resp and resp.headers_sent:
+                # If the requests have already been sent, we should close the
+                # connection to indicate the error.
+                self.log.exception("Error handling request")
+                try:
+                    conn.sock.shutdown(socket.SHUT_RDWR)
+                    conn.sock.close()
+                except EnvironmentError:
+                    pass
+                raise StopIteration()
+            raise
+        finally:
+            try:
+                self.cfg.post_request(self, req, environ, resp)
+            except Exception:
+                self.log.exception("Exception in post_request hook")
+
+        return True
Index: venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/RECORD
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/RECORD b/venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/RECORD
new file mode 100644
--- /dev/null	(date 1630065394616)
+++ b/venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/RECORD	(date 1630065394616)
@@ -0,0 +1,9 @@
+__pycache__/dj_database_url.cpython-39.pyc,,
+dj_database_url-0.5.0.dist-info/DESCRIPTION.rst,sha256=Wrza5v0DzZmfNFHyPvO6NhvWIWYhry_BfcLbT3KtqvU,1681
+dj_database_url-0.5.0.dist-info/INSTALLER,sha256=zuuue4knoyJ-UwPPXg8fezS7VCrXJQrAP7zeNuwvFQg,4
+dj_database_url-0.5.0.dist-info/METADATA,sha256=rjnjlInbIyvRf7F9urnn_X8Fh9cboFz718274IeGjCg,2675
+dj_database_url-0.5.0.dist-info/RECORD,,
+dj_database_url-0.5.0.dist-info/WHEEL,sha256=kdsN-5OJAZIiHN-iO4Rhl82KyS0bDWf4uBwMbkNafr8,110
+dj_database_url-0.5.0.dist-info/metadata.json,sha256=7AD_WjZCMNRkDR4S6_LBEiqvtfgGjXYSkYaI62eLGAU,1099
+dj_database_url-0.5.0.dist-info/top_level.txt,sha256=pntpOZz98RuKrz2758NE7pVfM4EG6zO_0IBiYUrqS94,16
+dj_database_url.py,sha256=y7UeNL4IVpnFP8U0ZJyAUxCkdPjavIm1dSZCM0siiVU,4446
Index: venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/top_level.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/top_level.txt b/venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/top_level.txt
new file mode 100644
--- /dev/null	(date 1630065394583)
+++ b/venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/top_level.txt	(date 1630065394583)
@@ -0,0 +1,1 @@
+dj_database_url
Index: venv/Lib/site-packages/gunicorn/workers/gtornado.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/workers/gtornado.py b/venv/Lib/site-packages/gunicorn/workers/gtornado.py
new file mode 100644
--- /dev/null	(date 1630065626435)
+++ b/venv/Lib/site-packages/gunicorn/workers/gtornado.py	(date 1630065626435)
@@ -0,0 +1,171 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+import copy
+import os
+import sys
+
+try:
+    import tornado
+except ImportError:
+    raise RuntimeError("You need tornado installed to use this worker.")
+import tornado.web
+import tornado.httpserver
+from tornado.ioloop import IOLoop, PeriodicCallback
+from tornado.wsgi import WSGIContainer
+from gunicorn.workers.base import Worker
+from gunicorn import __version__ as gversion
+
+
+# Tornado 5.0 updated its IOLoop, and the `io_loop` arguments to many
+# Tornado functions have been removed in Tornado 5.0. Also, they no
+# longer store PeriodCallbacks in ioloop._callbacks. Instead we store
+# them on our side, and use stop() on them when stopping the worker.
+# See https://www.tornadoweb.org/en/stable/releases/v5.0.0.html#backwards-compatibility-notes
+# for more details.
+TORNADO5 = tornado.version_info >= (5, 0, 0)
+
+
+class TornadoWorker(Worker):
+
+    @classmethod
+    def setup(cls):
+        web = sys.modules.pop("tornado.web")
+        old_clear = web.RequestHandler.clear
+
+        def clear(self):
+            old_clear(self)
+            if "Gunicorn" not in self._headers["Server"]:
+                self._headers["Server"] += " (Gunicorn/%s)" % gversion
+        web.RequestHandler.clear = clear
+        sys.modules["tornado.web"] = web
+
+    def handle_exit(self, sig, frame):
+        if self.alive:
+            super().handle_exit(sig, frame)
+
+    def handle_request(self):
+        self.nr += 1
+        if self.alive and self.nr >= self.max_requests:
+            self.log.info("Autorestarting worker after current request.")
+            self.alive = False
+
+    def watchdog(self):
+        if self.alive:
+            self.notify()
+
+        if self.ppid != os.getppid():
+            self.log.info("Parent changed, shutting down: %s", self)
+            self.alive = False
+
+    def heartbeat(self):
+        if not self.alive:
+            if self.server_alive:
+                if hasattr(self, 'server'):
+                    try:
+                        self.server.stop()
+                    except Exception:
+                        pass
+                self.server_alive = False
+            else:
+                if TORNADO5:
+                    for callback in self.callbacks:
+                        callback.stop()
+                    self.ioloop.stop()
+                else:
+                    if not self.ioloop._callbacks:
+                        self.ioloop.stop()
+
+    def init_process(self):
+        # IOLoop cannot survive a fork or be shared across processes
+        # in any way. When multiple processes are being used, each process
+        # should create its own IOLoop. We should clear current IOLoop
+        # if exists before os.fork.
+        IOLoop.clear_current()
+        super().init_process()
+
+    def run(self):
+        self.ioloop = IOLoop.instance()
+        self.alive = True
+        self.server_alive = False
+
+        if TORNADO5:
+            self.callbacks = []
+            self.callbacks.append(PeriodicCallback(self.watchdog, 1000))
+            self.callbacks.append(PeriodicCallback(self.heartbeat, 1000))
+            for callback in self.callbacks:
+                callback.start()
+        else:
+            PeriodicCallback(self.watchdog, 1000, io_loop=self.ioloop).start()
+            PeriodicCallback(self.heartbeat, 1000, io_loop=self.ioloop).start()
+
+        # Assume the app is a WSGI callable if its not an
+        # instance of tornado.web.Application or is an
+        # instance of tornado.wsgi.WSGIApplication
+        app = self.wsgi
+
+        if tornado.version_info[0] < 6:
+            if not isinstance(app, tornado.web.Application) or \
+            isinstance(app, tornado.wsgi.WSGIApplication):
+                app = WSGIContainer(app)
+        elif not isinstance(app, WSGIContainer):
+            app = WSGIContainer(app)
+
+        # Monkey-patching HTTPConnection.finish to count the
+        # number of requests being handled by Tornado. This
+        # will help gunicorn shutdown the worker if max_requests
+        # is exceeded.
+        httpserver = sys.modules["tornado.httpserver"]
+        if hasattr(httpserver, 'HTTPConnection'):
+            old_connection_finish = httpserver.HTTPConnection.finish
+
+            def finish(other):
+                self.handle_request()
+                old_connection_finish(other)
+            httpserver.HTTPConnection.finish = finish
+            sys.modules["tornado.httpserver"] = httpserver
+
+            server_class = tornado.httpserver.HTTPServer
+        else:
+
+            class _HTTPServer(tornado.httpserver.HTTPServer):
+
+                def on_close(instance, server_conn):
+                    self.handle_request()
+                    super(_HTTPServer, instance).on_close(server_conn)
+
+            server_class = _HTTPServer
+
+        if self.cfg.is_ssl:
+            _ssl_opt = copy.deepcopy(self.cfg.ssl_options)
+            # tornado refuses initialization if ssl_options contains following
+            # options
+            del _ssl_opt["do_handshake_on_connect"]
+            del _ssl_opt["suppress_ragged_eofs"]
+            if TORNADO5:
+                server = server_class(app, ssl_options=_ssl_opt)
+            else:
+                server = server_class(app, io_loop=self.ioloop,
+                                      ssl_options=_ssl_opt)
+        else:
+            if TORNADO5:
+                server = server_class(app)
+            else:
+                server = server_class(app, io_loop=self.ioloop)
+
+        self.server = server
+        self.server_alive = True
+
+        for s in self.sockets:
+            s.setblocking(0)
+            if hasattr(server, "add_socket"):  # tornado > 2.0
+                server.add_socket(s)
+            elif hasattr(server, "_sockets"):  # tornado 2.0
+                server._sockets[s.fileno()] = s
+
+        server.no_keep_alive = self.cfg.keepalive <= 0
+        server.start(num_processes=1)
+
+        self.ioloop.start()
Index: venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/METADATA
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/METADATA b/venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/METADATA
new file mode 100644
--- /dev/null	(date 1630065394589)
+++ b/venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/METADATA	(date 1630065394589)
@@ -0,0 +1,81 @@
+Metadata-Version: 2.0
+Name: dj-database-url
+Version: 0.5.0
+Summary: Use Database URLs in your Django Application.
+Home-page: https://github.com/kennethreitz/dj-database-url
+Author: Kenneth Reitz
+Author-email: me@kennethreitz.com
+License: BSD
+Description-Content-Type: UNKNOWN
+Platform: any
+Classifier: Environment :: Web Environment
+Classifier: Intended Audience :: Developers
+Classifier: License :: OSI Approved :: BSD License
+Classifier: Operating System :: OS Independent
+Classifier: Programming Language :: Python
+Classifier: Topic :: Internet :: WWW/HTTP :: Dynamic Content
+Classifier: Topic :: Software Development :: Libraries :: Python Modules
+Classifier: Programming Language :: Python
+Classifier: Programming Language :: Python :: 2.7
+Classifier: Programming Language :: Python :: 3
+Classifier: Programming Language :: Python :: 3.2
+Classifier: Programming Language :: Python :: 3.3
+Classifier: Programming Language :: Python :: 3.4
+Classifier: Programming Language :: Python :: 3.5
+
+
+dj-database-url
+~~~~~~~~~~~~~~~
+
+.. image:: https://secure.travis-ci.org/kennethreitz/dj-database-url.png?branch=master
+   :target: http://travis-ci.org/kennethreitz/dj-database-url
+
+This simple Django utility allows you to utilize the
+`12factor <http://www.12factor.net/backing-services>`_ inspired
+``DATABASE_URL`` environment variable to configure your Django application.
+
+The ``dj_database_url.config`` method returns a Django database connection
+dictionary, populated with all the data specified in your URL. There is
+also a `conn_max_age` argument to easily enable Django's connection pool.
+
+If you'd rather not use an environment variable, you can pass a URL in directly
+instead to ``dj_database_url.parse``.
+
+Supported Databases
+-------------------
+
+Support currently exists for PostgreSQL, PostGIS, MySQL, MySQL (GIS),
+Oracle, Oracle (GIS), and SQLite.
+
+Installation
+------------
+
+Installation is simple::
+
+    $ pip install dj-database-url
+
+Usage
+-----
+
+Configure your database in ``settings.py`` from ``DATABASE_URL``::
+
+    import dj_database_url
+
+    DATABASES['default'] = dj_database_url.config(conn_max_age=600, ssl_require=True)
+
+Provide a default::
+
+    DATABASES['default'] = dj_database_url.config(default='postgres://...'}
+
+Parse an arbitrary Database URL::
+
+    DATABASES['default'] = dj_database_url.parse('postgres://...', conn_max_age=600)
+
+The ``conn_max_age`` attribute is the lifetime of a database connection in seconds
+and is available in Django 1.6+. If you do not set a value, it will default to ``0``
+which is Django's historical behavior of using a new database connection on each
+request. Use ``None`` for unlimited persistent connections.
+
+
+
+
Index: venv/Lib/site-packages/gunicorn/workers/geventlet.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/workers/geventlet.py b/venv/Lib/site-packages/gunicorn/workers/geventlet.py
new file mode 100644
--- /dev/null	(date 1630065626424)
+++ b/venv/Lib/site-packages/gunicorn/workers/geventlet.py	(date 1630065626424)
@@ -0,0 +1,179 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+from functools import partial
+import sys
+
+try:
+    import eventlet
+except ImportError:
+    raise RuntimeError("eventlet worker requires eventlet 0.24.1 or higher")
+else:
+    from pkg_resources import parse_version
+    if parse_version(eventlet.__version__) < parse_version('0.24.1'):
+        raise RuntimeError("eventlet worker requires eventlet 0.24.1 or higher")
+
+from eventlet import hubs, greenthread
+from eventlet.greenio import GreenSocket
+from eventlet.wsgi import ALREADY_HANDLED as EVENTLET_ALREADY_HANDLED
+import greenlet
+
+from gunicorn.workers.base_async import AsyncWorker
+
+
+def _eventlet_socket_sendfile(self, file, offset=0, count=None):
+    # Based on the implementation in gevent which in turn is slightly
+    # modified from the standard library implementation.
+    if self.gettimeout() == 0:
+        raise ValueError("non-blocking sockets are not supported")
+    if offset:
+        file.seek(offset)
+    blocksize = min(count, 8192) if count else 8192
+    total_sent = 0
+    # localize variable access to minimize overhead
+    file_read = file.read
+    sock_send = self.send
+    try:
+        while True:
+            if count:
+                blocksize = min(count - total_sent, blocksize)
+                if blocksize <= 0:
+                    break
+            data = memoryview(file_read(blocksize))
+            if not data:
+                break  # EOF
+            while True:
+                try:
+                    sent = sock_send(data)
+                except BlockingIOError:
+                    continue
+                else:
+                    total_sent += sent
+                    if sent < len(data):
+                        data = data[sent:]
+                    else:
+                        break
+        return total_sent
+    finally:
+        if total_sent > 0 and hasattr(file, 'seek'):
+            file.seek(offset + total_sent)
+
+
+
+def _eventlet_serve(sock, handle, concurrency):
+    """
+    Serve requests forever.
+
+    This code is nearly identical to ``eventlet.convenience.serve`` except
+    that it attempts to join the pool at the end, which allows for gunicorn
+    graceful shutdowns.
+    """
+    pool = eventlet.greenpool.GreenPool(concurrency)
+    server_gt = eventlet.greenthread.getcurrent()
+
+    while True:
+        try:
+            conn, addr = sock.accept()
+            gt = pool.spawn(handle, conn, addr)
+            gt.link(_eventlet_stop, server_gt, conn)
+            conn, addr, gt = None, None, None
+        except eventlet.StopServe:
+            sock.close()
+            pool.waitall()
+            return
+
+
+def _eventlet_stop(client, server, conn):
+    """
+    Stop a greenlet handling a request and close its connection.
+
+    This code is lifted from eventlet so as not to depend on undocumented
+    functions in the library.
+    """
+    try:
+        try:
+            client.wait()
+        finally:
+            conn.close()
+    except greenlet.GreenletExit:
+        pass
+    except Exception:
+        greenthread.kill(server, *sys.exc_info())
+
+
+def patch_sendfile():
+    # As of eventlet 0.25.1, GreenSocket.sendfile doesn't exist,
+    # meaning the native implementations of socket.sendfile will be used.
+    # If os.sendfile exists, it will attempt to use that, failing explicitly
+    # if the socket is in non-blocking mode, which the underlying
+    # socket object /is/. Even the regular _sendfile_use_send will
+    # fail in that way; plus, it would use the underlying socket.send which isn't
+    # properly cooperative. So we have to monkey-patch a working socket.sendfile()
+    # into GreenSocket; in this method, `self.send` will be the GreenSocket's
+    # send method which is properly cooperative.
+    if not hasattr(GreenSocket, 'sendfile'):
+        GreenSocket.sendfile = _eventlet_socket_sendfile
+
+
+class EventletWorker(AsyncWorker):
+
+    def patch(self):
+        hubs.use_hub()
+        eventlet.monkey_patch()
+        patch_sendfile()
+
+    def is_already_handled(self, respiter):
+        if respiter == EVENTLET_ALREADY_HANDLED:
+            raise StopIteration()
+        return super().is_already_handled(respiter)
+
+    def init_process(self):
+        self.patch()
+        super().init_process()
+
+    def handle_quit(self, sig, frame):
+        eventlet.spawn(super().handle_quit, sig, frame)
+
+    def handle_usr1(self, sig, frame):
+        eventlet.spawn(super().handle_usr1, sig, frame)
+
+    def timeout_ctx(self):
+        return eventlet.Timeout(self.cfg.keepalive or None, False)
+
+    def handle(self, listener, client, addr):
+        if self.cfg.is_ssl:
+            client = eventlet.wrap_ssl(client, server_side=True,
+                                       **self.cfg.ssl_options)
+
+        super().handle(listener, client, addr)
+
+    def run(self):
+        acceptors = []
+        for sock in self.sockets:
+            gsock = GreenSocket(sock)
+            gsock.setblocking(1)
+            hfun = partial(self.handle, gsock)
+            acceptor = eventlet.spawn(_eventlet_serve, gsock, hfun,
+                                      self.worker_connections)
+
+            acceptors.append(acceptor)
+            eventlet.sleep(0.0)
+
+        while self.alive:
+            self.notify()
+            eventlet.sleep(1.0)
+
+        self.notify()
+        try:
+            with eventlet.Timeout(self.cfg.graceful_timeout) as t:
+                for a in acceptors:
+                    a.kill(eventlet.StopServe())
+                for a in acceptors:
+                    a.wait()
+        except eventlet.Timeout as te:
+            if te != t:
+                raise
+            for a in acceptors:
+                a.kill()
Index: venv/Lib/site-packages/gunicorn/workers/ggevent.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/workers/ggevent.py b/venv/Lib/site-packages/gunicorn/workers/ggevent.py
new file mode 100644
--- /dev/null	(date 1630065626429)
+++ b/venv/Lib/site-packages/gunicorn/workers/ggevent.py	(date 1630065626429)
@@ -0,0 +1,189 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+import os
+import sys
+from datetime import datetime
+from functools import partial
+import time
+
+try:
+    import gevent
+except ImportError:
+    raise RuntimeError("gevent worker requires gevent 1.4 or higher")
+else:
+    from pkg_resources import parse_version
+    if parse_version(gevent.__version__) < parse_version('1.4'):
+        raise RuntimeError("gevent worker requires gevent 1.4 or higher")
+
+from gevent.pool import Pool
+from gevent.server import StreamServer
+from gevent import hub, monkey, socket, pywsgi
+
+import gunicorn
+from gunicorn.http.wsgi import base_environ
+from gunicorn.workers.base_async import AsyncWorker
+
+VERSION = "gevent/%s gunicorn/%s" % (gevent.__version__, gunicorn.__version__)
+
+
+class GeventWorker(AsyncWorker):
+
+    server_class = None
+    wsgi_handler = None
+
+    def patch(self):
+        monkey.patch_all()
+
+        # patch sockets
+        sockets = []
+        for s in self.sockets:
+            sockets.append(socket.socket(s.FAMILY, socket.SOCK_STREAM,
+                fileno=s.sock.fileno()))
+        self.sockets = sockets
+
+    def notify(self):
+        super().notify()
+        if self.ppid != os.getppid():
+            self.log.info("Parent changed, shutting down: %s", self)
+            sys.exit(0)
+
+    def timeout_ctx(self):
+        return gevent.Timeout(self.cfg.keepalive, False)
+
+    def run(self):
+        servers = []
+        ssl_args = {}
+
+        if self.cfg.is_ssl:
+            ssl_args = dict(server_side=True, **self.cfg.ssl_options)
+
+        for s in self.sockets:
+            s.setblocking(1)
+            pool = Pool(self.worker_connections)
+            if self.server_class is not None:
+                environ = base_environ(self.cfg)
+                environ.update({
+                    "wsgi.multithread": True,
+                    "SERVER_SOFTWARE": VERSION,
+                })
+                server = self.server_class(
+                    s, application=self.wsgi, spawn=pool, log=self.log,
+                    handler_class=self.wsgi_handler, environ=environ,
+                    **ssl_args)
+            else:
+                hfun = partial(self.handle, s)
+                server = StreamServer(s, handle=hfun, spawn=pool, **ssl_args)
+                if self.cfg.workers > 1:
+                    server.max_accept = 1
+
+            server.start()
+            servers.append(server)
+
+        while self.alive:
+            self.notify()
+            gevent.sleep(1.0)
+
+        try:
+            # Stop accepting requests
+            for server in servers:
+                if hasattr(server, 'close'):  # gevent 1.0
+                    server.close()
+                if hasattr(server, 'kill'):  # gevent < 1.0
+                    server.kill()
+
+            # Handle current requests until graceful_timeout
+            ts = time.time()
+            while time.time() - ts <= self.cfg.graceful_timeout:
+                accepting = 0
+                for server in servers:
+                    if server.pool.free_count() != server.pool.size:
+                        accepting += 1
+
+                # if no server is accepting a connection, we can exit
+                if not accepting:
+                    return
+
+                self.notify()
+                gevent.sleep(1.0)
+
+            # Force kill all active the handlers
+            self.log.warning("Worker graceful timeout (pid:%s)" % self.pid)
+            for server in servers:
+                server.stop(timeout=1)
+        except Exception:
+            pass
+
+    def handle(self, listener, client, addr):
+        # Connected socket timeout defaults to socket.getdefaulttimeout().
+        # This forces to blocking mode.
+        client.setblocking(1)
+        super().handle(listener, client, addr)
+
+    def handle_request(self, listener_name, req, sock, addr):
+        try:
+            super().handle_request(listener_name, req, sock, addr)
+        except gevent.GreenletExit:
+            pass
+        except SystemExit:
+            pass
+
+    def handle_quit(self, sig, frame):
+        # Move this out of the signal handler so we can use
+        # blocking calls. See #1126
+        gevent.spawn(super().handle_quit, sig, frame)
+
+    def handle_usr1(self, sig, frame):
+        # Make the gevent workers handle the usr1 signal
+        # by deferring to a new greenlet. See #1645
+        gevent.spawn(super().handle_usr1, sig, frame)
+
+    def init_process(self):
+        self.patch()
+        hub.reinit()
+        super().init_process()
+
+
+class GeventResponse(object):
+
+    status = None
+    headers = None
+    sent = None
+
+    def __init__(self, status, headers, clength):
+        self.status = status
+        self.headers = headers
+        self.sent = clength
+
+
+class PyWSGIHandler(pywsgi.WSGIHandler):
+
+    def log_request(self):
+        start = datetime.fromtimestamp(self.time_start)
+        finish = datetime.fromtimestamp(self.time_finish)
+        response_time = finish - start
+        resp_headers = getattr(self, 'response_headers', {})
+        resp = GeventResponse(self.status, resp_headers, self.response_length)
+        if hasattr(self, 'headers'):
+            req_headers = self.headers.items()
+        else:
+            req_headers = []
+        self.server.log.access(resp, req_headers, self.environ, response_time)
+
+    def get_environ(self):
+        env = super().get_environ()
+        env['gunicorn.sock'] = self.socket
+        env['RAW_URI'] = self.path
+        return env
+
+
+class PyWSGIServer(pywsgi.WSGIServer):
+    pass
+
+
+class GeventPyWSGIWorker(GeventWorker):
+    "The Gevent StreamServer based workers."
+    server_class = PyWSGIServer
+    wsgi_handler = PyWSGIHandler
Index: venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/metadata.json
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/metadata.json b/venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/metadata.json
new file mode 100644
--- /dev/null	(date 1630065394580)
+++ b/venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/metadata.json	(date 1630065394580)
@@ -0,0 +1,1 @@
+{"classifiers": ["Environment :: Web Environment", "Intended Audience :: Developers", "License :: OSI Approved :: BSD License", "Operating System :: OS Independent", "Programming Language :: Python", "Topic :: Internet :: WWW/HTTP :: Dynamic Content", "Topic :: Software Development :: Libraries :: Python Modules", "Programming Language :: Python", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.2", "Programming Language :: Python :: 3.3", "Programming Language :: Python :: 3.4", "Programming Language :: Python :: 3.5"], "description_content_type": "UNKNOWN", "extensions": {"python.details": {"contacts": [{"email": "me@kennethreitz.com", "name": "Kenneth Reitz", "role": "author"}], "document_names": {"description": "DESCRIPTION.rst"}, "project_urls": {"Home": "https://github.com/kennethreitz/dj-database-url"}}}, "generator": "bdist_wheel (0.30.0)", "license": "BSD", "metadata_version": "2.0", "name": "dj-database-url", "platform": "any", "summary": "Use Database URLs in your Django Application.", "version": "0.5.0"}
\ No newline at end of file
Index: venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/DESCRIPTION.rst
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/DESCRIPTION.rst b/venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/DESCRIPTION.rst
new file mode 100644
--- /dev/null	(date 1630065394578)
+++ b/venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/DESCRIPTION.rst	(date 1630065394578)
@@ -0,0 +1,56 @@
+
+dj-database-url
+~~~~~~~~~~~~~~~
+
+.. image:: https://secure.travis-ci.org/kennethreitz/dj-database-url.png?branch=master
+   :target: http://travis-ci.org/kennethreitz/dj-database-url
+
+This simple Django utility allows you to utilize the
+`12factor <http://www.12factor.net/backing-services>`_ inspired
+``DATABASE_URL`` environment variable to configure your Django application.
+
+The ``dj_database_url.config`` method returns a Django database connection
+dictionary, populated with all the data specified in your URL. There is
+also a `conn_max_age` argument to easily enable Django's connection pool.
+
+If you'd rather not use an environment variable, you can pass a URL in directly
+instead to ``dj_database_url.parse``.
+
+Supported Databases
+-------------------
+
+Support currently exists for PostgreSQL, PostGIS, MySQL, MySQL (GIS),
+Oracle, Oracle (GIS), and SQLite.
+
+Installation
+------------
+
+Installation is simple::
+
+    $ pip install dj-database-url
+
+Usage
+-----
+
+Configure your database in ``settings.py`` from ``DATABASE_URL``::
+
+    import dj_database_url
+
+    DATABASES['default'] = dj_database_url.config(conn_max_age=600, ssl_require=True)
+
+Provide a default::
+
+    DATABASES['default'] = dj_database_url.config(default='postgres://...'}
+
+Parse an arbitrary Database URL::
+
+    DATABASES['default'] = dj_database_url.parse('postgres://...', conn_max_age=600)
+
+The ``conn_max_age`` attribute is the lifetime of a database connection in seconds
+and is available in Django 1.6+. If you do not set a value, it will default to ``0``
+which is Django's historical behavior of using a new database connection on each
+request. Use ``None`` for unlimited persistent connections.
+
+
+
+
Index: venv/Lib/site-packages/gunicorn/workers/base.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/workers/base.py b/venv/Lib/site-packages/gunicorn/workers/base.py
new file mode 100644
--- /dev/null	(date 1630065626417)
+++ b/venv/Lib/site-packages/gunicorn/workers/base.py	(date 1630065626417)
@@ -0,0 +1,273 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+import io
+import os
+import signal
+import sys
+import time
+import traceback
+from datetime import datetime
+from random import randint
+from ssl import SSLError
+
+from gunicorn import util
+from gunicorn.http.errors import (
+    ForbiddenProxyRequest, InvalidHeader,
+    InvalidHeaderName, InvalidHTTPVersion,
+    InvalidProxyLine, InvalidRequestLine,
+    InvalidRequestMethod, InvalidSchemeHeaders,
+    LimitRequestHeaders, LimitRequestLine,
+)
+from gunicorn.http.wsgi import Response, default_environ
+from gunicorn.reloader import reloader_engines
+from gunicorn.workers.workertmp import WorkerTmp
+
+
+class Worker(object):
+
+    SIGNALS = [getattr(signal, "SIG%s" % x) for x in (
+        "ABRT HUP QUIT INT TERM USR1 USR2 WINCH CHLD".split()
+    )]
+
+    PIPE = []
+
+    def __init__(self, age, ppid, sockets, app, timeout, cfg, log):
+        """\
+        This is called pre-fork so it shouldn't do anything to the
+        current process. If there's a need to make process wide
+        changes you'll want to do that in ``self.init_process()``.
+        """
+        self.age = age
+        self.pid = "[booting]"
+        self.ppid = ppid
+        self.sockets = sockets
+        self.app = app
+        self.timeout = timeout
+        self.cfg = cfg
+        self.booted = False
+        self.aborted = False
+        self.reloader = None
+
+        self.nr = 0
+
+        if cfg.max_requests > 0:
+            jitter = randint(0, cfg.max_requests_jitter)
+            self.max_requests = cfg.max_requests + jitter
+        else:
+            self.max_requests = sys.maxsize
+
+        self.alive = True
+        self.log = log
+        self.tmp = WorkerTmp(cfg)
+
+    def __str__(self):
+        return "<Worker %s>" % self.pid
+
+    def notify(self):
+        """\
+        Your worker subclass must arrange to have this method called
+        once every ``self.timeout`` seconds. If you fail in accomplishing
+        this task, the master process will murder your workers.
+        """
+        self.tmp.notify()
+
+    def run(self):
+        """\
+        This is the mainloop of a worker process. You should override
+        this method in a subclass to provide the intended behaviour
+        for your particular evil schemes.
+        """
+        raise NotImplementedError()
+
+    def init_process(self):
+        """\
+        If you override this method in a subclass, the last statement
+        in the function should be to call this method with
+        super().init_process() so that the ``run()`` loop is initiated.
+        """
+
+        # set environment' variables
+        if self.cfg.env:
+            for k, v in self.cfg.env.items():
+                os.environ[k] = v
+
+        util.set_owner_process(self.cfg.uid, self.cfg.gid,
+                               initgroups=self.cfg.initgroups)
+
+        # Reseed the random number generator
+        util.seed()
+
+        # For waking ourselves up
+        self.PIPE = os.pipe()
+        for p in self.PIPE:
+            util.set_non_blocking(p)
+            util.close_on_exec(p)
+
+        # Prevent fd inheritance
+        for s in self.sockets:
+            util.close_on_exec(s)
+        util.close_on_exec(self.tmp.fileno())
+
+        self.wait_fds = self.sockets + [self.PIPE[0]]
+
+        self.log.close_on_exec()
+
+        self.init_signals()
+
+        # start the reloader
+        if self.cfg.reload:
+            def changed(fname):
+                self.log.info("Worker reloading: %s modified", fname)
+                self.alive = False
+                os.write(self.PIPE[1], b"1")
+                self.cfg.worker_int(self)
+                time.sleep(0.1)
+                sys.exit(0)
+
+            reloader_cls = reloader_engines[self.cfg.reload_engine]
+            self.reloader = reloader_cls(extra_files=self.cfg.reload_extra_files,
+                                         callback=changed)
+
+        self.load_wsgi()
+        if self.reloader:
+            self.reloader.start()
+
+        self.cfg.post_worker_init(self)
+
+        # Enter main run loop
+        self.booted = True
+        self.run()
+
+    def load_wsgi(self):
+        try:
+            self.wsgi = self.app.wsgi()
+        except SyntaxError as e:
+            if not self.cfg.reload:
+                raise
+
+            self.log.exception(e)
+
+            # fix from PR #1228
+            # storing the traceback into exc_tb will create a circular reference.
+            # per https://docs.python.org/2/library/sys.html#sys.exc_info warning,
+            # delete the traceback after use.
+            try:
+                _, exc_val, exc_tb = sys.exc_info()
+                self.reloader.add_extra_file(exc_val.filename)
+
+                tb_string = io.StringIO()
+                traceback.print_tb(exc_tb, file=tb_string)
+                self.wsgi = util.make_fail_app(tb_string.getvalue())
+            finally:
+                del exc_tb
+
+    def init_signals(self):
+        # reset signaling
+        for s in self.SIGNALS:
+            signal.signal(s, signal.SIG_DFL)
+        # init new signaling
+        signal.signal(signal.SIGQUIT, self.handle_quit)
+        signal.signal(signal.SIGTERM, self.handle_exit)
+        signal.signal(signal.SIGINT, self.handle_quit)
+        signal.signal(signal.SIGWINCH, self.handle_winch)
+        signal.signal(signal.SIGUSR1, self.handle_usr1)
+        signal.signal(signal.SIGABRT, self.handle_abort)
+
+        # Don't let SIGTERM and SIGUSR1 disturb active requests
+        # by interrupting system calls
+        signal.siginterrupt(signal.SIGTERM, False)
+        signal.siginterrupt(signal.SIGUSR1, False)
+
+        if hasattr(signal, 'set_wakeup_fd'):
+            signal.set_wakeup_fd(self.PIPE[1])
+
+    def handle_usr1(self, sig, frame):
+        self.log.reopen_files()
+
+    def handle_exit(self, sig, frame):
+        self.alive = False
+
+    def handle_quit(self, sig, frame):
+        self.alive = False
+        # worker_int callback
+        self.cfg.worker_int(self)
+        time.sleep(0.1)
+        sys.exit(0)
+
+    def handle_abort(self, sig, frame):
+        self.alive = False
+        self.cfg.worker_abort(self)
+        sys.exit(1)
+
+    def handle_error(self, req, client, addr, exc):
+        request_start = datetime.now()
+        addr = addr or ('', -1)  # unix socket case
+        if isinstance(exc, (
+            InvalidRequestLine, InvalidRequestMethod,
+            InvalidHTTPVersion, InvalidHeader, InvalidHeaderName,
+            LimitRequestLine, LimitRequestHeaders,
+            InvalidProxyLine, ForbiddenProxyRequest,
+            InvalidSchemeHeaders,
+            SSLError,
+        )):
+
+            status_int = 400
+            reason = "Bad Request"
+
+            if isinstance(exc, InvalidRequestLine):
+                mesg = "Invalid Request Line '%s'" % str(exc)
+            elif isinstance(exc, InvalidRequestMethod):
+                mesg = "Invalid Method '%s'" % str(exc)
+            elif isinstance(exc, InvalidHTTPVersion):
+                mesg = "Invalid HTTP Version '%s'" % str(exc)
+            elif isinstance(exc, (InvalidHeaderName, InvalidHeader,)):
+                mesg = "%s" % str(exc)
+                if not req and hasattr(exc, "req"):
+                    req = exc.req  # for access log
+            elif isinstance(exc, LimitRequestLine):
+                mesg = "%s" % str(exc)
+            elif isinstance(exc, LimitRequestHeaders):
+                mesg = "Error parsing headers: '%s'" % str(exc)
+            elif isinstance(exc, InvalidProxyLine):
+                mesg = "'%s'" % str(exc)
+            elif isinstance(exc, ForbiddenProxyRequest):
+                reason = "Forbidden"
+                mesg = "Request forbidden"
+                status_int = 403
+            elif isinstance(exc, InvalidSchemeHeaders):
+                mesg = "%s" % str(exc)
+            elif isinstance(exc, SSLError):
+                reason = "Forbidden"
+                mesg = "'%s'" % str(exc)
+                status_int = 403
+
+            msg = "Invalid request from ip={ip}: {error}"
+            self.log.debug(msg.format(ip=addr[0], error=str(exc)))
+        else:
+            if hasattr(req, "uri"):
+                self.log.exception("Error handling request %s", req.uri)
+            status_int = 500
+            reason = "Internal Server Error"
+            mesg = ""
+
+        if req is not None:
+            request_time = datetime.now() - request_start
+            environ = default_environ(req, client, self.cfg)
+            environ['REMOTE_ADDR'] = addr[0]
+            environ['REMOTE_PORT'] = str(addr[1])
+            resp = Response(req, client, self.cfg)
+            resp.status = "%s %s" % (status_int, reason)
+            resp.response_length = len(mesg)
+            self.log.access(resp, req, environ, request_time)
+
+        try:
+            util.write_error(client, status_int, reason, mesg)
+        except Exception:
+            self.log.debug("Failed to send error message.")
+
+    def handle_winch(self, sig, fname):
+        # Ignore SIGWINCH in worker. Fixes a crash on OpenBSD.
+        self.log.debug("worker: SIGWINCH ignored.")
Index: venv/Lib/site-packages/gunicorn/workers/base_async.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/workers/base_async.py b/venv/Lib/site-packages/gunicorn/workers/base_async.py
new file mode 100644
--- /dev/null	(date 1630065626420)
+++ b/venv/Lib/site-packages/gunicorn/workers/base_async.py	(date 1630065626420)
@@ -0,0 +1,148 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+from datetime import datetime
+import errno
+import socket
+import ssl
+import sys
+
+import gunicorn.http as http
+import gunicorn.http.wsgi as wsgi
+import gunicorn.util as util
+import gunicorn.workers.base as base
+
+ALREADY_HANDLED = object()
+
+
+class AsyncWorker(base.Worker):
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.worker_connections = self.cfg.worker_connections
+
+    def timeout_ctx(self):
+        raise NotImplementedError()
+
+    def is_already_handled(self, respiter):
+        # some workers will need to overload this function to raise a StopIteration
+        return respiter == ALREADY_HANDLED
+
+    def handle(self, listener, client, addr):
+        req = None
+        try:
+            parser = http.RequestParser(self.cfg, client, addr)
+            try:
+                listener_name = listener.getsockname()
+                if not self.cfg.keepalive:
+                    req = next(parser)
+                    self.handle_request(listener_name, req, client, addr)
+                else:
+                    # keepalive loop
+                    proxy_protocol_info = {}
+                    while True:
+                        req = None
+                        with self.timeout_ctx():
+                            req = next(parser)
+                        if not req:
+                            break
+                        if req.proxy_protocol_info:
+                            proxy_protocol_info = req.proxy_protocol_info
+                        else:
+                            req.proxy_protocol_info = proxy_protocol_info
+                        self.handle_request(listener_name, req, client, addr)
+            except http.errors.NoMoreData as e:
+                self.log.debug("Ignored premature client disconnection. %s", e)
+            except StopIteration as e:
+                self.log.debug("Closing connection. %s", e)
+            except ssl.SSLError:
+                # pass to next try-except level
+                util.reraise(*sys.exc_info())
+            except EnvironmentError:
+                # pass to next try-except level
+                util.reraise(*sys.exc_info())
+            except Exception as e:
+                self.handle_error(req, client, addr, e)
+        except ssl.SSLError as e:
+            if e.args[0] == ssl.SSL_ERROR_EOF:
+                self.log.debug("ssl connection closed")
+                client.close()
+            else:
+                self.log.debug("Error processing SSL request.")
+                self.handle_error(req, client, addr, e)
+        except EnvironmentError as e:
+            if e.errno not in (errno.EPIPE, errno.ECONNRESET, errno.ENOTCONN):
+                self.log.exception("Socket error processing request.")
+            else:
+                if e.errno == errno.ECONNRESET:
+                    self.log.debug("Ignoring connection reset")
+                elif e.errno == errno.ENOTCONN:
+                    self.log.debug("Ignoring socket not connected")
+                else:
+                    self.log.debug("Ignoring EPIPE")
+        except Exception as e:
+            self.handle_error(req, client, addr, e)
+        finally:
+            util.close(client)
+
+    def handle_request(self, listener_name, req, sock, addr):
+        request_start = datetime.now()
+        environ = {}
+        resp = None
+        try:
+            self.cfg.pre_request(self, req)
+            resp, environ = wsgi.create(req, sock, addr,
+                                        listener_name, self.cfg)
+            environ["wsgi.multithread"] = True
+            self.nr += 1
+            if self.nr >= self.max_requests:
+                if self.alive:
+                    self.log.info("Autorestarting worker after current request.")
+                    self.alive = False
+
+            if not self.alive or not self.cfg.keepalive:
+                resp.force_close()
+
+            respiter = self.wsgi(environ, resp.start_response)
+            if self.is_already_handled(respiter):
+                return False
+            try:
+                if isinstance(respiter, environ['wsgi.file_wrapper']):
+                    resp.write_file(respiter)
+                else:
+                    for item in respiter:
+                        resp.write(item)
+                resp.close()
+                request_time = datetime.now() - request_start
+                self.log.access(resp, req, environ, request_time)
+            finally:
+                if hasattr(respiter, "close"):
+                    respiter.close()
+            if resp.should_close():
+                raise StopIteration()
+        except StopIteration:
+            raise
+        except EnvironmentError:
+            # If the original exception was a socket.error we delegate
+            # handling it to the caller (where handle() might ignore it)
+            util.reraise(*sys.exc_info())
+        except Exception:
+            if resp and resp.headers_sent:
+                # If the requests have already been sent, we should close the
+                # connection to indicate the error.
+                self.log.exception("Error handling request")
+                try:
+                    sock.shutdown(socket.SHUT_RDWR)
+                    sock.close()
+                except EnvironmentError:
+                    pass
+                raise StopIteration()
+            raise
+        finally:
+            try:
+                self.cfg.post_request(self, req, environ, resp)
+            except Exception:
+                self.log.exception("Exception in post_request hook")
+        return True
Index: venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/INSTALLER
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/INSTALLER b/venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/INSTALLER
new file mode 100644
--- /dev/null	(date 1630065394610)
+++ b/venv/Lib/site-packages/dj_database_url-0.5.0.dist-info/INSTALLER	(date 1630065394610)
@@ -0,0 +1,1 @@
+pip
Index: venv/Lib/site-packages/gunicorn/http/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/http/__init__.py b/venv/Lib/site-packages/gunicorn/http/__init__.py
new file mode 100644
--- /dev/null	(date 1630065626377)
+++ b/venv/Lib/site-packages/gunicorn/http/__init__.py	(date 1630065626377)
@@ -0,0 +1,9 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+from gunicorn.http.message import Message, Request
+from gunicorn.http.parser import RequestParser
+
+__all__ = ['Message', 'Request', 'RequestParser']
Index: venv/Lib/site-packages/gunicorn/http/unreader.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/http/unreader.py b/venv/Lib/site-packages/gunicorn/http/unreader.py
new file mode 100644
--- /dev/null	(date 1630065626393)
+++ b/venv/Lib/site-packages/gunicorn/http/unreader.py	(date 1630065626393)
@@ -0,0 +1,79 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+import io
+import os
+
+# Classes that can undo reading data from
+# a given type of data source.
+
+
+class Unreader(object):
+    def __init__(self):
+        self.buf = io.BytesIO()
+
+    def chunk(self):
+        raise NotImplementedError()
+
+    def read(self, size=None):
+        if size is not None and not isinstance(size, int):
+            raise TypeError("size parameter must be an int or long.")
+
+        if size is not None:
+            if size == 0:
+                return b""
+            if size < 0:
+                size = None
+
+        self.buf.seek(0, os.SEEK_END)
+
+        if size is None and self.buf.tell():
+            ret = self.buf.getvalue()
+            self.buf = io.BytesIO()
+            return ret
+        if size is None:
+            d = self.chunk()
+            return d
+
+        while self.buf.tell() < size:
+            chunk = self.chunk()
+            if not chunk:
+                ret = self.buf.getvalue()
+                self.buf = io.BytesIO()
+                return ret
+            self.buf.write(chunk)
+        data = self.buf.getvalue()
+        self.buf = io.BytesIO()
+        self.buf.write(data[size:])
+        return data[:size]
+
+    def unread(self, data):
+        self.buf.seek(0, os.SEEK_END)
+        self.buf.write(data)
+
+
+class SocketUnreader(Unreader):
+    def __init__(self, sock, max_chunk=8192):
+        super().__init__()
+        self.sock = sock
+        self.mxchunk = max_chunk
+
+    def chunk(self):
+        return self.sock.recv(self.mxchunk)
+
+
+class IterUnreader(Unreader):
+    def __init__(self, iterable):
+        super().__init__()
+        self.iter = iter(iterable)
+
+    def chunk(self):
+        if not self.iter:
+            return b""
+        try:
+            return next(self.iter)
+        except StopIteration:
+            self.iter = None
+            return b""
Index: venv/Lib/site-packages/gunicorn/http/wsgi.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/http/wsgi.py b/venv/Lib/site-packages/gunicorn/http/wsgi.py
new file mode 100644
--- /dev/null	(date 1630065626399)
+++ b/venv/Lib/site-packages/gunicorn/http/wsgi.py	(date 1630065626399)
@@ -0,0 +1,393 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+import io
+import logging
+import os
+import re
+import sys
+
+from gunicorn.http.message import HEADER_RE
+from gunicorn.http.errors import InvalidHeader, InvalidHeaderName
+from gunicorn import SERVER_SOFTWARE, SERVER
+import gunicorn.util as util
+
+# Send files in at most 1GB blocks as some operating systems can have problems
+# with sending files in blocks over 2GB.
+BLKSIZE = 0x3FFFFFFF
+
+HEADER_VALUE_RE = re.compile(r'[\x00-\x1F\x7F]')
+
+log = logging.getLogger(__name__)
+
+
+class FileWrapper(object):
+
+    def __init__(self, filelike, blksize=8192):
+        self.filelike = filelike
+        self.blksize = blksize
+        if hasattr(filelike, 'close'):
+            self.close = filelike.close
+
+    def __getitem__(self, key):
+        data = self.filelike.read(self.blksize)
+        if data:
+            return data
+        raise IndexError
+
+
+class WSGIErrorsWrapper(io.RawIOBase):
+
+    def __init__(self, cfg):
+        # There is no public __init__ method for RawIOBase so
+        # we don't need to call super() in the __init__ method.
+        # pylint: disable=super-init-not-called
+        errorlog = logging.getLogger("gunicorn.error")
+        handlers = errorlog.handlers
+        self.streams = []
+
+        if cfg.errorlog == "-":
+            self.streams.append(sys.stderr)
+            handlers = handlers[1:]
+
+        for h in handlers:
+            if hasattr(h, "stream"):
+                self.streams.append(h.stream)
+
+    def write(self, data):
+        for stream in self.streams:
+            try:
+                stream.write(data)
+            except UnicodeError:
+                stream.write(data.encode("UTF-8"))
+            stream.flush()
+
+
+def base_environ(cfg):
+    return {
+        "wsgi.errors": WSGIErrorsWrapper(cfg),
+        "wsgi.version": (1, 0),
+        "wsgi.multithread": False,
+        "wsgi.multiprocess": (cfg.workers > 1),
+        "wsgi.run_once": False,
+        "wsgi.file_wrapper": FileWrapper,
+        "wsgi.input_terminated": True,
+        "SERVER_SOFTWARE": SERVER_SOFTWARE,
+    }
+
+
+def default_environ(req, sock, cfg):
+    env = base_environ(cfg)
+    env.update({
+        "wsgi.input": req.body,
+        "gunicorn.socket": sock,
+        "REQUEST_METHOD": req.method,
+        "QUERY_STRING": req.query,
+        "RAW_URI": req.uri,
+        "SERVER_PROTOCOL": "HTTP/%s" % ".".join([str(v) for v in req.version])
+    })
+    return env
+
+
+def proxy_environ(req):
+    info = req.proxy_protocol_info
+
+    if not info:
+        return {}
+
+    return {
+        "PROXY_PROTOCOL": info["proxy_protocol"],
+        "REMOTE_ADDR": info["client_addr"],
+        "REMOTE_PORT": str(info["client_port"]),
+        "PROXY_ADDR": info["proxy_addr"],
+        "PROXY_PORT": str(info["proxy_port"]),
+    }
+
+
+def create(req, sock, client, server, cfg):
+    resp = Response(req, sock, cfg)
+
+    # set initial environ
+    environ = default_environ(req, sock, cfg)
+
+    # default variables
+    host = None
+    script_name = os.environ.get("SCRIPT_NAME", "")
+
+    # add the headers to the environ
+    for hdr_name, hdr_value in req.headers:
+        if hdr_name == "EXPECT":
+            # handle expect
+            if hdr_value.lower() == "100-continue":
+                sock.send(b"HTTP/1.1 100 Continue\r\n\r\n")
+        elif hdr_name == 'HOST':
+            host = hdr_value
+        elif hdr_name == "SCRIPT_NAME":
+            script_name = hdr_value
+        elif hdr_name == "CONTENT-TYPE":
+            environ['CONTENT_TYPE'] = hdr_value
+            continue
+        elif hdr_name == "CONTENT-LENGTH":
+            environ['CONTENT_LENGTH'] = hdr_value
+            continue
+
+        key = 'HTTP_' + hdr_name.replace('-', '_')
+        if key in environ:
+            hdr_value = "%s,%s" % (environ[key], hdr_value)
+        environ[key] = hdr_value
+
+    # set the url scheme
+    environ['wsgi.url_scheme'] = req.scheme
+
+    # set the REMOTE_* keys in environ
+    # authors should be aware that REMOTE_HOST and REMOTE_ADDR
+    # may not qualify the remote addr:
+    # http://www.ietf.org/rfc/rfc3875
+    if isinstance(client, str):
+        environ['REMOTE_ADDR'] = client
+    elif isinstance(client, bytes):
+        environ['REMOTE_ADDR'] = client.decode()
+    else:
+        environ['REMOTE_ADDR'] = client[0]
+        environ['REMOTE_PORT'] = str(client[1])
+
+    # handle the SERVER_*
+    # Normally only the application should use the Host header but since the
+    # WSGI spec doesn't support unix sockets, we are using it to create
+    # viable SERVER_* if possible.
+    if isinstance(server, str):
+        server = server.split(":")
+        if len(server) == 1:
+            # unix socket
+            if host:
+                server = host.split(':')
+                if len(server) == 1:
+                    if req.scheme == "http":
+                        server.append(80)
+                    elif req.scheme == "https":
+                        server.append(443)
+                    else:
+                        server.append('')
+            else:
+                # no host header given which means that we are not behind a
+                # proxy, so append an empty port.
+                server.append('')
+    environ['SERVER_NAME'] = server[0]
+    environ['SERVER_PORT'] = str(server[1])
+
+    # set the path and script name
+    path_info = req.path
+    if script_name:
+        path_info = path_info.split(script_name, 1)[1]
+    environ['PATH_INFO'] = util.unquote_to_wsgi_str(path_info)
+    environ['SCRIPT_NAME'] = script_name
+
+    # override the environ with the correct remote and server address if
+    # we are behind a proxy using the proxy protocol.
+    environ.update(proxy_environ(req))
+    return resp, environ
+
+
+class Response(object):
+
+    def __init__(self, req, sock, cfg):
+        self.req = req
+        self.sock = sock
+        self.version = SERVER
+        self.status = None
+        self.chunked = False
+        self.must_close = False
+        self.headers = []
+        self.headers_sent = False
+        self.response_length = None
+        self.sent = 0
+        self.upgrade = False
+        self.cfg = cfg
+
+    def force_close(self):
+        self.must_close = True
+
+    def should_close(self):
+        if self.must_close or self.req.should_close():
+            return True
+        if self.response_length is not None or self.chunked:
+            return False
+        if self.req.method == 'HEAD':
+            return False
+        if self.status_code < 200 or self.status_code in (204, 304):
+            return False
+        return True
+
+    def start_response(self, status, headers, exc_info=None):
+        if exc_info:
+            try:
+                if self.status and self.headers_sent:
+                    util.reraise(exc_info[0], exc_info[1], exc_info[2])
+            finally:
+                exc_info = None
+        elif self.status is not None:
+            raise AssertionError("Response headers already set!")
+
+        self.status = status
+
+        # get the status code from the response here so we can use it to check
+        # the need for the connection header later without parsing the string
+        # each time.
+        try:
+            self.status_code = int(self.status.split()[0])
+        except ValueError:
+            self.status_code = None
+
+        self.process_headers(headers)
+        self.chunked = self.is_chunked()
+        return self.write
+
+    def process_headers(self, headers):
+        for name, value in headers:
+            if not isinstance(name, str):
+                raise TypeError('%r is not a string' % name)
+
+            if HEADER_RE.search(name):
+                raise InvalidHeaderName('%r' % name)
+
+            if not isinstance(value, str):
+                raise TypeError('%r is not a string' % value)
+
+            if HEADER_VALUE_RE.search(value):
+                raise InvalidHeader('%r' % value)
+
+            value = value.strip()
+            lname = name.lower().strip()
+            if lname == "content-length":
+                self.response_length = int(value)
+            elif util.is_hoppish(name):
+                if lname == "connection":
+                    # handle websocket
+                    if value.lower().strip() == "upgrade":
+                        self.upgrade = True
+                elif lname == "upgrade":
+                    if value.lower().strip() == "websocket":
+                        self.headers.append((name.strip(), value))
+
+                # ignore hopbyhop headers
+                continue
+            self.headers.append((name.strip(), value))
+
+    def is_chunked(self):
+        # Only use chunked responses when the client is
+        # speaking HTTP/1.1 or newer and there was
+        # no Content-Length header set.
+        if self.response_length is not None:
+            return False
+        elif self.req.version <= (1, 0):
+            return False
+        elif self.req.method == 'HEAD':
+            # Responses to a HEAD request MUST NOT contain a response body.
+            return False
+        elif self.status_code in (204, 304):
+            # Do not use chunked responses when the response is guaranteed to
+            # not have a response body.
+            return False
+        return True
+
+    def default_headers(self):
+        # set the connection header
+        if self.upgrade:
+            connection = "upgrade"
+        elif self.should_close():
+            connection = "close"
+        else:
+            connection = "keep-alive"
+
+        headers = [
+            "HTTP/%s.%s %s\r\n" % (self.req.version[0],
+                                   self.req.version[1], self.status),
+            "Server: %s\r\n" % self.version,
+            "Date: %s\r\n" % util.http_date(),
+            "Connection: %s\r\n" % connection
+        ]
+        if self.chunked:
+            headers.append("Transfer-Encoding: chunked\r\n")
+        return headers
+
+    def send_headers(self):
+        if self.headers_sent:
+            return
+        tosend = self.default_headers()
+        tosend.extend(["%s: %s\r\n" % (k, v) for k, v in self.headers])
+
+        header_str = "%s\r\n" % "".join(tosend)
+        util.write(self.sock, util.to_bytestring(header_str, "latin-1"))
+        self.headers_sent = True
+
+    def write(self, arg):
+        self.send_headers()
+        if not isinstance(arg, bytes):
+            raise TypeError('%r is not a byte' % arg)
+        arglen = len(arg)
+        tosend = arglen
+        if self.response_length is not None:
+            if self.sent >= self.response_length:
+                # Never write more than self.response_length bytes
+                return
+
+            tosend = min(self.response_length - self.sent, tosend)
+            if tosend < arglen:
+                arg = arg[:tosend]
+
+        # Sending an empty chunk signals the end of the
+        # response and prematurely closes the response
+        if self.chunked and tosend == 0:
+            return
+
+        self.sent += tosend
+        util.write(self.sock, arg, self.chunked)
+
+    def can_sendfile(self):
+        return self.cfg.sendfile is not False
+
+    def sendfile(self, respiter):
+        if self.cfg.is_ssl or not self.can_sendfile():
+            return False
+
+        if not util.has_fileno(respiter.filelike):
+            return False
+
+        fileno = respiter.filelike.fileno()
+        try:
+            offset = os.lseek(fileno, 0, os.SEEK_CUR)
+            if self.response_length is None:
+                filesize = os.fstat(fileno).st_size
+                nbytes = filesize - offset
+            else:
+                nbytes = self.response_length
+        except (OSError, io.UnsupportedOperation):
+            return False
+
+        self.send_headers()
+
+        if self.is_chunked():
+            chunk_size = "%X\r\n" % nbytes
+            self.sock.sendall(chunk_size.encode('utf-8'))
+
+        self.sock.sendfile(respiter.filelike, count=nbytes)
+
+        if self.is_chunked():
+            self.sock.sendall(b"\r\n")
+
+        os.lseek(fileno, offset, os.SEEK_SET)
+
+        return True
+
+    def write_file(self, respiter):
+        if not self.sendfile(respiter):
+            for item in respiter:
+                self.write(item)
+
+    def close(self):
+        if not self.headers_sent:
+            self.send_headers()
+        if self.chunked:
+            util.write_chunk(self.sock, b"")
Index: venv/Lib/site-packages/gunicorn/http/message.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/http/message.py b/venv/Lib/site-packages/gunicorn/http/message.py
new file mode 100644
--- /dev/null	(date 1630065626386)
+++ b/venv/Lib/site-packages/gunicorn/http/message.py	(date 1630065626386)
@@ -0,0 +1,356 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+import io
+import re
+import socket
+
+from gunicorn.http.body import ChunkedReader, LengthReader, EOFReader, Body
+from gunicorn.http.errors import (
+    InvalidHeader, InvalidHeaderName, NoMoreData,
+    InvalidRequestLine, InvalidRequestMethod, InvalidHTTPVersion,
+    LimitRequestLine, LimitRequestHeaders,
+)
+from gunicorn.http.errors import InvalidProxyLine, ForbiddenProxyRequest
+from gunicorn.http.errors import InvalidSchemeHeaders
+from gunicorn.util import bytes_to_str, split_request_uri
+
+MAX_REQUEST_LINE = 8190
+MAX_HEADERS = 32768
+DEFAULT_MAX_HEADERFIELD_SIZE = 8190
+
+HEADER_RE = re.compile(r"[\x00-\x1F\x7F()<>@,;:\[\]={} \t\\\"]")
+METH_RE = re.compile(r"[A-Z0-9$-_.]{3,20}")
+VERSION_RE = re.compile(r"HTTP/(\d+)\.(\d+)")
+
+
+class Message(object):
+    def __init__(self, cfg, unreader, peer_addr):
+        self.cfg = cfg
+        self.unreader = unreader
+        self.peer_addr = peer_addr
+        self.version = None
+        self.headers = []
+        self.trailers = []
+        self.body = None
+        self.scheme = "https" if cfg.is_ssl else "http"
+
+        # set headers limits
+        self.limit_request_fields = cfg.limit_request_fields
+        if (self.limit_request_fields <= 0
+            or self.limit_request_fields > MAX_HEADERS):
+            self.limit_request_fields = MAX_HEADERS
+        self.limit_request_field_size = cfg.limit_request_field_size
+        if self.limit_request_field_size < 0:
+            self.limit_request_field_size = DEFAULT_MAX_HEADERFIELD_SIZE
+
+        # set max header buffer size
+        max_header_field_size = self.limit_request_field_size or DEFAULT_MAX_HEADERFIELD_SIZE
+        self.max_buffer_headers = self.limit_request_fields * \
+            (max_header_field_size + 2) + 4
+
+        unused = self.parse(self.unreader)
+        self.unreader.unread(unused)
+        self.set_body_reader()
+
+    def parse(self, unreader):
+        raise NotImplementedError()
+
+    def parse_headers(self, data):
+        cfg = self.cfg
+        headers = []
+
+        # Split lines on \r\n keeping the \r\n on each line
+        lines = [bytes_to_str(line) + "\r\n" for line in data.split(b"\r\n")]
+
+        # handle scheme headers
+        scheme_header = False
+        secure_scheme_headers = {}
+        if ('*' in cfg.forwarded_allow_ips or
+            not isinstance(self.peer_addr, tuple)
+            or self.peer_addr[0] in cfg.forwarded_allow_ips):
+            secure_scheme_headers = cfg.secure_scheme_headers
+
+        # Parse headers into key/value pairs paying attention
+        # to continuation lines.
+        while lines:
+            if len(headers) >= self.limit_request_fields:
+                raise LimitRequestHeaders("limit request headers fields")
+
+            # Parse initial header name : value pair.
+            curr = lines.pop(0)
+            header_length = len(curr)
+            if curr.find(":") < 0:
+                raise InvalidHeader(curr.strip())
+            name, value = curr.split(":", 1)
+            if self.cfg.strip_header_spaces:
+                name = name.rstrip(" \t").upper()
+            else:
+                name = name.upper()
+            if HEADER_RE.search(name):
+                raise InvalidHeaderName(name)
+
+            name, value = name.strip(), [value.lstrip()]
+
+            # Consume value continuation lines
+            while lines and lines[0].startswith((" ", "\t")):
+                curr = lines.pop(0)
+                header_length += len(curr)
+                if header_length > self.limit_request_field_size > 0:
+                    raise LimitRequestHeaders("limit request headers "
+                                              "fields size")
+                value.append(curr)
+            value = ''.join(value).rstrip()
+
+            if header_length > self.limit_request_field_size > 0:
+                raise LimitRequestHeaders("limit request headers fields size")
+
+            if name in secure_scheme_headers:
+                secure = value == secure_scheme_headers[name]
+                scheme = "https" if secure else "http"
+                if scheme_header:
+                    if scheme != self.scheme:
+                        raise InvalidSchemeHeaders()
+                else:
+                    scheme_header = True
+                    self.scheme = scheme
+
+            headers.append((name, value))
+
+        return headers
+
+    def set_body_reader(self):
+        chunked = False
+        content_length = None
+
+        for (name, value) in self.headers:
+            if name == "CONTENT-LENGTH":
+                if content_length is not None:
+                    raise InvalidHeader("CONTENT-LENGTH", req=self)
+                content_length = value
+            elif name == "TRANSFER-ENCODING":
+                if value.lower() == "chunked":
+                    chunked = True
+
+        if chunked:
+            self.body = Body(ChunkedReader(self, self.unreader))
+        elif content_length is not None:
+            try:
+                content_length = int(content_length)
+            except ValueError:
+                raise InvalidHeader("CONTENT-LENGTH", req=self)
+
+            if content_length < 0:
+                raise InvalidHeader("CONTENT-LENGTH", req=self)
+
+            self.body = Body(LengthReader(self.unreader, content_length))
+        else:
+            self.body = Body(EOFReader(self.unreader))
+
+    def should_close(self):
+        for (h, v) in self.headers:
+            if h == "CONNECTION":
+                v = v.lower().strip()
+                if v == "close":
+                    return True
+                elif v == "keep-alive":
+                    return False
+                break
+        return self.version <= (1, 0)
+
+
+class Request(Message):
+    def __init__(self, cfg, unreader, peer_addr, req_number=1):
+        self.method = None
+        self.uri = None
+        self.path = None
+        self.query = None
+        self.fragment = None
+
+        # get max request line size
+        self.limit_request_line = cfg.limit_request_line
+        if (self.limit_request_line < 0
+            or self.limit_request_line >= MAX_REQUEST_LINE):
+            self.limit_request_line = MAX_REQUEST_LINE
+
+        self.req_number = req_number
+        self.proxy_protocol_info = None
+        super().__init__(cfg, unreader, peer_addr)
+
+    def get_data(self, unreader, buf, stop=False):
+        data = unreader.read()
+        if not data:
+            if stop:
+                raise StopIteration()
+            raise NoMoreData(buf.getvalue())
+        buf.write(data)
+
+    def parse(self, unreader):
+        buf = io.BytesIO()
+        self.get_data(unreader, buf, stop=True)
+
+        # get request line
+        line, rbuf = self.read_line(unreader, buf, self.limit_request_line)
+
+        # proxy protocol
+        if self.proxy_protocol(bytes_to_str(line)):
+            # get next request line
+            buf = io.BytesIO()
+            buf.write(rbuf)
+            line, rbuf = self.read_line(unreader, buf, self.limit_request_line)
+
+        self.parse_request_line(line)
+        buf = io.BytesIO()
+        buf.write(rbuf)
+
+        # Headers
+        data = buf.getvalue()
+        idx = data.find(b"\r\n\r\n")
+
+        done = data[:2] == b"\r\n"
+        while True:
+            idx = data.find(b"\r\n\r\n")
+            done = data[:2] == b"\r\n"
+
+            if idx < 0 and not done:
+                self.get_data(unreader, buf)
+                data = buf.getvalue()
+                if len(data) > self.max_buffer_headers:
+                    raise LimitRequestHeaders("max buffer headers")
+            else:
+                break
+
+        if done:
+            self.unreader.unread(data[2:])
+            return b""
+
+        self.headers = self.parse_headers(data[:idx])
+
+        ret = data[idx + 4:]
+        buf = None
+        return ret
+
+    def read_line(self, unreader, buf, limit=0):
+        data = buf.getvalue()
+
+        while True:
+            idx = data.find(b"\r\n")
+            if idx >= 0:
+                # check if the request line is too large
+                if idx > limit > 0:
+                    raise LimitRequestLine(idx, limit)
+                break
+            if len(data) - 2 > limit > 0:
+                raise LimitRequestLine(len(data), limit)
+            self.get_data(unreader, buf)
+            data = buf.getvalue()
+
+        return (data[:idx],  # request line,
+                data[idx + 2:])  # residue in the buffer, skip \r\n
+
+    def proxy_protocol(self, line):
+        """\
+        Detect, check and parse proxy protocol.
+
+        :raises: ForbiddenProxyRequest, InvalidProxyLine.
+        :return: True for proxy protocol line else False
+        """
+        if not self.cfg.proxy_protocol:
+            return False
+
+        if self.req_number != 1:
+            return False
+
+        if not line.startswith("PROXY"):
+            return False
+
+        self.proxy_protocol_access_check()
+        self.parse_proxy_protocol(line)
+
+        return True
+
+    def proxy_protocol_access_check(self):
+        # check in allow list
+        if ("*" not in self.cfg.proxy_allow_ips and
+            isinstance(self.peer_addr, tuple) and
+            self.peer_addr[0] not in self.cfg.proxy_allow_ips):
+            raise ForbiddenProxyRequest(self.peer_addr[0])
+
+    def parse_proxy_protocol(self, line):
+        bits = line.split()
+
+        if len(bits) != 6:
+            raise InvalidProxyLine(line)
+
+        # Extract data
+        proto = bits[1]
+        s_addr = bits[2]
+        d_addr = bits[3]
+
+        # Validation
+        if proto not in ["TCP4", "TCP6"]:
+            raise InvalidProxyLine("protocol '%s' not supported" % proto)
+        if proto == "TCP4":
+            try:
+                socket.inet_pton(socket.AF_INET, s_addr)
+                socket.inet_pton(socket.AF_INET, d_addr)
+            except socket.error:
+                raise InvalidProxyLine(line)
+        elif proto == "TCP6":
+            try:
+                socket.inet_pton(socket.AF_INET6, s_addr)
+                socket.inet_pton(socket.AF_INET6, d_addr)
+            except socket.error:
+                raise InvalidProxyLine(line)
+
+        try:
+            s_port = int(bits[4])
+            d_port = int(bits[5])
+        except ValueError:
+            raise InvalidProxyLine("invalid port %s" % line)
+
+        if not ((0 <= s_port <= 65535) and (0 <= d_port <= 65535)):
+            raise InvalidProxyLine("invalid port %s" % line)
+
+        # Set data
+        self.proxy_protocol_info = {
+            "proxy_protocol": proto,
+            "client_addr": s_addr,
+            "client_port": s_port,
+            "proxy_addr": d_addr,
+            "proxy_port": d_port
+        }
+
+    def parse_request_line(self, line_bytes):
+        bits = [bytes_to_str(bit) for bit in line_bytes.split(None, 2)]
+        if len(bits) != 3:
+            raise InvalidRequestLine(bytes_to_str(line_bytes))
+
+        # Method
+        if not METH_RE.match(bits[0]):
+            raise InvalidRequestMethod(bits[0])
+        self.method = bits[0].upper()
+
+        # URI
+        self.uri = bits[1]
+
+        try:
+            parts = split_request_uri(self.uri)
+        except ValueError:
+            raise InvalidRequestLine(bytes_to_str(line_bytes))
+        self.path = parts.path or ""
+        self.query = parts.query or ""
+        self.fragment = parts.fragment or ""
+
+        # Version
+        match = VERSION_RE.match(bits[2])
+        if match is None:
+            raise InvalidHTTPVersion(bits[2])
+        self.version = (int(match.group(1)), int(match.group(2)))
+
+    def set_body_reader(self):
+        super().set_body_reader()
+        if isinstance(self.body.reader, EOFReader):
+            self.body = Body(LengthReader(self.unreader, 0))
Index: venv/Lib/site-packages/gunicorn/http/parser.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/http/parser.py b/venv/Lib/site-packages/gunicorn/http/parser.py
new file mode 100644
--- /dev/null	(date 1630065626390)
+++ b/venv/Lib/site-packages/gunicorn/http/parser.py	(date 1630065626390)
@@ -0,0 +1,52 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+from gunicorn.http.message import Request
+from gunicorn.http.unreader import SocketUnreader, IterUnreader
+
+
+class Parser(object):
+
+    mesg_class = None
+
+    def __init__(self, cfg, source, source_addr):
+        self.cfg = cfg
+        if hasattr(source, "recv"):
+            self.unreader = SocketUnreader(source)
+        else:
+            self.unreader = IterUnreader(source)
+        self.mesg = None
+        self.source_addr = source_addr
+
+        # request counter (for keepalive connetions)
+        self.req_count = 0
+
+    def __iter__(self):
+        return self
+
+    def __next__(self):
+        # Stop if HTTP dictates a stop.
+        if self.mesg and self.mesg.should_close():
+            raise StopIteration()
+
+        # Discard any unread body of the previous message
+        if self.mesg:
+            data = self.mesg.body.read(8192)
+            while data:
+                data = self.mesg.body.read(8192)
+
+        # Parse the next request
+        self.req_count += 1
+        self.mesg = self.mesg_class(self.cfg, self.unreader, self.source_addr, self.req_count)
+        if not self.mesg:
+            raise StopIteration()
+        return self.mesg
+
+    next = __next__
+
+
+class RequestParser(Parser):
+
+    mesg_class = Request
Index: venv/Lib/site-packages/gunicorn/http/body.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/http/body.py b/venv/Lib/site-packages/gunicorn/http/body.py
new file mode 100644
--- /dev/null	(date 1630065626381)
+++ b/venv/Lib/site-packages/gunicorn/http/body.py	(date 1630065626381)
@@ -0,0 +1,262 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+import io
+import sys
+
+from gunicorn.http.errors import (NoMoreData, ChunkMissingTerminator,
+                                  InvalidChunkSize)
+
+
+class ChunkedReader(object):
+    def __init__(self, req, unreader):
+        self.req = req
+        self.parser = self.parse_chunked(unreader)
+        self.buf = io.BytesIO()
+
+    def read(self, size):
+        if not isinstance(size, int):
+            raise TypeError("size must be an integral type")
+        if size < 0:
+            raise ValueError("Size must be positive.")
+        if size == 0:
+            return b""
+
+        if self.parser:
+            while self.buf.tell() < size:
+                try:
+                    self.buf.write(next(self.parser))
+                except StopIteration:
+                    self.parser = None
+                    break
+
+        data = self.buf.getvalue()
+        ret, rest = data[:size], data[size:]
+        self.buf = io.BytesIO()
+        self.buf.write(rest)
+        return ret
+
+    def parse_trailers(self, unreader, data):
+        buf = io.BytesIO()
+        buf.write(data)
+
+        idx = buf.getvalue().find(b"\r\n\r\n")
+        done = buf.getvalue()[:2] == b"\r\n"
+        while idx < 0 and not done:
+            self.get_data(unreader, buf)
+            idx = buf.getvalue().find(b"\r\n\r\n")
+            done = buf.getvalue()[:2] == b"\r\n"
+        if done:
+            unreader.unread(buf.getvalue()[2:])
+            return b""
+        self.req.trailers = self.req.parse_headers(buf.getvalue()[:idx])
+        unreader.unread(buf.getvalue()[idx + 4:])
+
+    def parse_chunked(self, unreader):
+        (size, rest) = self.parse_chunk_size(unreader)
+        while size > 0:
+            while size > len(rest):
+                size -= len(rest)
+                yield rest
+                rest = unreader.read()
+                if not rest:
+                    raise NoMoreData()
+            yield rest[:size]
+            # Remove \r\n after chunk
+            rest = rest[size:]
+            while len(rest) < 2:
+                rest += unreader.read()
+            if rest[:2] != b'\r\n':
+                raise ChunkMissingTerminator(rest[:2])
+            (size, rest) = self.parse_chunk_size(unreader, data=rest[2:])
+
+    def parse_chunk_size(self, unreader, data=None):
+        buf = io.BytesIO()
+        if data is not None:
+            buf.write(data)
+
+        idx = buf.getvalue().find(b"\r\n")
+        while idx < 0:
+            self.get_data(unreader, buf)
+            idx = buf.getvalue().find(b"\r\n")
+
+        data = buf.getvalue()
+        line, rest_chunk = data[:idx], data[idx + 2:]
+
+        chunk_size = line.split(b";", 1)[0].strip()
+        try:
+            chunk_size = int(chunk_size, 16)
+        except ValueError:
+            raise InvalidChunkSize(chunk_size)
+
+        if chunk_size == 0:
+            try:
+                self.parse_trailers(unreader, rest_chunk)
+            except NoMoreData:
+                pass
+            return (0, None)
+        return (chunk_size, rest_chunk)
+
+    def get_data(self, unreader, buf):
+        data = unreader.read()
+        if not data:
+            raise NoMoreData()
+        buf.write(data)
+
+
+class LengthReader(object):
+    def __init__(self, unreader, length):
+        self.unreader = unreader
+        self.length = length
+
+    def read(self, size):
+        if not isinstance(size, int):
+            raise TypeError("size must be an integral type")
+
+        size = min(self.length, size)
+        if size < 0:
+            raise ValueError("Size must be positive.")
+        if size == 0:
+            return b""
+
+        buf = io.BytesIO()
+        data = self.unreader.read()
+        while data:
+            buf.write(data)
+            if buf.tell() >= size:
+                break
+            data = self.unreader.read()
+
+        buf = buf.getvalue()
+        ret, rest = buf[:size], buf[size:]
+        self.unreader.unread(rest)
+        self.length -= size
+        return ret
+
+
+class EOFReader(object):
+    def __init__(self, unreader):
+        self.unreader = unreader
+        self.buf = io.BytesIO()
+        self.finished = False
+
+    def read(self, size):
+        if not isinstance(size, int):
+            raise TypeError("size must be an integral type")
+        if size < 0:
+            raise ValueError("Size must be positive.")
+        if size == 0:
+            return b""
+
+        if self.finished:
+            data = self.buf.getvalue()
+            ret, rest = data[:size], data[size:]
+            self.buf = io.BytesIO()
+            self.buf.write(rest)
+            return ret
+
+        data = self.unreader.read()
+        while data:
+            self.buf.write(data)
+            if self.buf.tell() > size:
+                break
+            data = self.unreader.read()
+
+        if not data:
+            self.finished = True
+
+        data = self.buf.getvalue()
+        ret, rest = data[:size], data[size:]
+        self.buf = io.BytesIO()
+        self.buf.write(rest)
+        return ret
+
+
+class Body(object):
+    def __init__(self, reader):
+        self.reader = reader
+        self.buf = io.BytesIO()
+
+    def __iter__(self):
+        return self
+
+    def __next__(self):
+        ret = self.readline()
+        if not ret:
+            raise StopIteration()
+        return ret
+
+    next = __next__
+
+    def getsize(self, size):
+        if size is None:
+            return sys.maxsize
+        elif not isinstance(size, int):
+            raise TypeError("size must be an integral type")
+        elif size < 0:
+            return sys.maxsize
+        return size
+
+    def read(self, size=None):
+        size = self.getsize(size)
+        if size == 0:
+            return b""
+
+        if size < self.buf.tell():
+            data = self.buf.getvalue()
+            ret, rest = data[:size], data[size:]
+            self.buf = io.BytesIO()
+            self.buf.write(rest)
+            return ret
+
+        while size > self.buf.tell():
+            data = self.reader.read(1024)
+            if not data:
+                break
+            self.buf.write(data)
+
+        data = self.buf.getvalue()
+        ret, rest = data[:size], data[size:]
+        self.buf = io.BytesIO()
+        self.buf.write(rest)
+        return ret
+
+    def readline(self, size=None):
+        size = self.getsize(size)
+        if size == 0:
+            return b""
+
+        data = self.buf.getvalue()
+        self.buf = io.BytesIO()
+
+        ret = []
+        while 1:
+            idx = data.find(b"\n", 0, size)
+            idx = idx + 1 if idx >= 0 else size if len(data) >= size else 0
+            if idx:
+                ret.append(data[:idx])
+                self.buf.write(data[idx:])
+                break
+
+            ret.append(data)
+            size -= len(data)
+            data = self.reader.read(min(1024, size))
+            if not data:
+                break
+
+        return b"".join(ret)
+
+    def readlines(self, size=None):
+        ret = []
+        data = self.read()
+        while data:
+            pos = data.find(b"\n")
+            if pos < 0:
+                ret.append(data)
+                data = b""
+            else:
+                line, data = data[:pos + 1], data[pos + 1:]
+                ret.append(line)
+        return ret
Index: venv/Lib/site-packages/gunicorn/http/errors.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/http/errors.py b/venv/Lib/site-packages/gunicorn/http/errors.py
new file mode 100644
--- /dev/null	(date 1630065626384)
+++ b/venv/Lib/site-packages/gunicorn/http/errors.py	(date 1630065626384)
@@ -0,0 +1,120 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+# We don't need to call super() in __init__ methods of our
+# BaseException and Exception classes because we also define
+# our own __str__ methods so there is no need to pass 'message'
+# to the base class to get a meaningful output from 'str(exc)'.
+# pylint: disable=super-init-not-called
+
+
+class ParseException(Exception):
+    pass
+
+
+class NoMoreData(IOError):
+    def __init__(self, buf=None):
+        self.buf = buf
+
+    def __str__(self):
+        return "No more data after: %r" % self.buf
+
+
+class InvalidRequestLine(ParseException):
+    def __init__(self, req):
+        self.req = req
+        self.code = 400
+
+    def __str__(self):
+        return "Invalid HTTP request line: %r" % self.req
+
+
+class InvalidRequestMethod(ParseException):
+    def __init__(self, method):
+        self.method = method
+
+    def __str__(self):
+        return "Invalid HTTP method: %r" % self.method
+
+
+class InvalidHTTPVersion(ParseException):
+    def __init__(self, version):
+        self.version = version
+
+    def __str__(self):
+        return "Invalid HTTP Version: %r" % self.version
+
+
+class InvalidHeader(ParseException):
+    def __init__(self, hdr, req=None):
+        self.hdr = hdr
+        self.req = req
+
+    def __str__(self):
+        return "Invalid HTTP Header: %r" % self.hdr
+
+
+class InvalidHeaderName(ParseException):
+    def __init__(self, hdr):
+        self.hdr = hdr
+
+    def __str__(self):
+        return "Invalid HTTP header name: %r" % self.hdr
+
+
+class InvalidChunkSize(IOError):
+    def __init__(self, data):
+        self.data = data
+
+    def __str__(self):
+        return "Invalid chunk size: %r" % self.data
+
+
+class ChunkMissingTerminator(IOError):
+    def __init__(self, term):
+        self.term = term
+
+    def __str__(self):
+        return "Invalid chunk terminator is not '\\r\\n': %r" % self.term
+
+
+class LimitRequestLine(ParseException):
+    def __init__(self, size, max_size):
+        self.size = size
+        self.max_size = max_size
+
+    def __str__(self):
+        return "Request Line is too large (%s > %s)" % (self.size, self.max_size)
+
+
+class LimitRequestHeaders(ParseException):
+    def __init__(self, msg):
+        self.msg = msg
+
+    def __str__(self):
+        return self.msg
+
+
+class InvalidProxyLine(ParseException):
+    def __init__(self, line):
+        self.line = line
+        self.code = 400
+
+    def __str__(self):
+        return "Invalid PROXY line: %r" % self.line
+
+
+class ForbiddenProxyRequest(ParseException):
+    def __init__(self, host):
+        self.host = host
+        self.code = 403
+
+    def __str__(self):
+        return "Proxy request from %r not allowed" % self.host
+
+
+class InvalidSchemeHeaders(ParseException):
+    def __str__(self):
+        return "Contradictory scheme headers"
Index: venv/Lib/site-packages/gunicorn/app/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/app/__init__.py b/venv/Lib/site-packages/gunicorn/app/__init__.py
new file mode 100644
--- /dev/null	(date 1630065626360)
+++ b/venv/Lib/site-packages/gunicorn/app/__init__.py	(date 1630065626360)
@@ -0,0 +1,4 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
Index: venv/Lib/site-packages/gunicorn/app/pasterapp.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/app/pasterapp.py b/venv/Lib/site-packages/gunicorn/app/pasterapp.py
new file mode 100644
--- /dev/null	(date 1630065626369)
+++ b/venv/Lib/site-packages/gunicorn/app/pasterapp.py	(date 1630065626369)
@@ -0,0 +1,75 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+import configparser
+import os
+
+from paste.deploy import loadapp
+
+from gunicorn.app.wsgiapp import WSGIApplication
+from gunicorn.config import get_default_config_file
+
+
+def get_wsgi_app(config_uri, name=None, defaults=None):
+    if ':' not in config_uri:
+        config_uri = "config:%s" % config_uri
+
+    return loadapp(
+        config_uri,
+        name=name,
+        relative_to=os.getcwd(),
+        global_conf=defaults,
+    )
+
+
+def has_logging_config(config_file):
+    parser = configparser.ConfigParser()
+    parser.read([config_file])
+    return parser.has_section('loggers')
+
+
+def serve(app, global_conf, **local_conf):
+    """\
+    A Paste Deployment server runner.
+
+    Example configuration:
+
+        [server:main]
+        use = egg:gunicorn#main
+        host = 127.0.0.1
+        port = 5000
+    """
+    config_file = global_conf['__file__']
+    gunicorn_config_file = local_conf.pop('config', None)
+
+    host = local_conf.pop('host', '')
+    port = local_conf.pop('port', '')
+    if host and port:
+        local_conf['bind'] = '%s:%s' % (host, port)
+    elif host:
+        local_conf['bind'] = host.split(',')
+
+    class PasterServerApplication(WSGIApplication):
+        def load_config(self):
+            self.cfg.set("default_proc_name", config_file)
+
+            if has_logging_config(config_file):
+                self.cfg.set("logconfig", config_file)
+
+            if gunicorn_config_file:
+                self.load_config_from_file(gunicorn_config_file)
+            else:
+                default_gunicorn_config_file = get_default_config_file()
+                if default_gunicorn_config_file is not None:
+                    self.load_config_from_file(default_gunicorn_config_file)
+
+            for k, v in local_conf.items():
+                if v is not None:
+                    self.cfg.set(k.lower(), v)
+
+        def load(self):
+            return app
+
+    PasterServerApplication().run()
Index: venv/Lib/site-packages/gunicorn/app/wsgiapp.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/app/wsgiapp.py b/venv/Lib/site-packages/gunicorn/app/wsgiapp.py
new file mode 100644
--- /dev/null	(date 1630065626373)
+++ b/venv/Lib/site-packages/gunicorn/app/wsgiapp.py	(date 1630065626373)
@@ -0,0 +1,71 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+import os
+
+from gunicorn.errors import ConfigError
+from gunicorn.app.base import Application
+from gunicorn import util
+
+
+class WSGIApplication(Application):
+    def init(self, parser, opts, args):
+        self.app_uri = None
+
+        if opts.paste:
+            from .pasterapp import has_logging_config
+
+            config_uri = os.path.abspath(opts.paste)
+            config_file = config_uri.split('#')[0]
+
+            if not os.path.exists(config_file):
+                raise ConfigError("%r not found" % config_file)
+
+            self.cfg.set("default_proc_name", config_file)
+            self.app_uri = config_uri
+
+            if has_logging_config(config_file):
+                self.cfg.set("logconfig", config_file)
+
+            return
+
+        if len(args) > 0:
+            self.cfg.set("default_proc_name", args[0])
+            self.app_uri = args[0]
+
+    def load_config(self):
+        super().load_config()
+
+        if self.app_uri is None:
+            if self.cfg.wsgi_app is not None:
+                self.app_uri = self.cfg.wsgi_app
+            else:
+                raise ConfigError("No application module specified.")
+
+    def load_wsgiapp(self):
+        return util.import_app(self.app_uri)
+
+    def load_pasteapp(self):
+        from .pasterapp import get_wsgi_app
+        return get_wsgi_app(self.app_uri, defaults=self.cfg.paste_global_conf)
+
+    def load(self):
+        if self.cfg.paste is not None:
+            return self.load_pasteapp()
+        else:
+            return self.load_wsgiapp()
+
+
+def run():
+    """\
+    The ``gunicorn`` command line runner for launching Gunicorn with
+    generic WSGI applications.
+    """
+    from gunicorn.app.wsgiapp import WSGIApplication
+    WSGIApplication("%(prog)s [OPTIONS] [APP_MODULE]").run()
+
+
+if __name__ == '__main__':
+    run()
Index: venv/Lib/site-packages/psycopg2/_range.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/psycopg2/_range.py b/venv/Lib/site-packages/psycopg2/_range.py
new file mode 100644
--- /dev/null	(date 1630065394283)
+++ b/venv/Lib/site-packages/psycopg2/_range.py	(date 1630065394283)
@@ -0,0 +1,537 @@
+"""Implementation of the Range type and adaptation
+
+"""
+
+# psycopg/_range.py - Implementation of the Range type and adaptation
+#
+# Copyright (C) 2012-2019 Daniele Varrazzo  <daniele.varrazzo@gmail.com>
+# Copyright (C) 2020-2021 The Psycopg Team
+#
+# psycopg2 is free software: you can redistribute it and/or modify it
+# under the terms of the GNU Lesser General Public License as published
+# by the Free Software Foundation, either version 3 of the License, or
+# (at your option) any later version.
+#
+# In addition, as a special exception, the copyright holders give
+# permission to link this program with the OpenSSL library (or with
+# modified versions of OpenSSL that use the same license as OpenSSL),
+# and distribute linked combinations including the two.
+#
+# You must obey the GNU Lesser General Public License in all respects for
+# all of the code used other than OpenSSL.
+#
+# psycopg2 is distributed in the hope that it will be useful, but WITHOUT
+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public
+# License for more details.
+
+import re
+
+from psycopg2._psycopg import ProgrammingError, InterfaceError
+from psycopg2.extensions import ISQLQuote, adapt, register_adapter
+from psycopg2.extensions import new_type, new_array_type, register_type
+
+
+class Range:
+    """Python representation for a PostgreSQL |range|_ type.
+
+    :param lower: lower bound for the range. `!None` means unbound
+    :param upper: upper bound for the range. `!None` means unbound
+    :param bounds: one of the literal strings ``()``, ``[)``, ``(]``, ``[]``,
+        representing whether the lower or upper bounds are included
+    :param empty: if `!True`, the range is empty
+
+    """
+    __slots__ = ('_lower', '_upper', '_bounds')
+
+    def __init__(self, lower=None, upper=None, bounds='[)', empty=False):
+        if not empty:
+            if bounds not in ('[)', '(]', '()', '[]'):
+                raise ValueError(f"bound flags not valid: {bounds!r}")
+
+            self._lower = lower
+            self._upper = upper
+            self._bounds = bounds
+        else:
+            self._lower = self._upper = self._bounds = None
+
+    def __repr__(self):
+        if self._bounds is None:
+            return f"{self.__class__.__name__}(empty=True)"
+        else:
+            return "{}({!r}, {!r}, {!r})".format(self.__class__.__name__,
+                self._lower, self._upper, self._bounds)
+
+    def __str__(self):
+        if self._bounds is None:
+            return 'empty'
+
+        items = [
+            self._bounds[0],
+            str(self._lower),
+            ', ',
+            str(self._upper),
+            self._bounds[1]
+        ]
+        return ''.join(items)
+
+    @property
+    def lower(self):
+        """The lower bound of the range. `!None` if empty or unbound."""
+        return self._lower
+
+    @property
+    def upper(self):
+        """The upper bound of the range. `!None` if empty or unbound."""
+        return self._upper
+
+    @property
+    def isempty(self):
+        """`!True` if the range is empty."""
+        return self._bounds is None
+
+    @property
+    def lower_inf(self):
+        """`!True` if the range doesn't have a lower bound."""
+        if self._bounds is None:
+            return False
+        return self._lower is None
+
+    @property
+    def upper_inf(self):
+        """`!True` if the range doesn't have an upper bound."""
+        if self._bounds is None:
+            return False
+        return self._upper is None
+
+    @property
+    def lower_inc(self):
+        """`!True` if the lower bound is included in the range."""
+        if self._bounds is None or self._lower is None:
+            return False
+        return self._bounds[0] == '['
+
+    @property
+    def upper_inc(self):
+        """`!True` if the upper bound is included in the range."""
+        if self._bounds is None or self._upper is None:
+            return False
+        return self._bounds[1] == ']'
+
+    def __contains__(self, x):
+        if self._bounds is None:
+            return False
+
+        if self._lower is not None:
+            if self._bounds[0] == '[':
+                if x < self._lower:
+                    return False
+            else:
+                if x <= self._lower:
+                    return False
+
+        if self._upper is not None:
+            if self._bounds[1] == ']':
+                if x > self._upper:
+                    return False
+            else:
+                if x >= self._upper:
+                    return False
+
+        return True
+
+    def __bool__(self):
+        return self._bounds is not None
+
+    def __nonzero__(self):
+        # Python 2 compatibility
+        return type(self).__bool__(self)
+
+    def __eq__(self, other):
+        if not isinstance(other, Range):
+            return False
+        return (self._lower == other._lower
+            and self._upper == other._upper
+            and self._bounds == other._bounds)
+
+    def __ne__(self, other):
+        return not self.__eq__(other)
+
+    def __hash__(self):
+        return hash((self._lower, self._upper, self._bounds))
+
+    # as the postgres docs describe for the server-side stuff,
+    # ordering is rather arbitrary, but will remain stable
+    # and consistent.
+
+    def __lt__(self, other):
+        if not isinstance(other, Range):
+            return NotImplemented
+        for attr in ('_lower', '_upper', '_bounds'):
+            self_value = getattr(self, attr)
+            other_value = getattr(other, attr)
+            if self_value == other_value:
+                pass
+            elif self_value is None:
+                return True
+            elif other_value is None:
+                return False
+            else:
+                return self_value < other_value
+        return False
+
+    def __le__(self, other):
+        if self == other:
+            return True
+        else:
+            return self.__lt__(other)
+
+    def __gt__(self, other):
+        if isinstance(other, Range):
+            return other.__lt__(self)
+        else:
+            return NotImplemented
+
+    def __ge__(self, other):
+        if self == other:
+            return True
+        else:
+            return self.__gt__(other)
+
+    def __getstate__(self):
+        return {slot: getattr(self, slot)
+            for slot in self.__slots__ if hasattr(self, slot)}
+
+    def __setstate__(self, state):
+        for slot, value in state.items():
+            setattr(self, slot, value)
+
+
+def register_range(pgrange, pyrange, conn_or_curs, globally=False):
+    """Create and register an adapter and the typecasters to convert between
+    a PostgreSQL |range|_ type and a PostgreSQL `Range` subclass.
+
+    :param pgrange: the name of the PostgreSQL |range| type. Can be
+        schema-qualified
+    :param pyrange: a `Range` strict subclass, or just a name to give to a new
+        class
+    :param conn_or_curs: a connection or cursor used to find the oid of the
+        range and its subtype; the typecaster is registered in a scope limited
+        to this object, unless *globally* is set to `!True`
+    :param globally: if `!False` (default) register the typecaster only on
+        *conn_or_curs*, otherwise register it globally
+    :return: `RangeCaster` instance responsible for the conversion
+
+    If a string is passed to *pyrange*, a new `Range` subclass is created
+    with such name and will be available as the `~RangeCaster.range` attribute
+    of the returned `RangeCaster` object.
+
+    The function queries the database on *conn_or_curs* to inspect the
+    *pgrange* type and raises `~psycopg2.ProgrammingError` if the type is not
+    found.  If querying the database is not advisable, use directly the
+    `RangeCaster` class and register the adapter and typecasters using the
+    provided functions.
+
+    """
+    caster = RangeCaster._from_db(pgrange, pyrange, conn_or_curs)
+    caster._register(not globally and conn_or_curs or None)
+    return caster
+
+
+class RangeAdapter:
+    """`ISQLQuote` adapter for `Range` subclasses.
+
+    This is an abstract class: concrete classes must set a `name` class
+    attribute or override `getquoted()`.
+    """
+    name = None
+
+    def __init__(self, adapted):
+        self.adapted = adapted
+
+    def __conform__(self, proto):
+        if self._proto is ISQLQuote:
+            return self
+
+    def prepare(self, conn):
+        self._conn = conn
+
+    def getquoted(self):
+        if self.name is None:
+            raise NotImplementedError(
+                'RangeAdapter must be subclassed overriding its name '
+                'or the getquoted() method')
+
+        r = self.adapted
+        if r.isempty:
+            return b"'empty'::" + self.name.encode('utf8')
+
+        if r.lower is not None:
+            a = adapt(r.lower)
+            if hasattr(a, 'prepare'):
+                a.prepare(self._conn)
+            lower = a.getquoted()
+        else:
+            lower = b'NULL'
+
+        if r.upper is not None:
+            a = adapt(r.upper)
+            if hasattr(a, 'prepare'):
+                a.prepare(self._conn)
+            upper = a.getquoted()
+        else:
+            upper = b'NULL'
+
+        return self.name.encode('utf8') + b'(' + lower + b', ' + upper \
+            + b", '" + r._bounds.encode('utf8') + b"')"
+
+
+class RangeCaster:
+    """Helper class to convert between `Range` and PostgreSQL range types.
+
+    Objects of this class are usually created by `register_range()`. Manual
+    creation could be useful if querying the database is not advisable: in
+    this case the oids must be provided.
+    """
+    def __init__(self, pgrange, pyrange, oid, subtype_oid, array_oid=None):
+        self.subtype_oid = subtype_oid
+        self._create_ranges(pgrange, pyrange)
+
+        name = self.adapter.name or self.adapter.__class__.__name__
+
+        self.typecaster = new_type((oid,), name, self.parse)
+
+        if array_oid is not None:
+            self.array_typecaster = new_array_type(
+                (array_oid,), name + "ARRAY", self.typecaster)
+        else:
+            self.array_typecaster = None
+
+    def _create_ranges(self, pgrange, pyrange):
+        """Create Range and RangeAdapter classes if needed."""
+        # if got a string create a new RangeAdapter concrete type (with a name)
+        # else take it as an adapter. Passing an adapter should be considered
+        # an implementation detail and is not documented. It is currently used
+        # for the numeric ranges.
+        self.adapter = None
+        if isinstance(pgrange, str):
+            self.adapter = type(pgrange, (RangeAdapter,), {})
+            self.adapter.name = pgrange
+        else:
+            try:
+                if issubclass(pgrange, RangeAdapter) \
+                        and pgrange is not RangeAdapter:
+                    self.adapter = pgrange
+            except TypeError:
+                pass
+
+        if self.adapter is None:
+            raise TypeError(
+                'pgrange must be a string or a RangeAdapter strict subclass')
+
+        self.range = None
+        try:
+            if isinstance(pyrange, str):
+                self.range = type(pyrange, (Range,), {})
+            if issubclass(pyrange, Range) and pyrange is not Range:
+                self.range = pyrange
+        except TypeError:
+            pass
+
+        if self.range is None:
+            raise TypeError(
+                'pyrange must be a type or a Range strict subclass')
+
+    @classmethod
+    def _from_db(self, name, pyrange, conn_or_curs):
+        """Return a `RangeCaster` instance for the type *pgrange*.
+
+        Raise `ProgrammingError` if the type is not found.
+        """
+        from psycopg2.extensions import STATUS_IN_TRANSACTION
+        from psycopg2.extras import _solve_conn_curs
+        conn, curs = _solve_conn_curs(conn_or_curs)
+
+        if conn.info.server_version < 90200:
+            raise ProgrammingError("range types not available in version %s"
+                % conn.info.server_version)
+
+        # Store the transaction status of the connection to revert it after use
+        conn_status = conn.status
+
+        # Use the correct schema
+        if '.' in name:
+            schema, tname = name.split('.', 1)
+        else:
+            tname = name
+            schema = 'public'
+
+        # get the type oid and attributes
+        try:
+            curs.execute("""\
+select rngtypid, rngsubtype,
+    (select typarray from pg_type where oid = rngtypid)
+from pg_range r
+join pg_type t on t.oid = rngtypid
+join pg_namespace ns on ns.oid = typnamespace
+where typname = %s and ns.nspname = %s;
+""", (tname, schema))
+
+        except ProgrammingError:
+            if not conn.autocommit:
+                conn.rollback()
+            raise
+        else:
+            rec = curs.fetchone()
+
+            # revert the status of the connection as before the command
+            if (conn_status != STATUS_IN_TRANSACTION
+            and not conn.autocommit):
+                conn.rollback()
+
+        if not rec:
+            raise ProgrammingError(
+                f"PostgreSQL type '{name}' not found")
+
+        type, subtype, array = rec
+
+        return RangeCaster(name, pyrange,
+            oid=type, subtype_oid=subtype, array_oid=array)
+
+    _re_range = re.compile(r"""
+        ( \(|\[ )                   # lower bound flag
+        (?:                         # lower bound:
+          " ( (?: [^"] | "")* ) "   #   - a quoted string
+          | ( [^",]+ )              #   - or an unquoted string
+        )?                          #   - or empty (not catched)
+        ,
+        (?:                         # upper bound:
+          " ( (?: [^"] | "")* ) "   #   - a quoted string
+          | ( [^"\)\]]+ )           #   - or an unquoted string
+        )?                          #   - or empty (not catched)
+        ( \)|\] )                   # upper bound flag
+        """, re.VERBOSE)
+
+    _re_undouble = re.compile(r'(["\\])\1')
+
+    def parse(self, s, cur=None):
+        if s is None:
+            return None
+
+        if s == 'empty':
+            return self.range(empty=True)
+
+        m = self._re_range.match(s)
+        if m is None:
+            raise InterfaceError(f"failed to parse range: '{s}'")
+
+        lower = m.group(3)
+        if lower is None:
+            lower = m.group(2)
+            if lower is not None:
+                lower = self._re_undouble.sub(r"\1", lower)
+
+        upper = m.group(5)
+        if upper is None:
+            upper = m.group(4)
+            if upper is not None:
+                upper = self._re_undouble.sub(r"\1", upper)
+
+        if cur is not None:
+            lower = cur.cast(self.subtype_oid, lower)
+            upper = cur.cast(self.subtype_oid, upper)
+
+        bounds = m.group(1) + m.group(6)
+
+        return self.range(lower, upper, bounds)
+
+    def _register(self, scope=None):
+        register_type(self.typecaster, scope)
+        if self.array_typecaster is not None:
+            register_type(self.array_typecaster, scope)
+
+        register_adapter(self.range, self.adapter)
+
+
+class NumericRange(Range):
+    """A `Range` suitable to pass Python numeric types to a PostgreSQL range.
+
+    PostgreSQL types :sql:`int4range`, :sql:`int8range`, :sql:`numrange` are
+    casted into `!NumericRange` instances.
+    """
+    pass
+
+
+class DateRange(Range):
+    """Represents :sql:`daterange` values."""
+    pass
+
+
+class DateTimeRange(Range):
+    """Represents :sql:`tsrange` values."""
+    pass
+
+
+class DateTimeTZRange(Range):
+    """Represents :sql:`tstzrange` values."""
+    pass
+
+
+# Special adaptation for NumericRange. Allows to pass number range regardless
+# of whether they are ints, floats and what size of ints are, which are
+# pointless in Python world. On the way back, no numeric range is casted to
+# NumericRange, but only to their subclasses
+
+class NumberRangeAdapter(RangeAdapter):
+    """Adapt a range if the subtype doesn't need quotes."""
+    def getquoted(self):
+        r = self.adapted
+        if r.isempty:
+            return b"'empty'"
+
+        if not r.lower_inf:
+            # not exactly: we are relying that none of these object is really
+            # quoted (they are numbers). Also, I'm lazy and not preparing the
+            # adapter because I assume encoding doesn't matter for these
+            # objects.
+            lower = adapt(r.lower).getquoted().decode('ascii')
+        else:
+            lower = ''
+
+        if not r.upper_inf:
+            upper = adapt(r.upper).getquoted().decode('ascii')
+        else:
+            upper = ''
+
+        return (f"'{r._bounds[0]}{lower},{upper}{r._bounds[1]}'").encode('ascii')
+
+
+# TODO: probably won't work with infs, nans and other tricky cases.
+register_adapter(NumericRange, NumberRangeAdapter)
+
+# Register globally typecasters and adapters for builtin range types.
+
+# note: the adapter is registered more than once, but this is harmless.
+int4range_caster = RangeCaster(NumberRangeAdapter, NumericRange,
+    oid=3904, subtype_oid=23, array_oid=3905)
+int4range_caster._register()
+
+int8range_caster = RangeCaster(NumberRangeAdapter, NumericRange,
+    oid=3926, subtype_oid=20, array_oid=3927)
+int8range_caster._register()
+
+numrange_caster = RangeCaster(NumberRangeAdapter, NumericRange,
+    oid=3906, subtype_oid=1700, array_oid=3907)
+numrange_caster._register()
+
+daterange_caster = RangeCaster('daterange', DateRange,
+    oid=3912, subtype_oid=1082, array_oid=3913)
+daterange_caster._register()
+
+tsrange_caster = RangeCaster('tsrange', DateTimeRange,
+    oid=3908, subtype_oid=1114, array_oid=3909)
+tsrange_caster._register()
+
+tstzrange_caster = RangeCaster('tstzrange', DateTimeTZRange,
+    oid=3910, subtype_oid=1184, array_oid=3911)
+tstzrange_caster._register()
Index: venv/Lib/site-packages/gunicorn/app/base.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/app/base.py b/venv/Lib/site-packages/gunicorn/app/base.py
new file mode 100644
--- /dev/null	(date 1630065626365)
+++ b/venv/Lib/site-packages/gunicorn/app/base.py	(date 1630065626365)
@@ -0,0 +1,231 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+import importlib.util
+import importlib.machinery
+import os
+import sys
+import traceback
+
+from gunicorn import util
+from gunicorn.arbiter import Arbiter
+from gunicorn.config import Config, get_default_config_file
+from gunicorn import debug
+
+
+class BaseApplication(object):
+    """
+    An application interface for configuring and loading
+    the various necessities for any given web framework.
+    """
+    def __init__(self, usage=None, prog=None):
+        self.usage = usage
+        self.cfg = None
+        self.callable = None
+        self.prog = prog
+        self.logger = None
+        self.do_load_config()
+
+    def do_load_config(self):
+        """
+        Loads the configuration
+        """
+        try:
+            self.load_default_config()
+            self.load_config()
+        except Exception as e:
+            print("\nError: %s" % str(e), file=sys.stderr)
+            sys.stderr.flush()
+            sys.exit(1)
+
+    def load_default_config(self):
+        # init configuration
+        self.cfg = Config(self.usage, prog=self.prog)
+
+    def init(self, parser, opts, args):
+        raise NotImplementedError
+
+    def load(self):
+        raise NotImplementedError
+
+    def load_config(self):
+        """
+        This method is used to load the configuration from one or several input(s).
+        Custom Command line, configuration file.
+        You have to override this method in your class.
+        """
+        raise NotImplementedError
+
+    def reload(self):
+        self.do_load_config()
+        if self.cfg.spew:
+            debug.spew()
+
+    def wsgi(self):
+        if self.callable is None:
+            self.callable = self.load()
+        return self.callable
+
+    def run(self):
+        try:
+            Arbiter(self).run()
+        except RuntimeError as e:
+            print("\nError: %s\n" % e, file=sys.stderr)
+            sys.stderr.flush()
+            sys.exit(1)
+
+
+class Application(BaseApplication):
+
+    # 'init' and 'load' methods are implemented by WSGIApplication.
+    # pylint: disable=abstract-method
+
+    def chdir(self):
+        # chdir to the configured path before loading,
+        # default is the current dir
+        os.chdir(self.cfg.chdir)
+
+        # add the path to sys.path
+        if self.cfg.chdir not in sys.path:
+            sys.path.insert(0, self.cfg.chdir)
+
+    def get_config_from_filename(self, filename):
+
+        if not os.path.exists(filename):
+            raise RuntimeError("%r doesn't exist" % filename)
+
+        ext = os.path.splitext(filename)[1]
+
+        try:
+            module_name = '__config__'
+            if ext in [".py", ".pyc"]:
+                spec = importlib.util.spec_from_file_location(module_name, filename)
+            else:
+                msg = "configuration file should have a valid Python extension.\n"
+                util.warn(msg)
+                loader_ = importlib.machinery.SourceFileLoader(module_name, filename)
+                spec = importlib.util.spec_from_file_location(module_name, filename, loader=loader_)
+            mod = importlib.util.module_from_spec(spec)
+            sys.modules[module_name] = mod
+            spec.loader.exec_module(mod)
+        except Exception:
+            print("Failed to read config file: %s" % filename, file=sys.stderr)
+            traceback.print_exc()
+            sys.stderr.flush()
+            sys.exit(1)
+
+        return vars(mod)
+
+    def get_config_from_module_name(self, module_name):
+        return vars(importlib.import_module(module_name))
+
+    def load_config_from_module_name_or_filename(self, location):
+        """
+        Loads the configuration file: the file is a python file, otherwise raise an RuntimeError
+        Exception or stop the process if the configuration file contains a syntax error.
+        """
+
+        if location.startswith("python:"):
+            module_name = location[len("python:"):]
+            cfg = self.get_config_from_module_name(module_name)
+        else:
+            if location.startswith("file:"):
+                filename = location[len("file:"):]
+            else:
+                filename = location
+            cfg = self.get_config_from_filename(filename)
+
+        for k, v in cfg.items():
+            # Ignore unknown names
+            if k not in self.cfg.settings:
+                continue
+            try:
+                self.cfg.set(k.lower(), v)
+            except Exception:
+                print("Invalid value for %s: %s\n" % (k, v), file=sys.stderr)
+                sys.stderr.flush()
+                raise
+
+        return cfg
+
+    def load_config_from_file(self, filename):
+        return self.load_config_from_module_name_or_filename(location=filename)
+
+    def load_config(self):
+        # parse console args
+        parser = self.cfg.parser()
+        args = parser.parse_args()
+
+        # optional settings from apps
+        cfg = self.init(parser, args, args.args)
+
+        # set up import paths and follow symlinks
+        self.chdir()
+
+        # Load up the any app specific configuration
+        if cfg:
+            for k, v in cfg.items():
+                self.cfg.set(k.lower(), v)
+
+        env_args = parser.parse_args(self.cfg.get_cmd_args_from_env())
+
+        if args.config:
+            self.load_config_from_file(args.config)
+        elif env_args.config:
+            self.load_config_from_file(env_args.config)
+        else:
+            default_config = get_default_config_file()
+            if default_config is not None:
+                self.load_config_from_file(default_config)
+
+        # Load up environment configuration
+        for k, v in vars(env_args).items():
+            if v is None:
+                continue
+            if k == "args":
+                continue
+            self.cfg.set(k.lower(), v)
+
+        # Lastly, update the configuration with any command line settings.
+        for k, v in vars(args).items():
+            if v is None:
+                continue
+            if k == "args":
+                continue
+            self.cfg.set(k.lower(), v)
+
+        # current directory might be changed by the config now
+        # set up import paths and follow symlinks
+        self.chdir()
+
+    def run(self):
+        if self.cfg.print_config:
+            print(self.cfg)
+
+        if self.cfg.print_config or self.cfg.check_config:
+            try:
+                self.load()
+            except Exception:
+                msg = "\nError while loading the application:\n"
+                print(msg, file=sys.stderr)
+                traceback.print_exc()
+                sys.stderr.flush()
+                sys.exit(1)
+            sys.exit(0)
+
+        if self.cfg.spew:
+            debug.spew()
+
+        if self.cfg.daemon:
+            util.daemonize(self.cfg.enable_stdio_inheritance)
+
+        # set python paths
+        if self.cfg.pythonpath:
+            paths = self.cfg.pythonpath.split(",")
+            for path in paths:
+                pythonpath = os.path.abspath(path)
+                if pythonpath not in sys.path:
+                    sys.path.insert(0, pythonpath)
+
+        super().run()
Index: venv/Lib/site-packages/psycopg2/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/psycopg2/__init__.py b/venv/Lib/site-packages/psycopg2/__init__.py
new file mode 100644
--- /dev/null	(date 1630065394238)
+++ b/venv/Lib/site-packages/psycopg2/__init__.py	(date 1630065394238)
@@ -0,0 +1,126 @@
+"""A Python driver for PostgreSQL
+
+psycopg is a PostgreSQL_ database adapter for the Python_ programming
+language. This is version 2, a complete rewrite of the original code to
+provide new-style classes for connection and cursor objects and other sweet
+candies. Like the original, psycopg 2 was written with the aim of being very
+small and fast, and stable as a rock.
+
+Homepage: https://psycopg.org/
+
+.. _PostgreSQL: https://www.postgresql.org/
+.. _Python: https://www.python.org/
+
+:Groups:
+  * `Connections creation`: connect
+  * `Value objects constructors`: Binary, Date, DateFromTicks, Time,
+    TimeFromTicks, Timestamp, TimestampFromTicks
+"""
+# psycopg/__init__.py - initialization of the psycopg module
+#
+# Copyright (C) 2003-2019 Federico Di Gregorio  <fog@debian.org>
+# Copyright (C) 2020-2021 The Psycopg Team
+#
+# psycopg2 is free software: you can redistribute it and/or modify it
+# under the terms of the GNU Lesser General Public License as published
+# by the Free Software Foundation, either version 3 of the License, or
+# (at your option) any later version.
+#
+# In addition, as a special exception, the copyright holders give
+# permission to link this program with the OpenSSL library (or with
+# modified versions of OpenSSL that use the same license as OpenSSL),
+# and distribute linked combinations including the two.
+#
+# You must obey the GNU Lesser General Public License in all respects for
+# all of the code used other than OpenSSL.
+#
+# psycopg2 is distributed in the hope that it will be useful, but WITHOUT
+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public
+# License for more details.
+
+# Import modules needed by _psycopg to allow tools like py2exe to do
+# their work without bothering about the module dependencies.
+
+# Note: the first internal import should be _psycopg, otherwise the real cause
+# of a failed loading of the C module may get hidden, see
+# https://archives.postgresql.org/psycopg/2011-02/msg00044.php
+
+# Import the DBAPI-2.0 stuff into top-level module.
+
+from psycopg2._psycopg import (                     # noqa
+    BINARY, NUMBER, STRING, DATETIME, ROWID,
+
+    Binary, Date, Time, Timestamp,
+    DateFromTicks, TimeFromTicks, TimestampFromTicks,
+
+    Error, Warning, DataError, DatabaseError, ProgrammingError, IntegrityError,
+    InterfaceError, InternalError, NotSupportedError, OperationalError,
+
+    _connect, apilevel, threadsafety, paramstyle,
+    __version__, __libpq_version__,
+)
+
+
+# Register default adapters.
+
+from psycopg2 import extensions as _ext
+_ext.register_adapter(tuple, _ext.SQL_IN)
+_ext.register_adapter(type(None), _ext.NoneAdapter)
+
+# Register the Decimal adapter here instead of in the C layer.
+# This way a new class is registered for each sub-interpreter.
+# See ticket #52
+from decimal import Decimal                         # noqa
+from psycopg2._psycopg import Decimal as Adapter    # noqa
+_ext.register_adapter(Decimal, Adapter)
+del Decimal, Adapter
+
+
+def connect(dsn=None, connection_factory=None, cursor_factory=None, **kwargs):
+    """
+    Create a new database connection.
+
+    The connection parameters can be specified as a string:
+
+        conn = psycopg2.connect("dbname=test user=postgres password=secret")
+
+    or using a set of keyword arguments:
+
+        conn = psycopg2.connect(database="test", user="postgres", password="secret")
+
+    Or as a mix of both. The basic connection parameters are:
+
+    - *dbname*: the database name
+    - *database*: the database name (only as keyword argument)
+    - *user*: user name used to authenticate
+    - *password*: password used to authenticate
+    - *host*: database host address (defaults to UNIX socket if not provided)
+    - *port*: connection port number (defaults to 5432 if not provided)
+
+    Using the *connection_factory* parameter a different class or connections
+    factory can be specified. It should be a callable object taking a dsn
+    argument.
+
+    Using the *cursor_factory* parameter, a new default cursor factory will be
+    used by cursor().
+
+    Using *async*=True an asynchronous connection will be created. *async_* is
+    a valid alias (for Python versions where ``async`` is a keyword).
+
+    Any other keyword parameter will be passed to the underlying client
+    library: the list of supported parameters depends on the library version.
+
+    """
+    kwasync = {}
+    if 'async' in kwargs:
+        kwasync['async'] = kwargs.pop('async')
+    if 'async_' in kwargs:
+        kwasync['async_'] = kwargs.pop('async_')
+
+    dsn = _ext.make_dsn(dsn, **kwargs)
+    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
+    if cursor_factory is not None:
+        conn.cursor_factory = cursor_factory
+
+    return conn
Index: venv/Lib/site-packages/psycopg2/_json.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/psycopg2/_json.py b/venv/Lib/site-packages/psycopg2/_json.py
new file mode 100644
--- /dev/null	(date 1630065394244)
+++ b/venv/Lib/site-packages/psycopg2/_json.py	(date 1630065394244)
@@ -0,0 +1,199 @@
+"""Implementation of the JSON adaptation objects
+
+This module exists to avoid a circular import problem: pyscopg2.extras depends
+on psycopg2.extension, so I can't create the default JSON typecasters in
+extensions importing register_json from extras.
+"""
+
+# psycopg/_json.py - Implementation of the JSON adaptation objects
+#
+# Copyright (C) 2012-2019 Daniele Varrazzo  <daniele.varrazzo@gmail.com>
+# Copyright (C) 2020-2021 The Psycopg Team
+#
+# psycopg2 is free software: you can redistribute it and/or modify it
+# under the terms of the GNU Lesser General Public License as published
+# by the Free Software Foundation, either version 3 of the License, or
+# (at your option) any later version.
+#
+# In addition, as a special exception, the copyright holders give
+# permission to link this program with the OpenSSL library (or with
+# modified versions of OpenSSL that use the same license as OpenSSL),
+# and distribute linked combinations including the two.
+#
+# You must obey the GNU Lesser General Public License in all respects for
+# all of the code used other than OpenSSL.
+#
+# psycopg2 is distributed in the hope that it will be useful, but WITHOUT
+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public
+# License for more details.
+
+import json
+
+from psycopg2._psycopg import ISQLQuote, QuotedString
+from psycopg2._psycopg import new_type, new_array_type, register_type
+
+
+# oids from PostgreSQL 9.2
+JSON_OID = 114
+JSONARRAY_OID = 199
+
+# oids from PostgreSQL 9.4
+JSONB_OID = 3802
+JSONBARRAY_OID = 3807
+
+
+class Json:
+    """
+    An `~psycopg2.extensions.ISQLQuote` wrapper to adapt a Python object to
+    :sql:`json` data type.
+
+    `!Json` can be used to wrap any object supported by the provided *dumps*
+    function. If none is provided, the standard :py:func:`json.dumps()` is
+    used.
+
+    """
+    def __init__(self, adapted, dumps=None):
+        self.adapted = adapted
+        self._conn = None
+        self._dumps = dumps or json.dumps
+
+    def __conform__(self, proto):
+        if proto is ISQLQuote:
+            return self
+
+    def dumps(self, obj):
+        """Serialize *obj* in JSON format.
+
+        The default is to call `!json.dumps()` or the *dumps* function
+        provided in the constructor. You can override this method to create a
+        customized JSON wrapper.
+        """
+        return self._dumps(obj)
+
+    def prepare(self, conn):
+        self._conn = conn
+
+    def getquoted(self):
+        s = self.dumps(self.adapted)
+        qs = QuotedString(s)
+        if self._conn is not None:
+            qs.prepare(self._conn)
+        return qs.getquoted()
+
+    def __str__(self):
+        # getquoted is binary
+        return self.getquoted().decode('ascii', 'replace')
+
+
+def register_json(conn_or_curs=None, globally=False, loads=None,
+                  oid=None, array_oid=None, name='json'):
+    """Create and register typecasters converting :sql:`json` type to Python objects.
+
+    :param conn_or_curs: a connection or cursor used to find the :sql:`json`
+        and :sql:`json[]` oids; the typecasters are registered in a scope
+        limited to this object, unless *globally* is set to `!True`. It can be
+        `!None` if the oids are provided
+    :param globally: if `!False` register the typecasters only on
+        *conn_or_curs*, otherwise register them globally
+    :param loads: the function used to parse the data into a Python object. If
+        `!None` use `!json.loads()`, where `!json` is the module chosen
+        according to the Python version (see above)
+    :param oid: the OID of the :sql:`json` type if known; If not, it will be
+        queried on *conn_or_curs*
+    :param array_oid: the OID of the :sql:`json[]` array type if known;
+        if not, it will be queried on *conn_or_curs*
+    :param name: the name of the data type to look for in *conn_or_curs*
+
+    The connection or cursor passed to the function will be used to query the
+    database and look for the OID of the :sql:`json` type (or an alternative
+    type if *name* if provided). No query is performed if *oid* and *array_oid*
+    are provided.  Raise `~psycopg2.ProgrammingError` if the type is not found.
+
+    """
+    if oid is None:
+        oid, array_oid = _get_json_oids(conn_or_curs, name)
+
+    JSON, JSONARRAY = _create_json_typecasters(
+        oid, array_oid, loads=loads, name=name.upper())
+
+    register_type(JSON, not globally and conn_or_curs or None)
+
+    if JSONARRAY is not None:
+        register_type(JSONARRAY, not globally and conn_or_curs or None)
+
+    return JSON, JSONARRAY
+
+
+def register_default_json(conn_or_curs=None, globally=False, loads=None):
+    """
+    Create and register :sql:`json` typecasters for PostgreSQL 9.2 and following.
+
+    Since PostgreSQL 9.2 :sql:`json` is a builtin type, hence its oid is known
+    and fixed. This function allows specifying a customized *loads* function
+    for the default :sql:`json` type without querying the database.
+    All the parameters have the same meaning of `register_json()`.
+    """
+    return register_json(conn_or_curs=conn_or_curs, globally=globally,
+        loads=loads, oid=JSON_OID, array_oid=JSONARRAY_OID)
+
+
+def register_default_jsonb(conn_or_curs=None, globally=False, loads=None):
+    """
+    Create and register :sql:`jsonb` typecasters for PostgreSQL 9.4 and following.
+
+    As in `register_default_json()`, the function allows to register a
+    customized *loads* function for the :sql:`jsonb` type at its known oid for
+    PostgreSQL 9.4 and following versions.  All the parameters have the same
+    meaning of `register_json()`.
+    """
+    return register_json(conn_or_curs=conn_or_curs, globally=globally,
+        loads=loads, oid=JSONB_OID, array_oid=JSONBARRAY_OID, name='jsonb')
+
+
+def _create_json_typecasters(oid, array_oid, loads=None, name='JSON'):
+    """Create typecasters for json data type."""
+    if loads is None:
+        loads = json.loads
+
+    def typecast_json(s, cur):
+        if s is None:
+            return None
+        return loads(s)
+
+    JSON = new_type((oid, ), name, typecast_json)
+    if array_oid is not None:
+        JSONARRAY = new_array_type((array_oid, ), f"{name}ARRAY", JSON)
+    else:
+        JSONARRAY = None
+
+    return JSON, JSONARRAY
+
+
+def _get_json_oids(conn_or_curs, name='json'):
+    # lazy imports
+    from psycopg2.extensions import STATUS_IN_TRANSACTION
+    from psycopg2.extras import _solve_conn_curs
+
+    conn, curs = _solve_conn_curs(conn_or_curs)
+
+    # Store the transaction status of the connection to revert it after use
+    conn_status = conn.status
+
+    # column typarray not available before PG 8.3
+    typarray = conn.info.server_version >= 80300 and "typarray" or "NULL"
+
+    # get the oid for the hstore
+    curs.execute(
+        "SELECT t.oid, %s FROM pg_type t WHERE t.typname = %%s;"
+        % typarray, (name,))
+    r = curs.fetchone()
+
+    # revert the status of the connection as before the command
+    if conn_status != STATUS_IN_TRANSACTION and not conn.autocommit:
+        conn.rollback()
+
+    if not r:
+        raise conn.ProgrammingError(f"{name} data type not found")
+
+    return r
Index: venv/Lib/site-packages/gunicorn/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/__init__.py b/venv/Lib/site-packages/gunicorn/__init__.py
new file mode 100644
--- /dev/null	(date 1630065626312)
+++ b/venv/Lib/site-packages/gunicorn/__init__.py	(date 1630065626312)
@@ -0,0 +1,9 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+version_info = (20, 1, 0)
+__version__ = ".".join([str(v) for v in version_info])
+SERVER = "gunicorn"
+SERVER_SOFTWARE = "%s/%s" % (SERVER, __version__)
Index: venv/Lib/site-packages/gunicorn/__main__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/__main__.py b/venv/Lib/site-packages/gunicorn/__main__.py
new file mode 100644
--- /dev/null	(date 1630065626315)
+++ b/venv/Lib/site-packages/gunicorn/__main__.py	(date 1630065626315)
@@ -0,0 +1,7 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+from gunicorn.app.wsgiapp import run
+run()
Index: venv/Lib/site-packages/psycopg2/tz.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/psycopg2/tz.py b/venv/Lib/site-packages/psycopg2/tz.py
new file mode 100644
--- /dev/null	(date 1630065394302)
+++ b/venv/Lib/site-packages/psycopg2/tz.py	(date 1630065394302)
@@ -0,0 +1,158 @@
+"""tzinfo implementations for psycopg2
+
+This module holds two different tzinfo implementations that can be used as
+the 'tzinfo' argument to datetime constructors, directly passed to psycopg
+functions or used to set the .tzinfo_factory attribute in cursors.
+"""
+# psycopg/tz.py - tzinfo implementation
+#
+# Copyright (C) 2003-2019 Federico Di Gregorio  <fog@debian.org>
+# Copyright (C) 2020-2021 The Psycopg Team
+#
+# psycopg2 is free software: you can redistribute it and/or modify it
+# under the terms of the GNU Lesser General Public License as published
+# by the Free Software Foundation, either version 3 of the License, or
+# (at your option) any later version.
+#
+# In addition, as a special exception, the copyright holders give
+# permission to link this program with the OpenSSL library (or with
+# modified versions of OpenSSL that use the same license as OpenSSL),
+# and distribute linked combinations including the two.
+#
+# You must obey the GNU Lesser General Public License in all respects for
+# all of the code used other than OpenSSL.
+#
+# psycopg2 is distributed in the hope that it will be useful, but WITHOUT
+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public
+# License for more details.
+
+import datetime
+import time
+
+ZERO = datetime.timedelta(0)
+
+
+class FixedOffsetTimezone(datetime.tzinfo):
+    """Fixed offset in minutes east from UTC.
+
+    This is exactly the implementation__ found in Python 2.3.x documentation,
+    with a small change to the `!__init__()` method to allow for pickling
+    and a default name in the form ``sHH:MM`` (``s`` is the sign.).
+
+    The implementation also caches instances. During creation, if a
+    FixedOffsetTimezone instance has previously been created with the same
+    offset and name that instance will be returned. This saves memory and
+    improves comparability.
+
+    .. versionchanged:: 2.9
+
+        The constructor can take either a timedelta or a number of minutes of
+        offset. Previously only minutes were supported.
+
+    .. __: https://docs.python.org/library/datetime.html
+    """
+    _name = None
+    _offset = ZERO
+
+    _cache = {}
+
+    def __init__(self, offset=None, name=None):
+        if offset is not None:
+            if not isinstance(offset, datetime.timedelta):
+                offset = datetime.timedelta(minutes=offset)
+            self._offset = offset
+        if name is not None:
+            self._name = name
+
+    def __new__(cls, offset=None, name=None):
+        """Return a suitable instance created earlier if it exists
+        """
+        key = (offset, name)
+        try:
+            return cls._cache[key]
+        except KeyError:
+            tz = super().__new__(cls, offset, name)
+            cls._cache[key] = tz
+            return tz
+
+    def __repr__(self):
+        return "psycopg2.tz.FixedOffsetTimezone(offset=%r, name=%r)" \
+            % (self._offset, self._name)
+
+    def __eq__(self, other):
+        if isinstance(other, FixedOffsetTimezone):
+            return self._offset == other._offset
+        else:
+            return NotImplemented
+
+    def __ne__(self, other):
+        if isinstance(other, FixedOffsetTimezone):
+            return self._offset != other._offset
+        else:
+            return NotImplemented
+
+    def __getinitargs__(self):
+        return self._offset, self._name
+
+    def utcoffset(self, dt):
+        return self._offset
+
+    def tzname(self, dt):
+        if self._name is not None:
+            return self._name
+
+        minutes, seconds = divmod(self._offset.total_seconds(), 60)
+        hours, minutes = divmod(minutes, 60)
+        rv = "%+03d" % hours
+        if minutes or seconds:
+            rv += ":%02d" % minutes
+            if seconds:
+                rv += ":%02d" % seconds
+
+        return rv
+
+    def dst(self, dt):
+        return ZERO
+
+
+STDOFFSET = datetime.timedelta(seconds=-time.timezone)
+if time.daylight:
+    DSTOFFSET = datetime.timedelta(seconds=-time.altzone)
+else:
+    DSTOFFSET = STDOFFSET
+DSTDIFF = DSTOFFSET - STDOFFSET
+
+
+class LocalTimezone(datetime.tzinfo):
+    """Platform idea of local timezone.
+
+    This is the exact implementation from the Python 2.3 documentation.
+    """
+    def utcoffset(self, dt):
+        if self._isdst(dt):
+            return DSTOFFSET
+        else:
+            return STDOFFSET
+
+    def dst(self, dt):
+        if self._isdst(dt):
+            return DSTDIFF
+        else:
+            return ZERO
+
+    def tzname(self, dt):
+        return time.tzname[self._isdst(dt)]
+
+    def _isdst(self, dt):
+        tt = (dt.year, dt.month, dt.day,
+              dt.hour, dt.minute, dt.second,
+              dt.weekday(), 0, -1)
+        stamp = time.mktime(tt)
+        tt = time.localtime(stamp)
+        return tt.tm_isdst > 0
+
+
+LOCAL = LocalTimezone()
+
+# TODO: pre-generate some interesting time zones?
Index: venv/Lib/site-packages/gunicorn/util.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/util.py b/venv/Lib/site-packages/gunicorn/util.py
new file mode 100644
--- /dev/null	(date 1630065626355)
+++ b/venv/Lib/site-packages/gunicorn/util.py	(date 1630065626355)
@@ -0,0 +1,639 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+import ast
+import email.utils
+import errno
+import fcntl
+import html
+import importlib
+import inspect
+import io
+import logging
+import os
+import pwd
+import random
+import re
+import socket
+import sys
+import textwrap
+import time
+import traceback
+import warnings
+
+import pkg_resources
+
+from gunicorn.errors import AppImportError
+from gunicorn.workers import SUPPORTED_WORKERS
+import urllib.parse
+
+REDIRECT_TO = getattr(os, 'devnull', '/dev/null')
+
+# Server and Date aren't technically hop-by-hop
+# headers, but they are in the purview of the
+# origin server which the WSGI spec says we should
+# act like. So we drop them and add our own.
+#
+# In the future, concatenation server header values
+# might be better, but nothing else does it and
+# dropping them is easier.
+hop_headers = set("""
+    connection keep-alive proxy-authenticate proxy-authorization
+    te trailers transfer-encoding upgrade
+    server date
+    """.split())
+
+try:
+    from setproctitle import setproctitle
+
+    def _setproctitle(title):
+        setproctitle("gunicorn: %s" % title)
+except ImportError:
+    def _setproctitle(title):
+        pass
+
+
+def load_class(uri, default="gunicorn.workers.sync.SyncWorker",
+               section="gunicorn.workers"):
+    if inspect.isclass(uri):
+        return uri
+    if uri.startswith("egg:"):
+        # uses entry points
+        entry_str = uri.split("egg:")[1]
+        try:
+            dist, name = entry_str.rsplit("#", 1)
+        except ValueError:
+            dist = entry_str
+            name = default
+
+        try:
+            return pkg_resources.load_entry_point(dist, section, name)
+        except Exception:
+            exc = traceback.format_exc()
+            msg = "class uri %r invalid or not found: \n\n[%s]"
+            raise RuntimeError(msg % (uri, exc))
+    else:
+        components = uri.split('.')
+        if len(components) == 1:
+            while True:
+                if uri.startswith("#"):
+                    uri = uri[1:]
+
+                if uri in SUPPORTED_WORKERS:
+                    components = SUPPORTED_WORKERS[uri].split(".")
+                    break
+
+                try:
+                    return pkg_resources.load_entry_point(
+                        "gunicorn", section, uri
+                    )
+                except Exception:
+                    exc = traceback.format_exc()
+                    msg = "class uri %r invalid or not found: \n\n[%s]"
+                    raise RuntimeError(msg % (uri, exc))
+
+        klass = components.pop(-1)
+
+        try:
+            mod = importlib.import_module('.'.join(components))
+        except:
+            exc = traceback.format_exc()
+            msg = "class uri %r invalid or not found: \n\n[%s]"
+            raise RuntimeError(msg % (uri, exc))
+        return getattr(mod, klass)
+
+
+positionals = (
+    inspect.Parameter.POSITIONAL_ONLY,
+    inspect.Parameter.POSITIONAL_OR_KEYWORD,
+)
+
+
+def get_arity(f):
+    sig = inspect.signature(f)
+    arity = 0
+
+    for param in sig.parameters.values():
+        if param.kind in positionals:
+            arity += 1
+
+    return arity
+
+
+def get_username(uid):
+    """ get the username for a user id"""
+    return pwd.getpwuid(uid).pw_name
+
+
+def set_owner_process(uid, gid, initgroups=False):
+    """ set user and group of workers processes """
+
+    if gid:
+        if uid:
+            try:
+                username = get_username(uid)
+            except KeyError:
+                initgroups = False
+
+        # versions of python < 2.6.2 don't manage unsigned int for
+        # groups like on osx or fedora
+        gid = abs(gid) & 0x7FFFFFFF
+
+        if initgroups:
+            os.initgroups(username, gid)
+        elif gid != os.getgid():
+            os.setgid(gid)
+
+    if uid:
+        os.setuid(uid)
+
+
+def chown(path, uid, gid):
+    os.chown(path, uid, gid)
+
+
+if sys.platform.startswith("win"):
+    def _waitfor(func, pathname, waitall=False):
+        # Perform the operation
+        func(pathname)
+        # Now setup the wait loop
+        if waitall:
+            dirname = pathname
+        else:
+            dirname, name = os.path.split(pathname)
+            dirname = dirname or '.'
+        # Check for `pathname` to be removed from the filesystem.
+        # The exponential backoff of the timeout amounts to a total
+        # of ~1 second after which the deletion is probably an error
+        # anyway.
+        # Testing on a i7@4.3GHz shows that usually only 1 iteration is
+        # required when contention occurs.
+        timeout = 0.001
+        while timeout < 1.0:
+            # Note we are only testing for the existence of the file(s) in
+            # the contents of the directory regardless of any security or
+            # access rights.  If we have made it this far, we have sufficient
+            # permissions to do that much using Python's equivalent of the
+            # Windows API FindFirstFile.
+            # Other Windows APIs can fail or give incorrect results when
+            # dealing with files that are pending deletion.
+            L = os.listdir(dirname)
+            if not L if waitall else name in L:
+                return
+            # Increase the timeout and try again
+            time.sleep(timeout)
+            timeout *= 2
+        warnings.warn('tests may fail, delete still pending for ' + pathname,
+                      RuntimeWarning, stacklevel=4)
+
+    def _unlink(filename):
+        _waitfor(os.unlink, filename)
+else:
+    _unlink = os.unlink
+
+
+def unlink(filename):
+    try:
+        _unlink(filename)
+    except OSError as error:
+        # The filename need not exist.
+        if error.errno not in (errno.ENOENT, errno.ENOTDIR):
+            raise
+
+
+def is_ipv6(addr):
+    try:
+        socket.inet_pton(socket.AF_INET6, addr)
+    except socket.error:  # not a valid address
+        return False
+    except ValueError:  # ipv6 not supported on this platform
+        return False
+    return True
+
+
+def parse_address(netloc, default_port='8000'):
+    if re.match(r'unix:(//)?', netloc):
+        return re.split(r'unix:(//)?', netloc)[-1]
+
+    if netloc.startswith("fd://"):
+        fd = netloc[5:]
+        try:
+            return int(fd)
+        except ValueError:
+            raise RuntimeError("%r is not a valid file descriptor." % fd) from None
+
+    if netloc.startswith("tcp://"):
+        netloc = netloc.split("tcp://")[1]
+    host, port = netloc, default_port
+
+    if '[' in netloc and ']' in netloc:
+        host = netloc.split(']')[0][1:]
+        port = (netloc.split(']:') + [default_port])[1]
+    elif ':' in netloc:
+        host, port = (netloc.split(':') + [default_port])[:2]
+    elif netloc == "":
+        host, port = "0.0.0.0", default_port
+
+    try:
+        port = int(port)
+    except ValueError:
+        raise RuntimeError("%r is not a valid port number." % port)
+
+    return host.lower(), port
+
+
+def close_on_exec(fd):
+    flags = fcntl.fcntl(fd, fcntl.F_GETFD)
+    flags |= fcntl.FD_CLOEXEC
+    fcntl.fcntl(fd, fcntl.F_SETFD, flags)
+
+
+def set_non_blocking(fd):
+    flags = fcntl.fcntl(fd, fcntl.F_GETFL) | os.O_NONBLOCK
+    fcntl.fcntl(fd, fcntl.F_SETFL, flags)
+
+
+def close(sock):
+    try:
+        sock.close()
+    except socket.error:
+        pass
+
+
+try:
+    from os import closerange
+except ImportError:
+    def closerange(fd_low, fd_high):
+        # Iterate through and close all file descriptors.
+        for fd in range(fd_low, fd_high):
+            try:
+                os.close(fd)
+            except OSError:  # ERROR, fd wasn't open to begin with (ignored)
+                pass
+
+
+def write_chunk(sock, data):
+    if isinstance(data, str):
+        data = data.encode('utf-8')
+    chunk_size = "%X\r\n" % len(data)
+    chunk = b"".join([chunk_size.encode('utf-8'), data, b"\r\n"])
+    sock.sendall(chunk)
+
+
+def write(sock, data, chunked=False):
+    if chunked:
+        return write_chunk(sock, data)
+    sock.sendall(data)
+
+
+def write_nonblock(sock, data, chunked=False):
+    timeout = sock.gettimeout()
+    if timeout != 0.0:
+        try:
+            sock.setblocking(0)
+            return write(sock, data, chunked)
+        finally:
+            sock.setblocking(1)
+    else:
+        return write(sock, data, chunked)
+
+
+def write_error(sock, status_int, reason, mesg):
+    html_error = textwrap.dedent("""\
+    <html>
+      <head>
+        <title>%(reason)s</title>
+      </head>
+      <body>
+        <h1><p>%(reason)s</p></h1>
+        %(mesg)s
+      </body>
+    </html>
+    """) % {"reason": reason, "mesg": html.escape(mesg)}
+
+    http = textwrap.dedent("""\
+    HTTP/1.1 %s %s\r
+    Connection: close\r
+    Content-Type: text/html\r
+    Content-Length: %d\r
+    \r
+    %s""") % (str(status_int), reason, len(html_error), html_error)
+    write_nonblock(sock, http.encode('latin1'))
+
+
+def _called_with_wrong_args(f):
+    """Check whether calling a function raised a ``TypeError`` because
+    the call failed or because something in the function raised the
+    error.
+
+    :param f: The function that was called.
+    :return: ``True`` if the call failed.
+    """
+    tb = sys.exc_info()[2]
+
+    try:
+        while tb is not None:
+            if tb.tb_frame.f_code is f.__code__:
+                # In the function, it was called successfully.
+                return False
+
+            tb = tb.tb_next
+
+        # Didn't reach the function.
+        return True
+    finally:
+        # Delete tb to break a circular reference in Python 2.
+        # https://docs.python.org/2/library/sys.html#sys.exc_info
+        del tb
+
+
+def import_app(module):
+    parts = module.split(":", 1)
+    if len(parts) == 1:
+        obj = "application"
+    else:
+        module, obj = parts[0], parts[1]
+
+    try:
+        mod = importlib.import_module(module)
+    except ImportError:
+        if module.endswith(".py") and os.path.exists(module):
+            msg = "Failed to find application, did you mean '%s:%s'?"
+            raise ImportError(msg % (module.rsplit(".", 1)[0], obj))
+        raise
+
+    # Parse obj as a single expression to determine if it's a valid
+    # attribute name or function call.
+    try:
+        expression = ast.parse(obj, mode="eval").body
+    except SyntaxError:
+        raise AppImportError(
+            "Failed to parse %r as an attribute name or function call." % obj
+        )
+
+    if isinstance(expression, ast.Name):
+        name = expression.id
+        args = kwargs = None
+    elif isinstance(expression, ast.Call):
+        # Ensure the function name is an attribute name only.
+        if not isinstance(expression.func, ast.Name):
+            raise AppImportError("Function reference must be a simple name: %r" % obj)
+
+        name = expression.func.id
+
+        # Parse the positional and keyword arguments as literals.
+        try:
+            args = [ast.literal_eval(arg) for arg in expression.args]
+            kwargs = {kw.arg: ast.literal_eval(kw.value) for kw in expression.keywords}
+        except ValueError:
+            # literal_eval gives cryptic error messages, show a generic
+            # message with the full expression instead.
+            raise AppImportError(
+                "Failed to parse arguments as literal values: %r" % obj
+            )
+    else:
+        raise AppImportError(
+            "Failed to parse %r as an attribute name or function call." % obj
+        )
+
+    is_debug = logging.root.level == logging.DEBUG
+    try:
+        app = getattr(mod, name)
+    except AttributeError:
+        if is_debug:
+            traceback.print_exception(*sys.exc_info())
+        raise AppImportError("Failed to find attribute %r in %r." % (name, module))
+
+    # If the expression was a function call, call the retrieved object
+    # to get the real application.
+    if args is not None:
+        try:
+            app = app(*args, **kwargs)
+        except TypeError as e:
+            # If the TypeError was due to bad arguments to the factory
+            # function, show Python's nice error message without a
+            # traceback.
+            if _called_with_wrong_args(app):
+                raise AppImportError(
+                    "".join(traceback.format_exception_only(TypeError, e)).strip()
+                )
+
+            # Otherwise it was raised from within the function, show the
+            # full traceback.
+            raise
+
+    if app is None:
+        raise AppImportError("Failed to find application object: %r" % obj)
+
+    if not callable(app):
+        raise AppImportError("Application object must be callable.")
+    return app
+
+
+def getcwd():
+    # get current path, try to use PWD env first
+    try:
+        a = os.stat(os.environ['PWD'])
+        b = os.stat(os.getcwd())
+        if a.st_ino == b.st_ino and a.st_dev == b.st_dev:
+            cwd = os.environ['PWD']
+        else:
+            cwd = os.getcwd()
+    except Exception:
+        cwd = os.getcwd()
+    return cwd
+
+
+def http_date(timestamp=None):
+    """Return the current date and time formatted for a message header."""
+    if timestamp is None:
+        timestamp = time.time()
+    s = email.utils.formatdate(timestamp, localtime=False, usegmt=True)
+    return s
+
+
+def is_hoppish(header):
+    return header.lower().strip() in hop_headers
+
+
+def daemonize(enable_stdio_inheritance=False):
+    """\
+    Standard daemonization of a process.
+    http://www.svbug.com/documentation/comp.unix.programmer-FAQ/faq_2.html#SEC16
+    """
+    if 'GUNICORN_FD' not in os.environ:
+        if os.fork():
+            os._exit(0)
+        os.setsid()
+
+        if os.fork():
+            os._exit(0)
+
+        os.umask(0o22)
+
+        # In both the following any file descriptors above stdin
+        # stdout and stderr are left untouched. The inheritance
+        # option simply allows one to have output go to a file
+        # specified by way of shell redirection when not wanting
+        # to use --error-log option.
+
+        if not enable_stdio_inheritance:
+            # Remap all of stdin, stdout and stderr on to
+            # /dev/null. The expectation is that users have
+            # specified the --error-log option.
+
+            closerange(0, 3)
+
+            fd_null = os.open(REDIRECT_TO, os.O_RDWR)
+
+            if fd_null != 0:
+                os.dup2(fd_null, 0)
+
+            os.dup2(fd_null, 1)
+            os.dup2(fd_null, 2)
+
+        else:
+            fd_null = os.open(REDIRECT_TO, os.O_RDWR)
+
+            # Always redirect stdin to /dev/null as we would
+            # never expect to need to read interactive input.
+
+            if fd_null != 0:
+                os.close(0)
+                os.dup2(fd_null, 0)
+
+            # If stdout and stderr are still connected to
+            # their original file descriptors we check to see
+            # if they are associated with terminal devices.
+            # When they are we map them to /dev/null so that
+            # are still detached from any controlling terminal
+            # properly. If not we preserve them as they are.
+            #
+            # If stdin and stdout were not hooked up to the
+            # original file descriptors, then all bets are
+            # off and all we can really do is leave them as
+            # they were.
+            #
+            # This will allow 'gunicorn ... > output.log 2>&1'
+            # to work with stdout/stderr going to the file
+            # as expected.
+            #
+            # Note that if using --error-log option, the log
+            # file specified through shell redirection will
+            # only be used up until the log file specified
+            # by the option takes over. As it replaces stdout
+            # and stderr at the file descriptor level, then
+            # anything using stdout or stderr, including having
+            # cached a reference to them, will still work.
+
+            def redirect(stream, fd_expect):
+                try:
+                    fd = stream.fileno()
+                    if fd == fd_expect and stream.isatty():
+                        os.close(fd)
+                        os.dup2(fd_null, fd)
+                except AttributeError:
+                    pass
+
+            redirect(sys.stdout, 1)
+            redirect(sys.stderr, 2)
+
+
+def seed():
+    try:
+        random.seed(os.urandom(64))
+    except NotImplementedError:
+        random.seed('%s.%s' % (time.time(), os.getpid()))
+
+
+def check_is_writeable(path):
+    try:
+        f = open(path, 'a')
+    except IOError as e:
+        raise RuntimeError("Error: '%s' isn't writable [%r]" % (path, e))
+    f.close()
+
+
+def to_bytestring(value, encoding="utf8"):
+    """Converts a string argument to a byte string"""
+    if isinstance(value, bytes):
+        return value
+    if not isinstance(value, str):
+        raise TypeError('%r is not a string' % value)
+
+    return value.encode(encoding)
+
+
+def has_fileno(obj):
+    if not hasattr(obj, "fileno"):
+        return False
+
+    # check BytesIO case and maybe others
+    try:
+        obj.fileno()
+    except (AttributeError, IOError, io.UnsupportedOperation):
+        return False
+
+    return True
+
+
+def warn(msg):
+    print("!!!", file=sys.stderr)
+
+    lines = msg.splitlines()
+    for i, line in enumerate(lines):
+        if i == 0:
+            line = "WARNING: %s" % line
+        print("!!! %s" % line, file=sys.stderr)
+
+    print("!!!\n", file=sys.stderr)
+    sys.stderr.flush()
+
+
+def make_fail_app(msg):
+    msg = to_bytestring(msg)
+
+    def app(environ, start_response):
+        start_response("500 Internal Server Error", [
+            ("Content-Type", "text/plain"),
+            ("Content-Length", str(len(msg)))
+        ])
+        return [msg]
+
+    return app
+
+
+def split_request_uri(uri):
+    if uri.startswith("//"):
+        # When the path starts with //, urlsplit considers it as a
+        # relative uri while the RFC says we should consider it as abs_path
+        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html#sec5.1.2
+        # We use temporary dot prefix to workaround this behaviour
+        parts = urllib.parse.urlsplit("." + uri)
+        return parts._replace(path=parts.path[1:])
+
+    return urllib.parse.urlsplit(uri)
+
+
+# From six.reraise
+def reraise(tp, value, tb=None):
+    try:
+        if value is None:
+            value = tp()
+        if value.__traceback__ is not tb:
+            raise value.with_traceback(tb)
+        raise value
+    finally:
+        value = None
+        tb = None
+
+
+def bytes_to_str(b):
+    if isinstance(b, str):
+        return b
+    return str(b, 'latin1')
+
+
+def unquote_to_wsgi_str(string):
+    return urllib.parse.unquote_to_bytes(string).decode('latin-1')
Index: venv/Lib/site-packages/psycopg2/_ipaddress.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/psycopg2/_ipaddress.py b/venv/Lib/site-packages/psycopg2/_ipaddress.py
new file mode 100644
--- /dev/null	(date 1630065394241)
+++ b/venv/Lib/site-packages/psycopg2/_ipaddress.py	(date 1630065394241)
@@ -0,0 +1,90 @@
+"""Implementation of the ipaddres-based network types adaptation
+"""
+
+# psycopg/_ipaddress.py - Ipaddres-based network types adaptation
+#
+# Copyright (C) 2016-2019 Daniele Varrazzo  <daniele.varrazzo@gmail.com>
+# Copyright (C) 2020-2021 The Psycopg Team
+#
+# psycopg2 is free software: you can redistribute it and/or modify it
+# under the terms of the GNU Lesser General Public License as published
+# by the Free Software Foundation, either version 3 of the License, or
+# (at your option) any later version.
+#
+# In addition, as a special exception, the copyright holders give
+# permission to link this program with the OpenSSL library (or with
+# modified versions of OpenSSL that use the same license as OpenSSL),
+# and distribute linked combinations including the two.
+#
+# You must obey the GNU Lesser General Public License in all respects for
+# all of the code used other than OpenSSL.
+#
+# psycopg2 is distributed in the hope that it will be useful, but WITHOUT
+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public
+# License for more details.
+
+from psycopg2.extensions import (
+    new_type, new_array_type, register_type, register_adapter, QuotedString)
+
+# The module is imported on register_ipaddress
+ipaddress = None
+
+# The typecasters are created only once
+_casters = None
+
+
+def register_ipaddress(conn_or_curs=None):
+    """
+    Register conversion support between `ipaddress` objects and `network types`__.
+
+    :param conn_or_curs: the scope where to register the type casters.
+        If `!None` register them globally.
+
+    After the function is called, PostgreSQL :sql:`inet` values will be
+    converted into `~ipaddress.IPv4Interface` or `~ipaddress.IPv6Interface`
+    objects, :sql:`cidr` values into into `~ipaddress.IPv4Network` or
+    `~ipaddress.IPv6Network`.
+
+    .. __: https://www.postgresql.org/docs/current/static/datatype-net-types.html
+    """
+    global ipaddress
+    import ipaddress
+
+    global _casters
+    if _casters is None:
+        _casters = _make_casters()
+
+    for c in _casters:
+        register_type(c, conn_or_curs)
+
+    for t in [ipaddress.IPv4Interface, ipaddress.IPv6Interface,
+              ipaddress.IPv4Network, ipaddress.IPv6Network]:
+        register_adapter(t, adapt_ipaddress)
+
+
+def _make_casters():
+    inet = new_type((869,), 'INET', cast_interface)
+    ainet = new_array_type((1041,), 'INET[]', inet)
+
+    cidr = new_type((650,), 'CIDR', cast_network)
+    acidr = new_array_type((651,), 'CIDR[]', cidr)
+
+    return [inet, ainet, cidr, acidr]
+
+
+def cast_interface(s, cur=None):
+    if s is None:
+        return None
+    # Py2 version force the use of unicode. meh.
+    return ipaddress.ip_interface(str(s))
+
+
+def cast_network(s, cur=None):
+    if s is None:
+        return None
+    return ipaddress.ip_network(str(s))
+
+
+def adapt_ipaddress(obj):
+    return QuotedString(str(obj))
Index: venv/Lib/site-packages/gunicorn/sock.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/sock.py b/venv/Lib/site-packages/gunicorn/sock.py
new file mode 100644
--- /dev/null	(date 1630065626344)
+++ b/venv/Lib/site-packages/gunicorn/sock.py	(date 1630065626344)
@@ -0,0 +1,212 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+import errno
+import os
+import socket
+import stat
+import sys
+import time
+
+from gunicorn import util
+
+
+class BaseSocket(object):
+
+    def __init__(self, address, conf, log, fd=None):
+        self.log = log
+        self.conf = conf
+
+        self.cfg_addr = address
+        if fd is None:
+            sock = socket.socket(self.FAMILY, socket.SOCK_STREAM)
+            bound = False
+        else:
+            sock = socket.fromfd(fd, self.FAMILY, socket.SOCK_STREAM)
+            os.close(fd)
+            bound = True
+
+        self.sock = self.set_options(sock, bound=bound)
+
+    def __str__(self):
+        return "<socket %d>" % self.sock.fileno()
+
+    def __getattr__(self, name):
+        return getattr(self.sock, name)
+
+    def set_options(self, sock, bound=False):
+        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
+        if (self.conf.reuse_port
+            and hasattr(socket, 'SO_REUSEPORT')):  # pragma: no cover
+            try:
+                sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)
+            except socket.error as err:
+                if err.errno not in (errno.ENOPROTOOPT, errno.EINVAL):
+                    raise
+        if not bound:
+            self.bind(sock)
+        sock.setblocking(0)
+
+        # make sure that the socket can be inherited
+        if hasattr(sock, "set_inheritable"):
+            sock.set_inheritable(True)
+
+        sock.listen(self.conf.backlog)
+        return sock
+
+    def bind(self, sock):
+        sock.bind(self.cfg_addr)
+
+    def close(self):
+        if self.sock is None:
+            return
+
+        try:
+            self.sock.close()
+        except socket.error as e:
+            self.log.info("Error while closing socket %s", str(e))
+
+        self.sock = None
+
+
+class TCPSocket(BaseSocket):
+
+    FAMILY = socket.AF_INET
+
+    def __str__(self):
+        if self.conf.is_ssl:
+            scheme = "https"
+        else:
+            scheme = "http"
+
+        addr = self.sock.getsockname()
+        return "%s://%s:%d" % (scheme, addr[0], addr[1])
+
+    def set_options(self, sock, bound=False):
+        sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)
+        return super().set_options(sock, bound=bound)
+
+
+class TCP6Socket(TCPSocket):
+
+    FAMILY = socket.AF_INET6
+
+    def __str__(self):
+        (host, port, _, _) = self.sock.getsockname()
+        return "http://[%s]:%d" % (host, port)
+
+
+class UnixSocket(BaseSocket):
+
+    FAMILY = socket.AF_UNIX
+
+    def __init__(self, addr, conf, log, fd=None):
+        if fd is None:
+            try:
+                st = os.stat(addr)
+            except OSError as e:
+                if e.args[0] != errno.ENOENT:
+                    raise
+            else:
+                if stat.S_ISSOCK(st.st_mode):
+                    os.remove(addr)
+                else:
+                    raise ValueError("%r is not a socket" % addr)
+        super().__init__(addr, conf, log, fd=fd)
+
+    def __str__(self):
+        return "unix:%s" % self.cfg_addr
+
+    def bind(self, sock):
+        old_umask = os.umask(self.conf.umask)
+        sock.bind(self.cfg_addr)
+        util.chown(self.cfg_addr, self.conf.uid, self.conf.gid)
+        os.umask(old_umask)
+
+
+def _sock_type(addr):
+    if isinstance(addr, tuple):
+        if util.is_ipv6(addr[0]):
+            sock_type = TCP6Socket
+        else:
+            sock_type = TCPSocket
+    elif isinstance(addr, (str, bytes)):
+        sock_type = UnixSocket
+    else:
+        raise TypeError("Unable to create socket from: %r" % addr)
+    return sock_type
+
+
+def create_sockets(conf, log, fds=None):
+    """
+    Create a new socket for the configured addresses or file descriptors.
+
+    If a configured address is a tuple then a TCP socket is created.
+    If it is a string, a Unix socket is created. Otherwise, a TypeError is
+    raised.
+    """
+    listeners = []
+
+    # get it only once
+    addr = conf.address
+    fdaddr = [bind for bind in addr if isinstance(bind, int)]
+    if fds:
+        fdaddr += list(fds)
+    laddr = [bind for bind in addr if not isinstance(bind, int)]
+
+    # check ssl config early to raise the error on startup
+    # only the certfile is needed since it can contains the keyfile
+    if conf.certfile and not os.path.exists(conf.certfile):
+        raise ValueError('certfile "%s" does not exist' % conf.certfile)
+
+    if conf.keyfile and not os.path.exists(conf.keyfile):
+        raise ValueError('keyfile "%s" does not exist' % conf.keyfile)
+
+    # sockets are already bound
+    if fdaddr:
+        for fd in fdaddr:
+            sock = socket.fromfd(fd, socket.AF_UNIX, socket.SOCK_STREAM)
+            sock_name = sock.getsockname()
+            sock_type = _sock_type(sock_name)
+            listener = sock_type(sock_name, conf, log, fd=fd)
+            listeners.append(listener)
+
+        return listeners
+
+    # no sockets is bound, first initialization of gunicorn in this env.
+    for addr in laddr:
+        sock_type = _sock_type(addr)
+        sock = None
+        for i in range(5):
+            try:
+                sock = sock_type(addr, conf, log)
+            except socket.error as e:
+                if e.args[0] == errno.EADDRINUSE:
+                    log.error("Connection in use: %s", str(addr))
+                if e.args[0] == errno.EADDRNOTAVAIL:
+                    log.error("Invalid address: %s", str(addr))
+                if i < 5:
+                    msg = "connection to {addr} failed: {error}"
+                    log.debug(msg.format(addr=str(addr), error=str(e)))
+                    log.error("Retrying in 1 second.")
+                    time.sleep(1)
+            else:
+                break
+
+        if sock is None:
+            log.error("Can't connect to %s", str(addr))
+            sys.exit(1)
+
+        listeners.append(sock)
+
+    return listeners
+
+
+def close_sockets(listeners, unlink=True):
+    for sock in listeners:
+        sock_name = sock.getsockname()
+        sock.close()
+        if unlink and _sock_type(sock_name) is UnixSocket:
+            os.unlink(sock_name)
Index: venv/Lib/site-packages/psycopg2/pool.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/psycopg2/pool.py b/venv/Lib/site-packages/psycopg2/pool.py
new file mode 100644
--- /dev/null	(date 1630065394298)
+++ b/venv/Lib/site-packages/psycopg2/pool.py	(date 1630065394298)
@@ -0,0 +1,187 @@
+"""Connection pooling for psycopg2
+
+This module implements thread-safe (and not) connection pools.
+"""
+# psycopg/pool.py - pooling code for psycopg
+#
+# Copyright (C) 2003-2019 Federico Di Gregorio  <fog@debian.org>
+# Copyright (C) 2020-2021 The Psycopg Team
+#
+# psycopg2 is free software: you can redistribute it and/or modify it
+# under the terms of the GNU Lesser General Public License as published
+# by the Free Software Foundation, either version 3 of the License, or
+# (at your option) any later version.
+#
+# In addition, as a special exception, the copyright holders give
+# permission to link this program with the OpenSSL library (or with
+# modified versions of OpenSSL that use the same license as OpenSSL),
+# and distribute linked combinations including the two.
+#
+# You must obey the GNU Lesser General Public License in all respects for
+# all of the code used other than OpenSSL.
+#
+# psycopg2 is distributed in the hope that it will be useful, but WITHOUT
+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public
+# License for more details.
+
+import psycopg2
+from psycopg2 import extensions as _ext
+
+
+class PoolError(psycopg2.Error):
+    pass
+
+
+class AbstractConnectionPool:
+    """Generic key-based pooling code."""
+
+    def __init__(self, minconn, maxconn, *args, **kwargs):
+        """Initialize the connection pool.
+
+        New 'minconn' connections are created immediately calling 'connfunc'
+        with given parameters. The connection pool will support a maximum of
+        about 'maxconn' connections.
+        """
+        self.minconn = int(minconn)
+        self.maxconn = int(maxconn)
+        self.closed = False
+
+        self._args = args
+        self._kwargs = kwargs
+
+        self._pool = []
+        self._used = {}
+        self._rused = {}    # id(conn) -> key map
+        self._keys = 0
+
+        for i in range(self.minconn):
+            self._connect()
+
+    def _connect(self, key=None):
+        """Create a new connection and assign it to 'key' if not None."""
+        conn = psycopg2.connect(*self._args, **self._kwargs)
+        if key is not None:
+            self._used[key] = conn
+            self._rused[id(conn)] = key
+        else:
+            self._pool.append(conn)
+        return conn
+
+    def _getkey(self):
+        """Return a new unique key."""
+        self._keys += 1
+        return self._keys
+
+    def _getconn(self, key=None):
+        """Get a free connection and assign it to 'key' if not None."""
+        if self.closed:
+            raise PoolError("connection pool is closed")
+        if key is None:
+            key = self._getkey()
+
+        if key in self._used:
+            return self._used[key]
+
+        if self._pool:
+            self._used[key] = conn = self._pool.pop()
+            self._rused[id(conn)] = key
+            return conn
+        else:
+            if len(self._used) == self.maxconn:
+                raise PoolError("connection pool exhausted")
+            return self._connect(key)
+
+    def _putconn(self, conn, key=None, close=False):
+        """Put away a connection."""
+        if self.closed:
+            raise PoolError("connection pool is closed")
+
+        if key is None:
+            key = self._rused.get(id(conn))
+            if key is None:
+                raise PoolError("trying to put unkeyed connection")
+
+        if len(self._pool) < self.minconn and not close:
+            # Return the connection into a consistent state before putting
+            # it back into the pool
+            if not conn.closed:
+                status = conn.info.transaction_status
+                if status == _ext.TRANSACTION_STATUS_UNKNOWN:
+                    # server connection lost
+                    conn.close()
+                elif status != _ext.TRANSACTION_STATUS_IDLE:
+                    # connection in error or in transaction
+                    conn.rollback()
+                    self._pool.append(conn)
+                else:
+                    # regular idle connection
+                    self._pool.append(conn)
+            # If the connection is closed, we just discard it.
+        else:
+            conn.close()
+
+        # here we check for the presence of key because it can happen that a
+        # thread tries to put back a connection after a call to close
+        if not self.closed or key in self._used:
+            del self._used[key]
+            del self._rused[id(conn)]
+
+    def _closeall(self):
+        """Close all connections.
+
+        Note that this can lead to some code fail badly when trying to use
+        an already closed connection. If you call .closeall() make sure
+        your code can deal with it.
+        """
+        if self.closed:
+            raise PoolError("connection pool is closed")
+        for conn in self._pool + list(self._used.values()):
+            try:
+                conn.close()
+            except Exception:
+                pass
+        self.closed = True
+
+
+class SimpleConnectionPool(AbstractConnectionPool):
+    """A connection pool that can't be shared across different threads."""
+
+    getconn = AbstractConnectionPool._getconn
+    putconn = AbstractConnectionPool._putconn
+    closeall = AbstractConnectionPool._closeall
+
+
+class ThreadedConnectionPool(AbstractConnectionPool):
+    """A connection pool that works with the threading module."""
+
+    def __init__(self, minconn, maxconn, *args, **kwargs):
+        """Initialize the threading lock."""
+        import threading
+        AbstractConnectionPool.__init__(
+            self, minconn, maxconn, *args, **kwargs)
+        self._lock = threading.Lock()
+
+    def getconn(self, key=None):
+        """Get a free connection and assign it to 'key' if not None."""
+        self._lock.acquire()
+        try:
+            return self._getconn(key)
+        finally:
+            self._lock.release()
+
+    def putconn(self, conn=None, key=None, close=False):
+        """Put away an unused connection."""
+        self._lock.acquire()
+        try:
+            self._putconn(conn, key, close)
+        finally:
+            self._lock.release()
+
+    def closeall(self):
+        """Close all connections (even the one currently in use.)"""
+        self._lock.acquire()
+        try:
+            self._closeall()
+        finally:
+            self._lock.release()
Index: venv/Lib/site-packages/psycopg2/sql.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/psycopg2/sql.py b/venv/Lib/site-packages/psycopg2/sql.py
new file mode 100644
--- /dev/null	(date 1630065394300)
+++ b/venv/Lib/site-packages/psycopg2/sql.py	(date 1630065394300)
@@ -0,0 +1,455 @@
+"""SQL composition utility module
+"""
+
+# psycopg/sql.py - SQL composition utility module
+#
+# Copyright (C) 2016-2019 Daniele Varrazzo  <daniele.varrazzo@gmail.com>
+# Copyright (C) 2020-2021 The Psycopg Team
+#
+# psycopg2 is free software: you can redistribute it and/or modify it
+# under the terms of the GNU Lesser General Public License as published
+# by the Free Software Foundation, either version 3 of the License, or
+# (at your option) any later version.
+#
+# In addition, as a special exception, the copyright holders give
+# permission to link this program with the OpenSSL library (or with
+# modified versions of OpenSSL that use the same license as OpenSSL),
+# and distribute linked combinations including the two.
+#
+# You must obey the GNU Lesser General Public License in all respects for
+# all of the code used other than OpenSSL.
+#
+# psycopg2 is distributed in the hope that it will be useful, but WITHOUT
+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public
+# License for more details.
+
+import string
+
+from psycopg2 import extensions as ext
+
+
+_formatter = string.Formatter()
+
+
+class Composable:
+    """
+    Abstract base class for objects that can be used to compose an SQL string.
+
+    `!Composable` objects can be passed directly to `~cursor.execute()`,
+    `~cursor.executemany()`, `~cursor.copy_expert()` in place of the query
+    string.
+
+    `!Composable` objects can be joined using the ``+`` operator: the result
+    will be a `Composed` instance containing the objects joined. The operator
+    ``*`` is also supported with an integer argument: the result is a
+    `!Composed` instance containing the left argument repeated as many times as
+    requested.
+    """
+    def __init__(self, wrapped):
+        self._wrapped = wrapped
+
+    def __repr__(self):
+        return f"{self.__class__.__name__}({self._wrapped!r})"
+
+    def as_string(self, context):
+        """
+        Return the string value of the object.
+
+        :param context: the context to evaluate the string into.
+        :type context: `connection` or `cursor`
+
+        The method is automatically invoked by `~cursor.execute()`,
+        `~cursor.executemany()`, `~cursor.copy_expert()` if a `!Composable` is
+        passed instead of the query string.
+        """
+        raise NotImplementedError
+
+    def __add__(self, other):
+        if isinstance(other, Composed):
+            return Composed([self]) + other
+        if isinstance(other, Composable):
+            return Composed([self]) + Composed([other])
+        else:
+            return NotImplemented
+
+    def __mul__(self, n):
+        return Composed([self] * n)
+
+    def __eq__(self, other):
+        return type(self) is type(other) and self._wrapped == other._wrapped
+
+    def __ne__(self, other):
+        return not self.__eq__(other)
+
+
+class Composed(Composable):
+    """
+    A `Composable` object made of a sequence of `!Composable`.
+
+    The object is usually created using `!Composable` operators and methods.
+    However it is possible to create a `!Composed` directly specifying a
+    sequence of `!Composable` as arguments.
+
+    Example::
+
+        >>> comp = sql.Composed(
+        ...     [sql.SQL("insert into "), sql.Identifier("table")])
+        >>> print(comp.as_string(conn))
+        insert into "table"
+
+    `!Composed` objects are iterable (so they can be used in `SQL.join` for
+    instance).
+    """
+    def __init__(self, seq):
+        wrapped = []
+        for i in seq:
+            if not isinstance(i, Composable):
+                raise TypeError(
+                    f"Composed elements must be Composable, got {i!r} instead")
+            wrapped.append(i)
+
+        super().__init__(wrapped)
+
+    @property
+    def seq(self):
+        """The list of the content of the `!Composed`."""
+        return list(self._wrapped)
+
+    def as_string(self, context):
+        rv = []
+        for i in self._wrapped:
+            rv.append(i.as_string(context))
+        return ''.join(rv)
+
+    def __iter__(self):
+        return iter(self._wrapped)
+
+    def __add__(self, other):
+        if isinstance(other, Composed):
+            return Composed(self._wrapped + other._wrapped)
+        if isinstance(other, Composable):
+            return Composed(self._wrapped + [other])
+        else:
+            return NotImplemented
+
+    def join(self, joiner):
+        """
+        Return a new `!Composed` interposing the *joiner* with the `!Composed` items.
+
+        The *joiner* must be a `SQL` or a string which will be interpreted as
+        an `SQL`.
+
+        Example::
+
+            >>> fields = sql.Identifier('foo') + sql.Identifier('bar')  # a Composed
+            >>> print(fields.join(', ').as_string(conn))
+            "foo", "bar"
+
+        """
+        if isinstance(joiner, str):
+            joiner = SQL(joiner)
+        elif not isinstance(joiner, SQL):
+            raise TypeError(
+                "Composed.join() argument must be a string or an SQL")
+
+        return joiner.join(self)
+
+
+class SQL(Composable):
+    """
+    A `Composable` representing a snippet of SQL statement.
+
+    `!SQL` exposes `join()` and `format()` methods useful to create a template
+    where to merge variable parts of a query (for instance field or table
+    names).
+
+    The *string* doesn't undergo any form of escaping, so it is not suitable to
+    represent variable identifiers or values: you should only use it to pass
+    constant strings representing templates or snippets of SQL statements; use
+    other objects such as `Identifier` or `Literal` to represent variable
+    parts.
+
+    Example::
+
+        >>> query = sql.SQL("select {0} from {1}").format(
+        ...    sql.SQL(', ').join([sql.Identifier('foo'), sql.Identifier('bar')]),
+        ...    sql.Identifier('table'))
+        >>> print(query.as_string(conn))
+        select "foo", "bar" from "table"
+    """
+    def __init__(self, string):
+        if not isinstance(string, str):
+            raise TypeError("SQL values must be strings")
+        super().__init__(string)
+
+    @property
+    def string(self):
+        """The string wrapped by the `!SQL` object."""
+        return self._wrapped
+
+    def as_string(self, context):
+        return self._wrapped
+
+    def format(self, *args, **kwargs):
+        """
+        Merge `Composable` objects into a template.
+
+        :param `Composable` args: parameters to replace to numbered
+            (``{0}``, ``{1}``) or auto-numbered (``{}``) placeholders
+        :param `Composable` kwargs: parameters to replace to named (``{name}``)
+            placeholders
+        :return: the union of the `!SQL` string with placeholders replaced
+        :rtype: `Composed`
+
+        The method is similar to the Python `str.format()` method: the string
+        template supports auto-numbered (``{}``), numbered (``{0}``,
+        ``{1}``...), and named placeholders (``{name}``), with positional
+        arguments replacing the numbered placeholders and keywords replacing
+        the named ones. However placeholder modifiers (``{0!r}``, ``{0:<10}``)
+        are not supported. Only `!Composable` objects can be passed to the
+        template.
+
+        Example::
+
+            >>> print(sql.SQL("select * from {} where {} = %s")
+            ...     .format(sql.Identifier('people'), sql.Identifier('id'))
+            ...     .as_string(conn))
+            select * from "people" where "id" = %s
+
+            >>> print(sql.SQL("select * from {tbl} where {pkey} = %s")
+            ...     .format(tbl=sql.Identifier('people'), pkey=sql.Identifier('id'))
+            ...     .as_string(conn))
+            select * from "people" where "id" = %s
+
+        """
+        rv = []
+        autonum = 0
+        for pre, name, spec, conv in _formatter.parse(self._wrapped):
+            if spec:
+                raise ValueError("no format specification supported by SQL")
+            if conv:
+                raise ValueError("no format conversion supported by SQL")
+            if pre:
+                rv.append(SQL(pre))
+
+            if name is None:
+                continue
+
+            if name.isdigit():
+                if autonum:
+                    raise ValueError(
+                        "cannot switch from automatic field numbering to manual")
+                rv.append(args[int(name)])
+                autonum = None
+
+            elif not name:
+                if autonum is None:
+                    raise ValueError(
+                        "cannot switch from manual field numbering to automatic")
+                rv.append(args[autonum])
+                autonum += 1
+
+            else:
+                rv.append(kwargs[name])
+
+        return Composed(rv)
+
+    def join(self, seq):
+        """
+        Join a sequence of `Composable`.
+
+        :param seq: the elements to join.
+        :type seq: iterable of `!Composable`
+
+        Use the `!SQL` object's *string* to separate the elements in *seq*.
+        Note that `Composed` objects are iterable too, so they can be used as
+        argument for this method.
+
+        Example::
+
+            >>> snip = sql.SQL(', ').join(
+            ...     sql.Identifier(n) for n in ['foo', 'bar', 'baz'])
+            >>> print(snip.as_string(conn))
+            "foo", "bar", "baz"
+        """
+        rv = []
+        it = iter(seq)
+        try:
+            rv.append(next(it))
+        except StopIteration:
+            pass
+        else:
+            for i in it:
+                rv.append(self)
+                rv.append(i)
+
+        return Composed(rv)
+
+
+class Identifier(Composable):
+    """
+    A `Composable` representing an SQL identifier or a dot-separated sequence.
+
+    Identifiers usually represent names of database objects, such as tables or
+    fields. PostgreSQL identifiers follow `different rules`__ than SQL string
+    literals for escaping (e.g. they use double quotes instead of single).
+
+    .. __: https://www.postgresql.org/docs/current/static/sql-syntax-lexical.html# \
+        SQL-SYNTAX-IDENTIFIERS
+
+    Example::
+
+        >>> t1 = sql.Identifier("foo")
+        >>> t2 = sql.Identifier("ba'r")
+        >>> t3 = sql.Identifier('ba"z')
+        >>> print(sql.SQL(', ').join([t1, t2, t3]).as_string(conn))
+        "foo", "ba'r", "ba""z"
+
+    Multiple strings can be passed to the object to represent a qualified name,
+    i.e. a dot-separated sequence of identifiers.
+
+    Example::
+
+        >>> query = sql.SQL("select {} from {}").format(
+        ...     sql.Identifier("table", "field"),
+        ...     sql.Identifier("schema", "table"))
+        >>> print(query.as_string(conn))
+        select "table"."field" from "schema"."table"
+
+    """
+    def __init__(self, *strings):
+        if not strings:
+            raise TypeError("Identifier cannot be empty")
+
+        for s in strings:
+            if not isinstance(s, str):
+                raise TypeError("SQL identifier parts must be strings")
+
+        super().__init__(strings)
+
+    @property
+    def strings(self):
+        """A tuple with the strings wrapped by the `Identifier`."""
+        return self._wrapped
+
+    @property
+    def string(self):
+        """The string wrapped by the `Identifier`.
+        """
+        if len(self._wrapped) == 1:
+            return self._wrapped[0]
+        else:
+            raise AttributeError(
+                "the Identifier wraps more than one than one string")
+
+    def __repr__(self):
+        return f"{self.__class__.__name__}({', '.join(map(repr, self._wrapped))})"
+
+    def as_string(self, context):
+        return '.'.join(ext.quote_ident(s, context) for s in self._wrapped)
+
+
+class Literal(Composable):
+    """
+    A `Composable` representing an SQL value to include in a query.
+
+    Usually you will want to include placeholders in the query and pass values
+    as `~cursor.execute()` arguments. If however you really really need to
+    include a literal value in the query you can use this object.
+
+    The string returned by `!as_string()` follows the normal :ref:`adaptation
+    rules <python-types-adaptation>` for Python objects.
+
+    Example::
+
+        >>> s1 = sql.Literal("foo")
+        >>> s2 = sql.Literal("ba'r")
+        >>> s3 = sql.Literal(42)
+        >>> print(sql.SQL(', ').join([s1, s2, s3]).as_string(conn))
+        'foo', 'ba''r', 42
+
+    """
+    @property
+    def wrapped(self):
+        """The object wrapped by the `!Literal`."""
+        return self._wrapped
+
+    def as_string(self, context):
+        # is it a connection or cursor?
+        if isinstance(context, ext.connection):
+            conn = context
+        elif isinstance(context, ext.cursor):
+            conn = context.connection
+        else:
+            raise TypeError("context must be a connection or a cursor")
+
+        a = ext.adapt(self._wrapped)
+        if hasattr(a, 'prepare'):
+            a.prepare(conn)
+
+        rv = a.getquoted()
+        if isinstance(rv, bytes):
+            rv = rv.decode(ext.encodings[conn.encoding])
+
+        return rv
+
+
+class Placeholder(Composable):
+    """A `Composable` representing a placeholder for query parameters.
+
+    If the name is specified, generate a named placeholder (e.g. ``%(name)s``),
+    otherwise generate a positional placeholder (e.g. ``%s``).
+
+    The object is useful to generate SQL queries with a variable number of
+    arguments.
+
+    Examples::
+
+        >>> names = ['foo', 'bar', 'baz']
+
+        >>> q1 = sql.SQL("insert into table ({}) values ({})").format(
+        ...     sql.SQL(', ').join(map(sql.Identifier, names)),
+        ...     sql.SQL(', ').join(sql.Placeholder() * len(names)))
+        >>> print(q1.as_string(conn))
+        insert into table ("foo", "bar", "baz") values (%s, %s, %s)
+
+        >>> q2 = sql.SQL("insert into table ({}) values ({})").format(
+        ...     sql.SQL(', ').join(map(sql.Identifier, names)),
+        ...     sql.SQL(', ').join(map(sql.Placeholder, names)))
+        >>> print(q2.as_string(conn))
+        insert into table ("foo", "bar", "baz") values (%(foo)s, %(bar)s, %(baz)s)
+
+    """
+
+    def __init__(self, name=None):
+        if isinstance(name, str):
+            if ')' in name:
+                raise ValueError(f"invalid name: {name!r}")
+
+        elif name is not None:
+            raise TypeError(f"expected string or None as name, got {name!r}")
+
+        super().__init__(name)
+
+    @property
+    def name(self):
+        """The name of the `!Placeholder`."""
+        return self._wrapped
+
+    def __repr__(self):
+        if self._wrapped is None:
+            return f"{self.__class__.__name__}()"
+        else:
+            return f"{self.__class__.__name__}({self._wrapped!r})"
+
+    def as_string(self, context):
+        if self._wrapped is not None:
+            return f"%({self._wrapped})s"
+        else:
+            return "%s"
+
+
+# Literals
+NULL = SQL("NULL")
+DEFAULT = SQL("DEFAULT")
Index: venv/Lib/site-packages/gunicorn/systemd.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/systemd.py b/venv/Lib/site-packages/gunicorn/systemd.py
new file mode 100644
--- /dev/null	(date 1630065626350)
+++ b/venv/Lib/site-packages/gunicorn/systemd.py	(date 1630065626350)
@@ -0,0 +1,77 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+import os
+import socket
+
+SD_LISTEN_FDS_START = 3
+
+
+def listen_fds(unset_environment=True):
+    """
+    Get the number of sockets inherited from systemd socket activation.
+
+    :param unset_environment: clear systemd environment variables unless False
+    :type unset_environment: bool
+    :return: the number of sockets to inherit from systemd socket activation
+    :rtype: int
+
+    Returns zero immediately if $LISTEN_PID is not set to the current pid.
+    Otherwise, returns the number of systemd activation sockets specified by
+    $LISTEN_FDS.
+
+    When $LISTEN_PID matches the current pid, unsets the environment variables
+    unless the ``unset_environment`` flag is ``False``.
+
+    .. note::
+        Unlike the sd_listen_fds C function, this implementation does not set
+        the FD_CLOEXEC flag because the gunicorn arbiter never needs to do this.
+
+    .. seealso::
+        `<https://www.freedesktop.org/software/systemd/man/sd_listen_fds.html>`_
+
+    """
+    fds = int(os.environ.get('LISTEN_FDS', 0))
+    listen_pid = int(os.environ.get('LISTEN_PID', 0))
+
+    if listen_pid != os.getpid():
+        return 0
+
+    if unset_environment:
+        os.environ.pop('LISTEN_PID', None)
+        os.environ.pop('LISTEN_FDS', None)
+
+    return fds
+
+
+def sd_notify(state, logger, unset_environment=False):
+    """Send a notification to systemd. state is a string; see
+    the man page of sd_notify (http://www.freedesktop.org/software/systemd/man/sd_notify.html)
+    for a description of the allowable values.
+
+    If the unset_environment parameter is True, sd_notify() will unset
+    the $NOTIFY_SOCKET environment variable before returning (regardless of
+    whether the function call itself succeeded or not). Further calls to
+    sd_notify() will then fail, but the variable is no longer inherited by
+    child processes.
+    """
+
+
+    addr = os.environ.get('NOTIFY_SOCKET')
+    if addr is None:
+        # not run in a service, just a noop
+        return
+    try:
+        sock = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM | socket.SOCK_CLOEXEC)
+        if addr[0] == '@':
+            addr = '\0' + addr[1:]
+        sock.connect(addr)
+        sock.sendall(state.encode('utf-8'))
+    except:
+        logger.debug("Exception while invoking sd_notify()", exc_info=True)
+    finally:
+        if unset_environment:
+            os.environ.pop('NOTIFY_SOCKET')
+        sock.close()
Index: venv/Lib/site-packages/gunicorn/pidfile.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/pidfile.py b/venv/Lib/site-packages/gunicorn/pidfile.py
new file mode 100644
--- /dev/null	(date 1630065626339)
+++ b/venv/Lib/site-packages/gunicorn/pidfile.py	(date 1630065626339)
@@ -0,0 +1,86 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+import errno
+import os
+import tempfile
+
+
+class Pidfile(object):
+    """\
+    Manage a PID file. If a specific name is provided
+    it and '"%s.oldpid" % name' will be used. Otherwise
+    we create a temp file using os.mkstemp.
+    """
+
+    def __init__(self, fname):
+        self.fname = fname
+        self.pid = None
+
+    def create(self, pid):
+        oldpid = self.validate()
+        if oldpid:
+            if oldpid == os.getpid():
+                return
+            msg = "Already running on PID %s (or pid file '%s' is stale)"
+            raise RuntimeError(msg % (oldpid, self.fname))
+
+        self.pid = pid
+
+        # Write pidfile
+        fdir = os.path.dirname(self.fname)
+        if fdir and not os.path.isdir(fdir):
+            raise RuntimeError("%s doesn't exist. Can't create pidfile." % fdir)
+        fd, fname = tempfile.mkstemp(dir=fdir)
+        os.write(fd, ("%s\n" % self.pid).encode('utf-8'))
+        if self.fname:
+            os.rename(fname, self.fname)
+        else:
+            self.fname = fname
+        os.close(fd)
+
+        # set permissions to -rw-r--r--
+        os.chmod(self.fname, 420)
+
+    def rename(self, path):
+        self.unlink()
+        self.fname = path
+        self.create(self.pid)
+
+    def unlink(self):
+        """ delete pidfile"""
+        try:
+            with open(self.fname, "r") as f:
+                pid1 = int(f.read() or 0)
+
+            if pid1 == self.pid:
+                os.unlink(self.fname)
+        except Exception:
+            pass
+
+    def validate(self):
+        """ Validate pidfile and make it stale if needed"""
+        if not self.fname:
+            return
+        try:
+            with open(self.fname, "r") as f:
+                try:
+                    wpid = int(f.read())
+                except ValueError:
+                    return
+
+                try:
+                    os.kill(wpid, 0)
+                    return wpid
+                except OSError as e:
+                    if e.args[0] == errno.EPERM:
+                        return wpid
+                    if e.args[0] == errno.ESRCH:
+                        return
+                    raise
+        except IOError as e:
+            if e.args[0] == errno.ENOENT:
+                return
+            raise
Index: venv/Lib/site-packages/psycopg2/extensions.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/psycopg2/extensions.py b/venv/Lib/site-packages/psycopg2/extensions.py
new file mode 100644
--- /dev/null	(date 1630065394292)
+++ b/venv/Lib/site-packages/psycopg2/extensions.py	(date 1630065394292)
@@ -0,0 +1,213 @@
+"""psycopg extensions to the DBAPI-2.0
+
+This module holds all the extensions to the DBAPI-2.0 provided by psycopg.
+
+- `connection` -- the new-type inheritable connection class
+- `cursor` -- the new-type inheritable cursor class
+- `lobject` -- the new-type inheritable large object class
+- `adapt()` -- exposes the PEP-246_ compatible adapting mechanism used
+  by psycopg to adapt Python types to PostgreSQL ones
+
+.. _PEP-246: https://www.python.org/dev/peps/pep-0246/
+"""
+# psycopg/extensions.py - DBAPI-2.0 extensions specific to psycopg
+#
+# Copyright (C) 2003-2019 Federico Di Gregorio  <fog@debian.org>
+# Copyright (C) 2020-2021 The Psycopg Team
+#
+# psycopg2 is free software: you can redistribute it and/or modify it
+# under the terms of the GNU Lesser General Public License as published
+# by the Free Software Foundation, either version 3 of the License, or
+# (at your option) any later version.
+#
+# In addition, as a special exception, the copyright holders give
+# permission to link this program with the OpenSSL library (or with
+# modified versions of OpenSSL that use the same license as OpenSSL),
+# and distribute linked combinations including the two.
+#
+# You must obey the GNU Lesser General Public License in all respects for
+# all of the code used other than OpenSSL.
+#
+# psycopg2 is distributed in the hope that it will be useful, but WITHOUT
+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public
+# License for more details.
+
+import re as _re
+
+from psycopg2._psycopg import (                             # noqa
+    BINARYARRAY, BOOLEAN, BOOLEANARRAY, BYTES, BYTESARRAY, DATE, DATEARRAY,
+    DATETIMEARRAY, DECIMAL, DECIMALARRAY, FLOAT, FLOATARRAY, INTEGER,
+    INTEGERARRAY, INTERVAL, INTERVALARRAY, LONGINTEGER, LONGINTEGERARRAY,
+    ROWIDARRAY, STRINGARRAY, TIME, TIMEARRAY, UNICODE, UNICODEARRAY,
+    AsIs, Binary, Boolean, Float, Int, QuotedString, )
+
+from psycopg2._psycopg import (                         # noqa
+    PYDATE, PYDATETIME, PYDATETIMETZ, PYINTERVAL, PYTIME, PYDATEARRAY,
+    PYDATETIMEARRAY, PYDATETIMETZARRAY, PYINTERVALARRAY, PYTIMEARRAY,
+    DateFromPy, TimeFromPy, TimestampFromPy, IntervalFromPy, )
+
+from psycopg2._psycopg import (                             # noqa
+    adapt, adapters, encodings, connection, cursor,
+    lobject, Xid, libpq_version, parse_dsn, quote_ident,
+    string_types, binary_types, new_type, new_array_type, register_type,
+    ISQLQuote, Notify, Diagnostics, Column, ConnectionInfo,
+    QueryCanceledError, TransactionRollbackError,
+    set_wait_callback, get_wait_callback, encrypt_password, )
+
+
+"""Isolation level values."""
+ISOLATION_LEVEL_AUTOCOMMIT = 0
+ISOLATION_LEVEL_READ_UNCOMMITTED = 4
+ISOLATION_LEVEL_READ_COMMITTED = 1
+ISOLATION_LEVEL_REPEATABLE_READ = 2
+ISOLATION_LEVEL_SERIALIZABLE = 3
+ISOLATION_LEVEL_DEFAULT = None
+
+
+"""psycopg connection status values."""
+STATUS_SETUP = 0
+STATUS_READY = 1
+STATUS_BEGIN = 2
+STATUS_SYNC = 3  # currently unused
+STATUS_ASYNC = 4  # currently unused
+STATUS_PREPARED = 5
+
+# This is a useful mnemonic to check if the connection is in a transaction
+STATUS_IN_TRANSACTION = STATUS_BEGIN
+
+
+"""psycopg asynchronous connection polling values"""
+POLL_OK = 0
+POLL_READ = 1
+POLL_WRITE = 2
+POLL_ERROR = 3
+
+
+"""Backend transaction status values."""
+TRANSACTION_STATUS_IDLE = 0
+TRANSACTION_STATUS_ACTIVE = 1
+TRANSACTION_STATUS_INTRANS = 2
+TRANSACTION_STATUS_INERROR = 3
+TRANSACTION_STATUS_UNKNOWN = 4
+
+
+def register_adapter(typ, callable):
+    """Register 'callable' as an ISQLQuote adapter for type 'typ'."""
+    adapters[(typ, ISQLQuote)] = callable
+
+
+# The SQL_IN class is the official adapter for tuples starting from 2.0.6.
+class SQL_IN:
+    """Adapt any iterable to an SQL quotable object."""
+    def __init__(self, seq):
+        self._seq = seq
+        self._conn = None
+
+    def prepare(self, conn):
+        self._conn = conn
+
+    def getquoted(self):
+        # this is the important line: note how every object in the
+        # list is adapted and then how getquoted() is called on it
+        pobjs = [adapt(o) for o in self._seq]
+        if self._conn is not None:
+            for obj in pobjs:
+                if hasattr(obj, 'prepare'):
+                    obj.prepare(self._conn)
+        qobjs = [o.getquoted() for o in pobjs]
+        return b'(' + b', '.join(qobjs) + b')'
+
+    def __str__(self):
+        return str(self.getquoted())
+
+
+class NoneAdapter:
+    """Adapt None to NULL.
+
+    This adapter is not used normally as a fast path in mogrify uses NULL,
+    but it makes easier to adapt composite types.
+    """
+    def __init__(self, obj):
+        pass
+
+    def getquoted(self, _null=b"NULL"):
+        return _null
+
+
+def make_dsn(dsn=None, **kwargs):
+    """Convert a set of keywords into a connection strings."""
+    if dsn is None and not kwargs:
+        return ''
+
+    # If no kwarg is specified don't mung the dsn, but verify it
+    if not kwargs:
+        parse_dsn(dsn)
+        return dsn
+
+    # Override the dsn with the parameters
+    if 'database' in kwargs:
+        if 'dbname' in kwargs:
+            raise TypeError(
+                "you can't specify both 'database' and 'dbname' arguments")
+        kwargs['dbname'] = kwargs.pop('database')
+
+    # Drop the None arguments
+    kwargs = {k: v for (k, v) in kwargs.items() if v is not None}
+
+    if dsn is not None:
+        tmp = parse_dsn(dsn)
+        tmp.update(kwargs)
+        kwargs = tmp
+
+    dsn = " ".join(["{}={}".format(k, _param_escape(str(v)))
+        for (k, v) in kwargs.items()])
+
+    # verify that the returned dsn is valid
+    parse_dsn(dsn)
+
+    return dsn
+
+
+def _param_escape(s,
+        re_escape=_re.compile(r"([\\'])"),
+        re_space=_re.compile(r'\s')):
+    """
+    Apply the escaping rule required by PQconnectdb
+    """
+    if not s:
+        return "''"
+
+    s = re_escape.sub(r'\\\1', s)
+    if re_space.search(s):
+        s = "'" + s + "'"
+
+    return s
+
+
+# Create default json typecasters for PostgreSQL 9.2 oids
+from psycopg2._json import register_default_json, register_default_jsonb    # noqa
+
+try:
+    JSON, JSONARRAY = register_default_json()
+    JSONB, JSONBARRAY = register_default_jsonb()
+except ImportError:
+    pass
+
+del register_default_json, register_default_jsonb
+
+
+# Create default Range typecasters
+from psycopg2. _range import Range                              # noqa
+del Range
+
+
+# Add the "cleaned" version of the encodings to the key.
+# When the encoding is set its name is cleaned up from - and _ and turned
+# uppercase, so an encoding not respecting these rules wouldn't be found in the
+# encodings keys and would raise an exception with the unicode typecaster
+for k, v in list(encodings.items()):
+    k = k.replace('_', '').replace('-', '').upper()
+    encodings[k] = v
+
+del k, v
Index: venv/Lib/site-packages/gunicorn/reloader.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/reloader.py b/venv/Lib/site-packages/gunicorn/reloader.py
new file mode 100644
--- /dev/null	(date 1630065626342)
+++ b/venv/Lib/site-packages/gunicorn/reloader.py	(date 1630065626342)
@@ -0,0 +1,132 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+# pylint: disable=no-else-continue
+
+import os
+import os.path
+import re
+import sys
+import time
+import threading
+
+COMPILED_EXT_RE = re.compile(r'py[co]$')
+
+
+class Reloader(threading.Thread):
+    def __init__(self, extra_files=None, interval=1, callback=None):
+        super().__init__()
+        self.setDaemon(True)
+        self._extra_files = set(extra_files or ())
+        self._interval = interval
+        self._callback = callback
+
+    def add_extra_file(self, filename):
+        self._extra_files.add(filename)
+
+    def get_files(self):
+        fnames = [
+            COMPILED_EXT_RE.sub('py', module.__file__)
+            for module in tuple(sys.modules.values())
+            if getattr(module, '__file__', None)
+        ]
+
+        fnames.extend(self._extra_files)
+
+        return fnames
+
+    def run(self):
+        mtimes = {}
+        while True:
+            for filename in self.get_files():
+                try:
+                    mtime = os.stat(filename).st_mtime
+                except OSError:
+                    continue
+                old_time = mtimes.get(filename)
+                if old_time is None:
+                    mtimes[filename] = mtime
+                    continue
+                elif mtime > old_time:
+                    if self._callback:
+                        self._callback(filename)
+            time.sleep(self._interval)
+
+
+has_inotify = False
+if sys.platform.startswith('linux'):
+    try:
+        from inotify.adapters import Inotify
+        import inotify.constants
+        has_inotify = True
+    except ImportError:
+        pass
+
+
+if has_inotify:
+
+    class InotifyReloader(threading.Thread):
+        event_mask = (inotify.constants.IN_CREATE | inotify.constants.IN_DELETE
+                      | inotify.constants.IN_DELETE_SELF | inotify.constants.IN_MODIFY
+                      | inotify.constants.IN_MOVE_SELF | inotify.constants.IN_MOVED_FROM
+                      | inotify.constants.IN_MOVED_TO)
+
+        def __init__(self, extra_files=None, callback=None):
+            super().__init__()
+            self.setDaemon(True)
+            self._callback = callback
+            self._dirs = set()
+            self._watcher = Inotify()
+
+            for extra_file in extra_files:
+                self.add_extra_file(extra_file)
+
+        def add_extra_file(self, filename):
+            dirname = os.path.dirname(filename)
+
+            if dirname in self._dirs:
+                return
+
+            self._watcher.add_watch(dirname, mask=self.event_mask)
+            self._dirs.add(dirname)
+
+        def get_dirs(self):
+            fnames = [
+                os.path.dirname(os.path.abspath(COMPILED_EXT_RE.sub('py', module.__file__)))
+                for module in tuple(sys.modules.values())
+                if getattr(module, '__file__', None)
+            ]
+
+            return set(fnames)
+
+        def run(self):
+            self._dirs = self.get_dirs()
+
+            for dirname in self._dirs:
+                if os.path.isdir(dirname):
+                    self._watcher.add_watch(dirname, mask=self.event_mask)
+
+            for event in self._watcher.event_gen():
+                if event is None:
+                    continue
+
+                filename = event[3]
+
+                self._callback(filename)
+
+else:
+
+    class InotifyReloader(object):
+        def __init__(self, callback=None):
+            raise ImportError('You must have the inotify module installed to '
+                              'use the inotify reloader')
+
+
+preferred_reloader = InotifyReloader if has_inotify else Reloader
+
+reloader_engines = {
+    'auto': preferred_reloader,
+    'poll': Reloader,
+    'inotify': InotifyReloader,
+}
Index: venv/Lib/site-packages/psycopg2/extras.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/psycopg2/extras.py b/venv/Lib/site-packages/psycopg2/extras.py
new file mode 100644
--- /dev/null	(date 1630065394296)
+++ b/venv/Lib/site-packages/psycopg2/extras.py	(date 1630065394296)
@@ -0,0 +1,1306 @@
+"""Miscellaneous goodies for psycopg2
+
+This module is a generic place used to hold little helper functions
+and classes until a better place in the distribution is found.
+"""
+# psycopg/extras.py - miscellaneous extra goodies for psycopg
+#
+# Copyright (C) 2003-2019 Federico Di Gregorio  <fog@debian.org>
+# Copyright (C) 2020-2021 The Psycopg Team
+#
+# psycopg2 is free software: you can redistribute it and/or modify it
+# under the terms of the GNU Lesser General Public License as published
+# by the Free Software Foundation, either version 3 of the License, or
+# (at your option) any later version.
+#
+# In addition, as a special exception, the copyright holders give
+# permission to link this program with the OpenSSL library (or with
+# modified versions of OpenSSL that use the same license as OpenSSL),
+# and distribute linked combinations including the two.
+#
+# You must obey the GNU Lesser General Public License in all respects for
+# all of the code used other than OpenSSL.
+#
+# psycopg2 is distributed in the hope that it will be useful, but WITHOUT
+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public
+# License for more details.
+
+import os as _os
+import time as _time
+import re as _re
+from collections import namedtuple, OrderedDict
+
+import logging as _logging
+
+import psycopg2
+from psycopg2 import extensions as _ext
+from .extensions import cursor as _cursor
+from .extensions import connection as _connection
+from .extensions import adapt as _A, quote_ident
+from functools import lru_cache
+
+from psycopg2._psycopg import (                             # noqa
+    REPLICATION_PHYSICAL, REPLICATION_LOGICAL,
+    ReplicationConnection as _replicationConnection,
+    ReplicationCursor as _replicationCursor,
+    ReplicationMessage)
+
+
+# expose the json adaptation stuff into the module
+from psycopg2._json import (                                # noqa
+    json, Json, register_json, register_default_json, register_default_jsonb)
+
+
+# Expose range-related objects
+from psycopg2._range import (                               # noqa
+    Range, NumericRange, DateRange, DateTimeRange, DateTimeTZRange,
+    register_range, RangeAdapter, RangeCaster)
+
+
+# Expose ipaddress-related objects
+from psycopg2._ipaddress import register_ipaddress          # noqa
+
+
+class DictCursorBase(_cursor):
+    """Base class for all dict-like cursors."""
+
+    def __init__(self, *args, **kwargs):
+        if 'row_factory' in kwargs:
+            row_factory = kwargs['row_factory']
+            del kwargs['row_factory']
+        else:
+            raise NotImplementedError(
+                "DictCursorBase can't be instantiated without a row factory.")
+        super().__init__(*args, **kwargs)
+        self._query_executed = False
+        self._prefetch = False
+        self.row_factory = row_factory
+
+    def fetchone(self):
+        if self._prefetch:
+            res = super().fetchone()
+        if self._query_executed:
+            self._build_index()
+        if not self._prefetch:
+            res = super().fetchone()
+        return res
+
+    def fetchmany(self, size=None):
+        if self._prefetch:
+            res = super().fetchmany(size)
+        if self._query_executed:
+            self._build_index()
+        if not self._prefetch:
+            res = super().fetchmany(size)
+        return res
+
+    def fetchall(self):
+        if self._prefetch:
+            res = super().fetchall()
+        if self._query_executed:
+            self._build_index()
+        if not self._prefetch:
+            res = super().fetchall()
+        return res
+
+    def __iter__(self):
+        try:
+            if self._prefetch:
+                res = super().__iter__()
+                first = next(res)
+            if self._query_executed:
+                self._build_index()
+            if not self._prefetch:
+                res = super().__iter__()
+                first = next(res)
+
+            yield first
+            while True:
+                yield next(res)
+        except StopIteration:
+            return
+
+
+class DictConnection(_connection):
+    """A connection that uses `DictCursor` automatically."""
+    def cursor(self, *args, **kwargs):
+        kwargs.setdefault('cursor_factory', self.cursor_factory or DictCursor)
+        return super().cursor(*args, **kwargs)
+
+
+class DictCursor(DictCursorBase):
+    """A cursor that keeps a list of column name -> index mappings__.
+
+    .. __: https://docs.python.org/glossary.html#term-mapping
+    """
+
+    def __init__(self, *args, **kwargs):
+        kwargs['row_factory'] = DictRow
+        super().__init__(*args, **kwargs)
+        self._prefetch = True
+
+    def execute(self, query, vars=None):
+        self.index = OrderedDict()
+        self._query_executed = True
+        return super().execute(query, vars)
+
+    def callproc(self, procname, vars=None):
+        self.index = OrderedDict()
+        self._query_executed = True
+        return super().callproc(procname, vars)
+
+    def _build_index(self):
+        if self._query_executed and self.description:
+            for i in range(len(self.description)):
+                self.index[self.description[i][0]] = i
+            self._query_executed = False
+
+
+class DictRow(list):
+    """A row object that allow by-column-name access to data."""
+
+    __slots__ = ('_index',)
+
+    def __init__(self, cursor):
+        self._index = cursor.index
+        self[:] = [None] * len(cursor.description)
+
+    def __getitem__(self, x):
+        if not isinstance(x, (int, slice)):
+            x = self._index[x]
+        return super().__getitem__(x)
+
+    def __setitem__(self, x, v):
+        if not isinstance(x, (int, slice)):
+            x = self._index[x]
+        super().__setitem__(x, v)
+
+    def items(self):
+        g = super().__getitem__
+        return ((n, g(self._index[n])) for n in self._index)
+
+    def keys(self):
+        return iter(self._index)
+
+    def values(self):
+        g = super().__getitem__
+        return (g(self._index[n]) for n in self._index)
+
+    def get(self, x, default=None):
+        try:
+            return self[x]
+        except Exception:
+            return default
+
+    def copy(self):
+        return OrderedDict(self.items())
+
+    def __contains__(self, x):
+        return x in self._index
+
+    def __reduce__(self):
+        # this is apparently useless, but it fixes #1073
+        return super().__reduce__()
+
+    def __getstate__(self):
+        return self[:], self._index.copy()
+
+    def __setstate__(self, data):
+        self[:] = data[0]
+        self._index = data[1]
+
+
+class RealDictConnection(_connection):
+    """A connection that uses `RealDictCursor` automatically."""
+    def cursor(self, *args, **kwargs):
+        kwargs.setdefault('cursor_factory', self.cursor_factory or RealDictCursor)
+        return super().cursor(*args, **kwargs)
+
+
+class RealDictCursor(DictCursorBase):
+    """A cursor that uses a real dict as the base type for rows.
+
+    Note that this cursor is extremely specialized and does not allow
+    the normal access (using integer indices) to fetched data. If you need
+    to access database rows both as a dictionary and a list, then use
+    the generic `DictCursor` instead of `!RealDictCursor`.
+    """
+    def __init__(self, *args, **kwargs):
+        kwargs['row_factory'] = RealDictRow
+        super().__init__(*args, **kwargs)
+
+    def execute(self, query, vars=None):
+        self.column_mapping = []
+        self._query_executed = True
+        return super().execute(query, vars)
+
+    def callproc(self, procname, vars=None):
+        self.column_mapping = []
+        self._query_executed = True
+        return super().callproc(procname, vars)
+
+    def _build_index(self):
+        if self._query_executed and self.description:
+            self.column_mapping = [d[0] for d in self.description]
+            self._query_executed = False
+
+
+class RealDictRow(OrderedDict):
+    """A `!dict` subclass representing a data record."""
+
+    def __init__(self, *args, **kwargs):
+        if args and isinstance(args[0], _cursor):
+            cursor = args[0]
+            args = args[1:]
+        else:
+            cursor = None
+
+        super().__init__(*args, **kwargs)
+
+        if cursor is not None:
+            # Required for named cursors
+            if cursor.description and not cursor.column_mapping:
+                cursor._build_index()
+
+            # Store the cols mapping in the dict itself until the row is fully
+            # populated, so we don't need to add attributes to the class
+            # (hence keeping its maintenance, special pickle support, etc.)
+            self[RealDictRow] = cursor.column_mapping
+
+    def __setitem__(self, key, value):
+        if RealDictRow in self:
+            # We are in the row building phase
+            mapping = self[RealDictRow]
+            super().__setitem__(mapping[key], value)
+            if key == len(mapping) - 1:
+                # Row building finished
+                del self[RealDictRow]
+            return
+
+        super().__setitem__(key, value)
+
+
+class NamedTupleConnection(_connection):
+    """A connection that uses `NamedTupleCursor` automatically."""
+    def cursor(self, *args, **kwargs):
+        kwargs.setdefault('cursor_factory', self.cursor_factory or NamedTupleCursor)
+        return super().cursor(*args, **kwargs)
+
+
+class NamedTupleCursor(_cursor):
+    """A cursor that generates results as `~collections.namedtuple`.
+
+    `!fetch*()` methods will return named tuples instead of regular tuples, so
+    their elements can be accessed both as regular numeric items as well as
+    attributes.
+
+        >>> nt_cur = conn.cursor(cursor_factory=psycopg2.extras.NamedTupleCursor)
+        >>> rec = nt_cur.fetchone()
+        >>> rec
+        Record(id=1, num=100, data="abc'def")
+        >>> rec[1]
+        100
+        >>> rec.data
+        "abc'def"
+    """
+    Record = None
+    MAX_CACHE = 1024
+
+    def execute(self, query, vars=None):
+        self.Record = None
+        return super().execute(query, vars)
+
+    def executemany(self, query, vars):
+        self.Record = None
+        return super().executemany(query, vars)
+
+    def callproc(self, procname, vars=None):
+        self.Record = None
+        return super().callproc(procname, vars)
+
+    def fetchone(self):
+        t = super().fetchone()
+        if t is not None:
+            nt = self.Record
+            if nt is None:
+                nt = self.Record = self._make_nt()
+            return nt._make(t)
+
+    def fetchmany(self, size=None):
+        ts = super().fetchmany(size)
+        nt = self.Record
+        if nt is None:
+            nt = self.Record = self._make_nt()
+        return list(map(nt._make, ts))
+
+    def fetchall(self):
+        ts = super().fetchall()
+        nt = self.Record
+        if nt is None:
+            nt = self.Record = self._make_nt()
+        return list(map(nt._make, ts))
+
+    def __iter__(self):
+        try:
+            it = super().__iter__()
+            t = next(it)
+
+            nt = self.Record
+            if nt is None:
+                nt = self.Record = self._make_nt()
+
+            yield nt._make(t)
+
+            while True:
+                yield nt._make(next(it))
+        except StopIteration:
+            return
+
+    # ascii except alnum and underscore
+    _re_clean = _re.compile(
+        '[' + _re.escape(' !"#$%&\'()*+,-./:;<=>?@[\\]^`{|}~') + ']')
+
+    def _make_nt(self):
+        key = tuple(d[0] for d in self.description) if self.description else ()
+        return self._cached_make_nt(key)
+
+    @classmethod
+    def _do_make_nt(cls, key):
+        fields = []
+        for s in key:
+            s = cls._re_clean.sub('_', s)
+            # Python identifier cannot start with numbers, namedtuple fields
+            # cannot start with underscore. So...
+            if s[0] == '_' or '0' <= s[0] <= '9':
+                s = 'f' + s
+            fields.append(s)
+
+        nt = namedtuple("Record", fields)
+        return nt
+
+
+@lru_cache(512)
+def _cached_make_nt(cls, key):
+    return cls._do_make_nt(key)
+
+
+# Exposed for testability, and if someone wants to monkeypatch to tweak
+# the cache size.
+NamedTupleCursor._cached_make_nt = classmethod(_cached_make_nt)
+
+
+class LoggingConnection(_connection):
+    """A connection that logs all queries to a file or logger__ object.
+
+    .. __: https://docs.python.org/library/logging.html
+    """
+
+    def initialize(self, logobj):
+        """Initialize the connection to log to `!logobj`.
+
+        The `!logobj` parameter can be an open file object or a Logger/LoggerAdapter
+        instance from the standard logging module.
+        """
+        self._logobj = logobj
+        if _logging and isinstance(
+                logobj, (_logging.Logger, _logging.LoggerAdapter)):
+            self.log = self._logtologger
+        else:
+            self.log = self._logtofile
+
+    def filter(self, msg, curs):
+        """Filter the query before logging it.
+
+        This is the method to overwrite to filter unwanted queries out of the
+        log or to add some extra data to the output. The default implementation
+        just does nothing.
+        """
+        return msg
+
+    def _logtofile(self, msg, curs):
+        msg = self.filter(msg, curs)
+        if msg:
+            if isinstance(msg, bytes):
+                msg = msg.decode(_ext.encodings[self.encoding], 'replace')
+            self._logobj.write(msg + _os.linesep)
+
+    def _logtologger(self, msg, curs):
+        msg = self.filter(msg, curs)
+        if msg:
+            self._logobj.debug(msg)
+
+    def _check(self):
+        if not hasattr(self, '_logobj'):
+            raise self.ProgrammingError(
+                "LoggingConnection object has not been initialize()d")
+
+    def cursor(self, *args, **kwargs):
+        self._check()
+        kwargs.setdefault('cursor_factory', self.cursor_factory or LoggingCursor)
+        return super().cursor(*args, **kwargs)
+
+
+class LoggingCursor(_cursor):
+    """A cursor that logs queries using its connection logging facilities."""
+
+    def execute(self, query, vars=None):
+        try:
+            return super().execute(query, vars)
+        finally:
+            self.connection.log(self.query, self)
+
+    def callproc(self, procname, vars=None):
+        try:
+            return super().callproc(procname, vars)
+        finally:
+            self.connection.log(self.query, self)
+
+
+class MinTimeLoggingConnection(LoggingConnection):
+    """A connection that logs queries based on execution time.
+
+    This is just an example of how to sub-class `LoggingConnection` to
+    provide some extra filtering for the logged queries. Both the
+    `initialize()` and `filter()` methods are overwritten to make sure
+    that only queries executing for more than ``mintime`` ms are logged.
+
+    Note that this connection uses the specialized cursor
+    `MinTimeLoggingCursor`.
+    """
+    def initialize(self, logobj, mintime=0):
+        LoggingConnection.initialize(self, logobj)
+        self._mintime = mintime
+
+    def filter(self, msg, curs):
+        t = (_time.time() - curs.timestamp) * 1000
+        if t > self._mintime:
+            if isinstance(msg, bytes):
+                msg = msg.decode(_ext.encodings[self.encoding], 'replace')
+            return f"{msg}{_os.linesep}  (execution time: {t} ms)"
+
+    def cursor(self, *args, **kwargs):
+        kwargs.setdefault('cursor_factory',
+            self.cursor_factory or MinTimeLoggingCursor)
+        return LoggingConnection.cursor(self, *args, **kwargs)
+
+
+class MinTimeLoggingCursor(LoggingCursor):
+    """The cursor sub-class companion to `MinTimeLoggingConnection`."""
+
+    def execute(self, query, vars=None):
+        self.timestamp = _time.time()
+        return LoggingCursor.execute(self, query, vars)
+
+    def callproc(self, procname, vars=None):
+        self.timestamp = _time.time()
+        return LoggingCursor.callproc(self, procname, vars)
+
+
+class LogicalReplicationConnection(_replicationConnection):
+
+    def __init__(self, *args, **kwargs):
+        kwargs['replication_type'] = REPLICATION_LOGICAL
+        super().__init__(*args, **kwargs)
+
+
+class PhysicalReplicationConnection(_replicationConnection):
+
+    def __init__(self, *args, **kwargs):
+        kwargs['replication_type'] = REPLICATION_PHYSICAL
+        super().__init__(*args, **kwargs)
+
+
+class StopReplication(Exception):
+    """
+    Exception used to break out of the endless loop in
+    `~ReplicationCursor.consume_stream()`.
+
+    Subclass of `~exceptions.Exception`.  Intentionally *not* inherited from
+    `~psycopg2.Error` as occurrence of this exception does not indicate an
+    error.
+    """
+    pass
+
+
+class ReplicationCursor(_replicationCursor):
+    """A cursor used for communication on replication connections."""
+
+    def create_replication_slot(self, slot_name, slot_type=None, output_plugin=None):
+        """Create streaming replication slot."""
+
+        command = f"CREATE_REPLICATION_SLOT {quote_ident(slot_name, self)} "
+
+        if slot_type is None:
+            slot_type = self.connection.replication_type
+
+        if slot_type == REPLICATION_LOGICAL:
+            if output_plugin is None:
+                raise psycopg2.ProgrammingError(
+                    "output plugin name is required to create "
+                    "logical replication slot")
+
+            command += f"LOGICAL {quote_ident(output_plugin, self)}"
+
+        elif slot_type == REPLICATION_PHYSICAL:
+            if output_plugin is not None:
+                raise psycopg2.ProgrammingError(
+                    "cannot specify output plugin name when creating "
+                    "physical replication slot")
+
+            command += "PHYSICAL"
+
+        else:
+            raise psycopg2.ProgrammingError(
+                f"unrecognized replication type: {repr(slot_type)}")
+
+        self.execute(command)
+
+    def drop_replication_slot(self, slot_name):
+        """Drop streaming replication slot."""
+
+        command = f"DROP_REPLICATION_SLOT {quote_ident(slot_name, self)}"
+        self.execute(command)
+
+    def start_replication(
+            self, slot_name=None, slot_type=None, start_lsn=0,
+            timeline=0, options=None, decode=False, status_interval=10):
+        """Start replication stream."""
+
+        command = "START_REPLICATION "
+
+        if slot_type is None:
+            slot_type = self.connection.replication_type
+
+        if slot_type == REPLICATION_LOGICAL:
+            if slot_name:
+                command += f"SLOT {quote_ident(slot_name, self)} "
+            else:
+                raise psycopg2.ProgrammingError(
+                    "slot name is required for logical replication")
+
+            command += "LOGICAL "
+
+        elif slot_type == REPLICATION_PHYSICAL:
+            if slot_name:
+                command += f"SLOT {quote_ident(slot_name, self)} "
+            # don't add "PHYSICAL", before 9.4 it was just START_REPLICATION XXX/XXX
+
+        else:
+            raise psycopg2.ProgrammingError(
+                f"unrecognized replication type: {repr(slot_type)}")
+
+        if type(start_lsn) is str:
+            lsn = start_lsn.split('/')
+            lsn = f"{int(lsn[0], 16):X}/{int(lsn[1], 16):08X}"
+        else:
+            lsn = f"{start_lsn >> 32 & 4294967295:X}/{start_lsn & 4294967295:08X}"
+
+        command += lsn
+
+        if timeline != 0:
+            if slot_type == REPLICATION_LOGICAL:
+                raise psycopg2.ProgrammingError(
+                    "cannot specify timeline for logical replication")
+
+            command += f" TIMELINE {timeline}"
+
+        if options:
+            if slot_type == REPLICATION_PHYSICAL:
+                raise psycopg2.ProgrammingError(
+                    "cannot specify output plugin options for physical replication")
+
+            command += " ("
+            for k, v in options.items():
+                if not command.endswith('('):
+                    command += ", "
+                command += f"{quote_ident(k, self)} {_A(str(v))}"
+            command += ")"
+
+        self.start_replication_expert(
+            command, decode=decode, status_interval=status_interval)
+
+    # allows replication cursors to be used in select.select() directly
+    def fileno(self):
+        return self.connection.fileno()
+
+
+# a dbtype and adapter for Python UUID type
+
+class UUID_adapter:
+    """Adapt Python's uuid.UUID__ type to PostgreSQL's uuid__.
+
+    .. __: https://docs.python.org/library/uuid.html
+    .. __: https://www.postgresql.org/docs/current/static/datatype-uuid.html
+    """
+
+    def __init__(self, uuid):
+        self._uuid = uuid
+
+    def __conform__(self, proto):
+        if proto is _ext.ISQLQuote:
+            return self
+
+    def getquoted(self):
+        return (f"'{self._uuid}'::uuid").encode('utf8')
+
+    def __str__(self):
+        return f"'{self._uuid}'::uuid"
+
+
+def register_uuid(oids=None, conn_or_curs=None):
+    """Create the UUID type and an uuid.UUID adapter.
+
+    :param oids: oid for the PostgreSQL :sql:`uuid` type, or 2-items sequence
+        with oids of the type and the array. If not specified, use PostgreSQL
+        standard oids.
+    :param conn_or_curs: where to register the typecaster. If not specified,
+        register it globally.
+    """
+
+    import uuid
+
+    if not oids:
+        oid1 = 2950
+        oid2 = 2951
+    elif isinstance(oids, (list, tuple)):
+        oid1, oid2 = oids
+    else:
+        oid1 = oids
+        oid2 = 2951
+
+    _ext.UUID = _ext.new_type((oid1, ), "UUID",
+            lambda data, cursor: data and uuid.UUID(data) or None)
+    _ext.UUIDARRAY = _ext.new_array_type((oid2,), "UUID[]", _ext.UUID)
+
+    _ext.register_type(_ext.UUID, conn_or_curs)
+    _ext.register_type(_ext.UUIDARRAY, conn_or_curs)
+    _ext.register_adapter(uuid.UUID, UUID_adapter)
+
+    return _ext.UUID
+
+
+# a type, dbtype and adapter for PostgreSQL inet type
+
+class Inet:
+    """Wrap a string to allow for correct SQL-quoting of inet values.
+
+    Note that this adapter does NOT check the passed value to make
+    sure it really is an inet-compatible address but DOES call adapt()
+    on it to make sure it is impossible to execute an SQL-injection
+    by passing an evil value to the initializer.
+    """
+    def __init__(self, addr):
+        self.addr = addr
+
+    def __repr__(self):
+        return f"{self.__class__.__name__}({self.addr!r})"
+
+    def prepare(self, conn):
+        self._conn = conn
+
+    def getquoted(self):
+        obj = _A(self.addr)
+        if hasattr(obj, 'prepare'):
+            obj.prepare(self._conn)
+        return obj.getquoted() + b"::inet"
+
+    def __conform__(self, proto):
+        if proto is _ext.ISQLQuote:
+            return self
+
+    def __str__(self):
+        return str(self.addr)
+
+
+def register_inet(oid=None, conn_or_curs=None):
+    """Create the INET type and an Inet adapter.
+
+    :param oid: oid for the PostgreSQL :sql:`inet` type, or 2-items sequence
+        with oids of the type and the array. If not specified, use PostgreSQL
+        standard oids.
+    :param conn_or_curs: where to register the typecaster. If not specified,
+        register it globally.
+    """
+    import warnings
+    warnings.warn(
+        "the inet adapter is deprecated, it's not very useful",
+        DeprecationWarning)
+
+    if not oid:
+        oid1 = 869
+        oid2 = 1041
+    elif isinstance(oid, (list, tuple)):
+        oid1, oid2 = oid
+    else:
+        oid1 = oid
+        oid2 = 1041
+
+    _ext.INET = _ext.new_type((oid1, ), "INET",
+            lambda data, cursor: data and Inet(data) or None)
+    _ext.INETARRAY = _ext.new_array_type((oid2, ), "INETARRAY", _ext.INET)
+
+    _ext.register_type(_ext.INET, conn_or_curs)
+    _ext.register_type(_ext.INETARRAY, conn_or_curs)
+
+    return _ext.INET
+
+
+def wait_select(conn):
+    """Wait until a connection or cursor has data available.
+
+    The function is an example of a wait callback to be registered with
+    `~psycopg2.extensions.set_wait_callback()`. This function uses
+    :py:func:`~select.select()` to wait for data to become available, and
+    therefore is able to handle/receive SIGINT/KeyboardInterrupt.
+    """
+    import select
+    from psycopg2.extensions import POLL_OK, POLL_READ, POLL_WRITE
+
+    while True:
+        try:
+            state = conn.poll()
+            if state == POLL_OK:
+                break
+            elif state == POLL_READ:
+                select.select([conn.fileno()], [], [])
+            elif state == POLL_WRITE:
+                select.select([], [conn.fileno()], [])
+            else:
+                raise conn.OperationalError(f"bad state from poll: {state}")
+        except KeyboardInterrupt:
+            conn.cancel()
+            # the loop will be broken by a server error
+            continue
+
+
+def _solve_conn_curs(conn_or_curs):
+    """Return the connection and a DBAPI cursor from a connection or cursor."""
+    if conn_or_curs is None:
+        raise psycopg2.ProgrammingError("no connection or cursor provided")
+
+    if hasattr(conn_or_curs, 'execute'):
+        conn = conn_or_curs.connection
+        curs = conn.cursor(cursor_factory=_cursor)
+    else:
+        conn = conn_or_curs
+        curs = conn.cursor(cursor_factory=_cursor)
+
+    return conn, curs
+
+
+class HstoreAdapter:
+    """Adapt a Python dict to the hstore syntax."""
+    def __init__(self, wrapped):
+        self.wrapped = wrapped
+
+    def prepare(self, conn):
+        self.conn = conn
+
+        # use an old-style getquoted implementation if required
+        if conn.info.server_version < 90000:
+            self.getquoted = self._getquoted_8
+
+    def _getquoted_8(self):
+        """Use the operators available in PG pre-9.0."""
+        if not self.wrapped:
+            return b"''::hstore"
+
+        adapt = _ext.adapt
+        rv = []
+        for k, v in self.wrapped.items():
+            k = adapt(k)
+            k.prepare(self.conn)
+            k = k.getquoted()
+
+            if v is not None:
+                v = adapt(v)
+                v.prepare(self.conn)
+                v = v.getquoted()
+            else:
+                v = b'NULL'
+
+            # XXX this b'ing is painfully inefficient!
+            rv.append(b"(" + k + b" => " + v + b")")
+
+        return b"(" + b'||'.join(rv) + b")"
+
+    def _getquoted_9(self):
+        """Use the hstore(text[], text[]) function."""
+        if not self.wrapped:
+            return b"''::hstore"
+
+        k = _ext.adapt(list(self.wrapped.keys()))
+        k.prepare(self.conn)
+        v = _ext.adapt(list(self.wrapped.values()))
+        v.prepare(self.conn)
+        return b"hstore(" + k.getquoted() + b", " + v.getquoted() + b")"
+
+    getquoted = _getquoted_9
+
+    _re_hstore = _re.compile(r"""
+        # hstore key:
+        # a string of normal or escaped chars
+        "((?: [^"\\] | \\. )*)"
+        \s*=>\s* # hstore value
+        (?:
+            NULL # the value can be null - not catched
+            # or a quoted string like the key
+            | "((?: [^"\\] | \\. )*)"
+        )
+        (?:\s*,\s*|$) # pairs separated by comma or end of string.
+    """, _re.VERBOSE)
+
+    @classmethod
+    def parse(self, s, cur, _bsdec=_re.compile(r"\\(.)")):
+        """Parse an hstore representation in a Python string.
+
+        The hstore is represented as something like::
+
+            "a"=>"1", "b"=>"2"
+
+        with backslash-escaped strings.
+        """
+        if s is None:
+            return None
+
+        rv = {}
+        start = 0
+        for m in self._re_hstore.finditer(s):
+            if m is None or m.start() != start:
+                raise psycopg2.InterfaceError(
+                    f"error parsing hstore pair at char {start}")
+            k = _bsdec.sub(r'\1', m.group(1))
+            v = m.group(2)
+            if v is not None:
+                v = _bsdec.sub(r'\1', v)
+
+            rv[k] = v
+            start = m.end()
+
+        if start < len(s):
+            raise psycopg2.InterfaceError(
+                f"error parsing hstore: unparsed data after char {start}")
+
+        return rv
+
+    @classmethod
+    def parse_unicode(self, s, cur):
+        """Parse an hstore returning unicode keys and values."""
+        if s is None:
+            return None
+
+        s = s.decode(_ext.encodings[cur.connection.encoding])
+        return self.parse(s, cur)
+
+    @classmethod
+    def get_oids(self, conn_or_curs):
+        """Return the lists of OID of the hstore and hstore[] types.
+        """
+        conn, curs = _solve_conn_curs(conn_or_curs)
+
+        # Store the transaction status of the connection to revert it after use
+        conn_status = conn.status
+
+        # column typarray not available before PG 8.3
+        typarray = conn.info.server_version >= 80300 and "typarray" or "NULL"
+
+        rv0, rv1 = [], []
+
+        # get the oid for the hstore
+        curs.execute(f"""SELECT t.oid, {typarray}
+FROM pg_type t JOIN pg_namespace ns
+    ON typnamespace = ns.oid
+WHERE typname = 'hstore';
+""")
+        for oids in curs:
+            rv0.append(oids[0])
+            rv1.append(oids[1])
+
+        # revert the status of the connection as before the command
+        if (conn_status != _ext.STATUS_IN_TRANSACTION
+        and not conn.autocommit):
+            conn.rollback()
+
+        return tuple(rv0), tuple(rv1)
+
+
+def register_hstore(conn_or_curs, globally=False, unicode=False,
+                    oid=None, array_oid=None):
+    r"""Register adapter and typecaster for `!dict`\-\ |hstore| conversions.
+
+    :param conn_or_curs: a connection or cursor: the typecaster will be
+        registered only on this object unless *globally* is set to `!True`
+    :param globally: register the adapter globally, not only on *conn_or_curs*
+    :param unicode: if `!True`, keys and values returned from the database
+        will be `!unicode` instead of `!str`. The option is not available on
+        Python 3
+    :param oid: the OID of the |hstore| type if known. If not, it will be
+        queried on *conn_or_curs*.
+    :param array_oid: the OID of the |hstore| array type if known. If not, it
+        will be queried on *conn_or_curs*.
+
+    The connection or cursor passed to the function will be used to query the
+    database and look for the OID of the |hstore| type (which may be different
+    across databases). If querying is not desirable (e.g. with
+    :ref:`asynchronous connections <async-support>`) you may specify it in the
+    *oid* parameter, which can be found using a query such as :sql:`SELECT
+    'hstore'::regtype::oid`. Analogously you can obtain a value for *array_oid*
+    using a query such as :sql:`SELECT 'hstore[]'::regtype::oid`.
+
+    Note that, when passing a dictionary from Python to the database, both
+    strings and unicode keys and values are supported. Dictionaries returned
+    from the database have keys/values according to the *unicode* parameter.
+
+    The |hstore| contrib module must be already installed in the database
+    (executing the ``hstore.sql`` script in your ``contrib`` directory).
+    Raise `~psycopg2.ProgrammingError` if the type is not found.
+    """
+    if oid is None:
+        oid = HstoreAdapter.get_oids(conn_or_curs)
+        if oid is None or not oid[0]:
+            raise psycopg2.ProgrammingError(
+                "hstore type not found in the database. "
+                "please install it from your 'contrib/hstore.sql' file")
+        else:
+            array_oid = oid[1]
+            oid = oid[0]
+
+    if isinstance(oid, int):
+        oid = (oid,)
+
+    if array_oid is not None:
+        if isinstance(array_oid, int):
+            array_oid = (array_oid,)
+        else:
+            array_oid = tuple([x for x in array_oid if x])
+
+    # create and register the typecaster
+    HSTORE = _ext.new_type(oid, "HSTORE", HstoreAdapter.parse)
+    _ext.register_type(HSTORE, not globally and conn_or_curs or None)
+    _ext.register_adapter(dict, HstoreAdapter)
+
+    if array_oid:
+        HSTOREARRAY = _ext.new_array_type(array_oid, "HSTOREARRAY", HSTORE)
+        _ext.register_type(HSTOREARRAY, not globally and conn_or_curs or None)
+
+
+class CompositeCaster:
+    """Helps conversion of a PostgreSQL composite type into a Python object.
+
+    The class is usually created by the `register_composite()` function.
+    You may want to create and register manually instances of the class if
+    querying the database at registration time is not desirable (such as when
+    using an :ref:`asynchronous connections <async-support>`).
+
+    """
+    def __init__(self, name, oid, attrs, array_oid=None, schema=None):
+        self.name = name
+        self.schema = schema
+        self.oid = oid
+        self.array_oid = array_oid
+
+        self.attnames = [a[0] for a in attrs]
+        self.atttypes = [a[1] for a in attrs]
+        self._create_type(name, self.attnames)
+        self.typecaster = _ext.new_type((oid,), name, self.parse)
+        if array_oid:
+            self.array_typecaster = _ext.new_array_type(
+                (array_oid,), f"{name}ARRAY", self.typecaster)
+        else:
+            self.array_typecaster = None
+
+    def parse(self, s, curs):
+        if s is None:
+            return None
+
+        tokens = self.tokenize(s)
+        if len(tokens) != len(self.atttypes):
+            raise psycopg2.DataError(
+                "expecting %d components for the type %s, %d found instead" %
+                (len(self.atttypes), self.name, len(tokens)))
+
+        values = [curs.cast(oid, token)
+            for oid, token in zip(self.atttypes, tokens)]
+
+        return self.make(values)
+
+    def make(self, values):
+        """Return a new Python object representing the data being casted.
+
+        *values* is the list of attributes, already casted into their Python
+        representation.
+
+        You can subclass this method to :ref:`customize the composite cast
+        <custom-composite>`.
+        """
+
+        return self._ctor(values)
+
+    _re_tokenize = _re.compile(r"""
+  \(? ([,)])                        # an empty token, representing NULL
+| \(? " ((?: [^"] | "")*) " [,)]    # or a quoted string
+| \(? ([^",)]+) [,)]                # or an unquoted string
+    """, _re.VERBOSE)
+
+    _re_undouble = _re.compile(r'(["\\])\1')
+
+    @classmethod
+    def tokenize(self, s):
+        rv = []
+        for m in self._re_tokenize.finditer(s):
+            if m is None:
+                raise psycopg2.InterfaceError(f"can't parse type: {s!r}")
+            if m.group(1) is not None:
+                rv.append(None)
+            elif m.group(2) is not None:
+                rv.append(self._re_undouble.sub(r"\1", m.group(2)))
+            else:
+                rv.append(m.group(3))
+
+        return rv
+
+    def _create_type(self, name, attnames):
+        self.type = namedtuple(name, attnames)
+        self._ctor = self.type._make
+
+    @classmethod
+    def _from_db(self, name, conn_or_curs):
+        """Return a `CompositeCaster` instance for the type *name*.
+
+        Raise `ProgrammingError` if the type is not found.
+        """
+        conn, curs = _solve_conn_curs(conn_or_curs)
+
+        # Store the transaction status of the connection to revert it after use
+        conn_status = conn.status
+
+        # Use the correct schema
+        if '.' in name:
+            schema, tname = name.split('.', 1)
+        else:
+            tname = name
+            schema = 'public'
+
+        # column typarray not available before PG 8.3
+        typarray = conn.info.server_version >= 80300 and "typarray" or "NULL"
+
+        # get the type oid and attributes
+        curs.execute("""\
+SELECT t.oid, %s, attname, atttypid
+FROM pg_type t
+JOIN pg_namespace ns ON typnamespace = ns.oid
+JOIN pg_attribute a ON attrelid = typrelid
+WHERE typname = %%s AND nspname = %%s
+    AND attnum > 0 AND NOT attisdropped
+ORDER BY attnum;
+""" % typarray, (tname, schema))
+
+        recs = curs.fetchall()
+
+        # revert the status of the connection as before the command
+        if (conn_status != _ext.STATUS_IN_TRANSACTION
+        and not conn.autocommit):
+            conn.rollback()
+
+        if not recs:
+            raise psycopg2.ProgrammingError(
+                f"PostgreSQL type '{name}' not found")
+
+        type_oid = recs[0][0]
+        array_oid = recs[0][1]
+        type_attrs = [(r[2], r[3]) for r in recs]
+
+        return self(tname, type_oid, type_attrs,
+            array_oid=array_oid, schema=schema)
+
+
+def register_composite(name, conn_or_curs, globally=False, factory=None):
+    """Register a typecaster to convert a composite type into a tuple.
+
+    :param name: the name of a PostgreSQL composite type, e.g. created using
+        the |CREATE TYPE|_ command
+    :param conn_or_curs: a connection or cursor used to find the type oid and
+        components; the typecaster is registered in a scope limited to this
+        object, unless *globally* is set to `!True`
+    :param globally: if `!False` (default) register the typecaster only on
+        *conn_or_curs*, otherwise register it globally
+    :param factory: if specified it should be a `CompositeCaster` subclass: use
+        it to :ref:`customize how to cast composite types <custom-composite>`
+    :return: the registered `CompositeCaster` or *factory* instance
+        responsible for the conversion
+    """
+    if factory is None:
+        factory = CompositeCaster
+
+    caster = factory._from_db(name, conn_or_curs)
+    _ext.register_type(caster.typecaster, not globally and conn_or_curs or None)
+
+    if caster.array_typecaster is not None:
+        _ext.register_type(
+            caster.array_typecaster, not globally and conn_or_curs or None)
+
+    return caster
+
+
+def _paginate(seq, page_size):
+    """Consume an iterable and return it in chunks.
+
+    Every chunk is at most `page_size`. Never return an empty chunk.
+    """
+    page = []
+    it = iter(seq)
+    while True:
+        try:
+            for i in range(page_size):
+                page.append(next(it))
+            yield page
+            page = []
+        except StopIteration:
+            if page:
+                yield page
+            return
+
+
+def execute_batch(cur, sql, argslist, page_size=100):
+    r"""Execute groups of statements in fewer server roundtrips.
+
+    Execute *sql* several times, against all parameters set (sequences or
+    mappings) found in *argslist*.
+
+    The function is semantically similar to
+
+    .. parsed-literal::
+
+        *cur*\.\ `~cursor.executemany`\ (\ *sql*\ , *argslist*\ )
+
+    but has a different implementation: Psycopg will join the statements into
+    fewer multi-statement commands, each one containing at most *page_size*
+    statements, resulting in a reduced number of server roundtrips.
+
+    After the execution of the function the `cursor.rowcount` property will
+    **not** contain a total result.
+
+    """
+    for page in _paginate(argslist, page_size=page_size):
+        sqls = [cur.mogrify(sql, args) for args in page]
+        cur.execute(b";".join(sqls))
+
+
+def execute_values(cur, sql, argslist, template=None, page_size=100, fetch=False):
+    '''Execute a statement using :sql:`VALUES` with a sequence of parameters.
+
+    :param cur: the cursor to use to execute the query.
+
+    :param sql: the query to execute. It must contain a single ``%s``
+        placeholder, which will be replaced by a `VALUES list`__.
+        Example: ``"INSERT INTO mytable (id, f1, f2) VALUES %s"``.
+
+    :param argslist: sequence of sequences or dictionaries with the arguments
+        to send to the query. The type and content must be consistent with
+        *template*.
+
+    :param template: the snippet to merge to every item in *argslist* to
+        compose the query.
+
+        - If the *argslist* items are sequences it should contain positional
+          placeholders (e.g. ``"(%s, %s, %s)"``, or ``"(%s, %s, 42)``" if there
+          are constants value...).
+
+        - If the *argslist* items are mappings it should contain named
+          placeholders (e.g. ``"(%(id)s, %(f1)s, 42)"``).
+
+        If not specified, assume the arguments are sequence and use a simple
+        positional template (i.e.  ``(%s, %s, ...)``), with the number of
+        placeholders sniffed by the first element in *argslist*.
+
+    :param page_size: maximum number of *argslist* items to include in every
+        statement. If there are more items the function will execute more than
+        one statement.
+
+    :param fetch: if `!True` return the query results into a list (like in a
+        `~cursor.fetchall()`).  Useful for queries with :sql:`RETURNING`
+        clause.
+
+    .. __: https://www.postgresql.org/docs/current/static/queries-values.html
+
+    After the execution of the function the `cursor.rowcount` property will
+    **not** contain a total result.
+
+    While :sql:`INSERT` is an obvious candidate for this function it is
+    possible to use it with other statements, for example::
+
+        >>> cur.execute(
+        ... "create table test (id int primary key, v1 int, v2 int)")
+
+        >>> execute_values(cur,
+        ... "INSERT INTO test (id, v1, v2) VALUES %s",
+        ... [(1, 2, 3), (4, 5, 6), (7, 8, 9)])
+
+        >>> execute_values(cur,
+        ... """UPDATE test SET v1 = data.v1 FROM (VALUES %s) AS data (id, v1)
+        ... WHERE test.id = data.id""",
+        ... [(1, 20), (4, 50)])
+
+        >>> cur.execute("select * from test order by id")
+        >>> cur.fetchall()
+        [(1, 20, 3), (4, 50, 6), (7, 8, 9)])
+
+    '''
+    from psycopg2.sql import Composable
+    if isinstance(sql, Composable):
+        sql = sql.as_string(cur)
+
+    # we can't just use sql % vals because vals is bytes: if sql is bytes
+    # there will be some decoding error because of stupid codec used, and Py3
+    # doesn't implement % on bytes.
+    if not isinstance(sql, bytes):
+        sql = sql.encode(_ext.encodings[cur.connection.encoding])
+    pre, post = _split_sql(sql)
+
+    result = [] if fetch else None
+    for page in _paginate(argslist, page_size=page_size):
+        if template is None:
+            template = b'(' + b','.join([b'%s'] * len(page[0])) + b')'
+        parts = pre[:]
+        for args in page:
+            parts.append(cur.mogrify(template, args))
+            parts.append(b',')
+        parts[-1:] = post
+        cur.execute(b''.join(parts))
+        if fetch:
+            result.extend(cur.fetchall())
+
+    return result
+
+
+def _split_sql(sql):
+    """Split *sql* on a single ``%s`` placeholder.
+
+    Split on the %s, perform %% replacement and return pre, post lists of
+    snippets.
+    """
+    curr = pre = []
+    post = []
+    tokens = _re.split(br'(%.)', sql)
+    for token in tokens:
+        if len(token) != 2 or token[:1] != b'%':
+            curr.append(token)
+            continue
+
+        if token[1:] == b's':
+            if curr is pre:
+                curr = post
+            else:
+                raise ValueError(
+                    "the query contains more than one '%s' placeholder")
+        elif token[1:] == b'%':
+            curr.append(b'%')
+        else:
+            raise ValueError("unsupported format character: '%s'"
+                % token[1:].decode('ascii', 'replace'))
+
+    if curr is pre:
+        raise ValueError("the query doesn't contain any '%s' placeholder")
+
+    return pre, post
Index: venv/Lib/site-packages/psycopg2/errorcodes.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/psycopg2/errorcodes.py b/venv/Lib/site-packages/psycopg2/errorcodes.py
new file mode 100644
--- /dev/null	(date 1630065394286)
+++ b/venv/Lib/site-packages/psycopg2/errorcodes.py	(date 1630065394286)
@@ -0,0 +1,447 @@
+"""Error codes for PostgreSQL
+
+This module contains symbolic names for all PostgreSQL error codes.
+"""
+# psycopg2/errorcodes.py - PostgreSQL error codes
+#
+# Copyright (C) 2006-2019 Johan Dahlin  <jdahlin@async.com.br>
+# Copyright (C) 2020-2021 The Psycopg Team
+#
+# psycopg2 is free software: you can redistribute it and/or modify it
+# under the terms of the GNU Lesser General Public License as published
+# by the Free Software Foundation, either version 3 of the License, or
+# (at your option) any later version.
+#
+# In addition, as a special exception, the copyright holders give
+# permission to link this program with the OpenSSL library (or with
+# modified versions of OpenSSL that use the same license as OpenSSL),
+# and distribute linked combinations including the two.
+#
+# You must obey the GNU Lesser General Public License in all respects for
+# all of the code used other than OpenSSL.
+#
+# psycopg2 is distributed in the hope that it will be useful, but WITHOUT
+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public
+# License for more details.
+#
+# Based on:
+#
+#   https://www.postgresql.org/docs/current/static/errcodes-appendix.html
+#
+
+
+def lookup(code, _cache={}):
+    """Lookup an error code or class code and return its symbolic name.
+
+    Raise `KeyError` if the code is not found.
+    """
+    if _cache:
+        return _cache[code]
+
+    # Generate the lookup map at first usage.
+    tmp = {}
+    for k, v in globals().items():
+        if isinstance(v, str) and len(v) in (2, 5):
+            # Strip trailing underscore used to disambiguate duplicate values
+            tmp[v] = k.rstrip("_")
+
+    assert tmp
+
+    # Atomic update, to avoid race condition on import (bug #382)
+    _cache.update(tmp)
+
+    return _cache[code]
+
+
+# autogenerated data: do not edit below this point.
+
+# Error classes
+CLASS_SUCCESSFUL_COMPLETION = '00'
+CLASS_WARNING = '01'
+CLASS_NO_DATA = '02'
+CLASS_SQL_STATEMENT_NOT_YET_COMPLETE = '03'
+CLASS_CONNECTION_EXCEPTION = '08'
+CLASS_TRIGGERED_ACTION_EXCEPTION = '09'
+CLASS_FEATURE_NOT_SUPPORTED = '0A'
+CLASS_INVALID_TRANSACTION_INITIATION = '0B'
+CLASS_LOCATOR_EXCEPTION = '0F'
+CLASS_INVALID_GRANTOR = '0L'
+CLASS_INVALID_ROLE_SPECIFICATION = '0P'
+CLASS_DIAGNOSTICS_EXCEPTION = '0Z'
+CLASS_CASE_NOT_FOUND = '20'
+CLASS_CARDINALITY_VIOLATION = '21'
+CLASS_DATA_EXCEPTION = '22'
+CLASS_INTEGRITY_CONSTRAINT_VIOLATION = '23'
+CLASS_INVALID_CURSOR_STATE = '24'
+CLASS_INVALID_TRANSACTION_STATE = '25'
+CLASS_INVALID_SQL_STATEMENT_NAME = '26'
+CLASS_TRIGGERED_DATA_CHANGE_VIOLATION = '27'
+CLASS_INVALID_AUTHORIZATION_SPECIFICATION = '28'
+CLASS_DEPENDENT_PRIVILEGE_DESCRIPTORS_STILL_EXIST = '2B'
+CLASS_INVALID_TRANSACTION_TERMINATION = '2D'
+CLASS_SQL_ROUTINE_EXCEPTION = '2F'
+CLASS_INVALID_CURSOR_NAME = '34'
+CLASS_EXTERNAL_ROUTINE_EXCEPTION = '38'
+CLASS_EXTERNAL_ROUTINE_INVOCATION_EXCEPTION = '39'
+CLASS_SAVEPOINT_EXCEPTION = '3B'
+CLASS_INVALID_CATALOG_NAME = '3D'
+CLASS_INVALID_SCHEMA_NAME = '3F'
+CLASS_TRANSACTION_ROLLBACK = '40'
+CLASS_SYNTAX_ERROR_OR_ACCESS_RULE_VIOLATION = '42'
+CLASS_WITH_CHECK_OPTION_VIOLATION = '44'
+CLASS_INSUFFICIENT_RESOURCES = '53'
+CLASS_PROGRAM_LIMIT_EXCEEDED = '54'
+CLASS_OBJECT_NOT_IN_PREREQUISITE_STATE = '55'
+CLASS_OPERATOR_INTERVENTION = '57'
+CLASS_SYSTEM_ERROR = '58'
+CLASS_SNAPSHOT_FAILURE = '72'
+CLASS_CONFIGURATION_FILE_ERROR = 'F0'
+CLASS_FOREIGN_DATA_WRAPPER_ERROR = 'HV'
+CLASS_PL_PGSQL_ERROR = 'P0'
+CLASS_INTERNAL_ERROR = 'XX'
+
+# Class 00 - Successful Completion
+SUCCESSFUL_COMPLETION = '00000'
+
+# Class 01 - Warning
+WARNING = '01000'
+NULL_VALUE_ELIMINATED_IN_SET_FUNCTION = '01003'
+STRING_DATA_RIGHT_TRUNCATION_ = '01004'
+PRIVILEGE_NOT_REVOKED = '01006'
+PRIVILEGE_NOT_GRANTED = '01007'
+IMPLICIT_ZERO_BIT_PADDING = '01008'
+DYNAMIC_RESULT_SETS_RETURNED = '0100C'
+DEPRECATED_FEATURE = '01P01'
+
+# Class 02 - No Data (this is also a warning class per the SQL standard)
+NO_DATA = '02000'
+NO_ADDITIONAL_DYNAMIC_RESULT_SETS_RETURNED = '02001'
+
+# Class 03 - SQL Statement Not Yet Complete
+SQL_STATEMENT_NOT_YET_COMPLETE = '03000'
+
+# Class 08 - Connection Exception
+CONNECTION_EXCEPTION = '08000'
+SQLCLIENT_UNABLE_TO_ESTABLISH_SQLCONNECTION = '08001'
+CONNECTION_DOES_NOT_EXIST = '08003'
+SQLSERVER_REJECTED_ESTABLISHMENT_OF_SQLCONNECTION = '08004'
+CONNECTION_FAILURE = '08006'
+TRANSACTION_RESOLUTION_UNKNOWN = '08007'
+PROTOCOL_VIOLATION = '08P01'
+
+# Class 09 - Triggered Action Exception
+TRIGGERED_ACTION_EXCEPTION = '09000'
+
+# Class 0A - Feature Not Supported
+FEATURE_NOT_SUPPORTED = '0A000'
+
+# Class 0B - Invalid Transaction Initiation
+INVALID_TRANSACTION_INITIATION = '0B000'
+
+# Class 0F - Locator Exception
+LOCATOR_EXCEPTION = '0F000'
+INVALID_LOCATOR_SPECIFICATION = '0F001'
+
+# Class 0L - Invalid Grantor
+INVALID_GRANTOR = '0L000'
+INVALID_GRANT_OPERATION = '0LP01'
+
+# Class 0P - Invalid Role Specification
+INVALID_ROLE_SPECIFICATION = '0P000'
+
+# Class 0Z - Diagnostics Exception
+DIAGNOSTICS_EXCEPTION = '0Z000'
+STACKED_DIAGNOSTICS_ACCESSED_WITHOUT_ACTIVE_HANDLER = '0Z002'
+
+# Class 20 - Case Not Found
+CASE_NOT_FOUND = '20000'
+
+# Class 21 - Cardinality Violation
+CARDINALITY_VIOLATION = '21000'
+
+# Class 22 - Data Exception
+DATA_EXCEPTION = '22000'
+STRING_DATA_RIGHT_TRUNCATION = '22001'
+NULL_VALUE_NO_INDICATOR_PARAMETER = '22002'
+NUMERIC_VALUE_OUT_OF_RANGE = '22003'
+NULL_VALUE_NOT_ALLOWED_ = '22004'
+ERROR_IN_ASSIGNMENT = '22005'
+INVALID_DATETIME_FORMAT = '22007'
+DATETIME_FIELD_OVERFLOW = '22008'
+INVALID_TIME_ZONE_DISPLACEMENT_VALUE = '22009'
+ESCAPE_CHARACTER_CONFLICT = '2200B'
+INVALID_USE_OF_ESCAPE_CHARACTER = '2200C'
+INVALID_ESCAPE_OCTET = '2200D'
+ZERO_LENGTH_CHARACTER_STRING = '2200F'
+MOST_SPECIFIC_TYPE_MISMATCH = '2200G'
+SEQUENCE_GENERATOR_LIMIT_EXCEEDED = '2200H'
+NOT_AN_XML_DOCUMENT = '2200L'
+INVALID_XML_DOCUMENT = '2200M'
+INVALID_XML_CONTENT = '2200N'
+INVALID_XML_COMMENT = '2200S'
+INVALID_XML_PROCESSING_INSTRUCTION = '2200T'
+INVALID_INDICATOR_PARAMETER_VALUE = '22010'
+SUBSTRING_ERROR = '22011'
+DIVISION_BY_ZERO = '22012'
+INVALID_PRECEDING_OR_FOLLOWING_SIZE = '22013'
+INVALID_ARGUMENT_FOR_NTILE_FUNCTION = '22014'
+INTERVAL_FIELD_OVERFLOW = '22015'
+INVALID_ARGUMENT_FOR_NTH_VALUE_FUNCTION = '22016'
+INVALID_CHARACTER_VALUE_FOR_CAST = '22018'
+INVALID_ESCAPE_CHARACTER = '22019'
+INVALID_REGULAR_EXPRESSION = '2201B'
+INVALID_ARGUMENT_FOR_LOGARITHM = '2201E'
+INVALID_ARGUMENT_FOR_POWER_FUNCTION = '2201F'
+INVALID_ARGUMENT_FOR_WIDTH_BUCKET_FUNCTION = '2201G'
+INVALID_ROW_COUNT_IN_LIMIT_CLAUSE = '2201W'
+INVALID_ROW_COUNT_IN_RESULT_OFFSET_CLAUSE = '2201X'
+INVALID_LIMIT_VALUE = '22020'
+CHARACTER_NOT_IN_REPERTOIRE = '22021'
+INDICATOR_OVERFLOW = '22022'
+INVALID_PARAMETER_VALUE = '22023'
+UNTERMINATED_C_STRING = '22024'
+INVALID_ESCAPE_SEQUENCE = '22025'
+STRING_DATA_LENGTH_MISMATCH = '22026'
+TRIM_ERROR = '22027'
+ARRAY_SUBSCRIPT_ERROR = '2202E'
+INVALID_TABLESAMPLE_REPEAT = '2202G'
+INVALID_TABLESAMPLE_ARGUMENT = '2202H'
+DUPLICATE_JSON_OBJECT_KEY_VALUE = '22030'
+INVALID_ARGUMENT_FOR_SQL_JSON_DATETIME_FUNCTION = '22031'
+INVALID_JSON_TEXT = '22032'
+INVALID_SQL_JSON_SUBSCRIPT = '22033'
+MORE_THAN_ONE_SQL_JSON_ITEM = '22034'
+NO_SQL_JSON_ITEM = '22035'
+NON_NUMERIC_SQL_JSON_ITEM = '22036'
+NON_UNIQUE_KEYS_IN_A_JSON_OBJECT = '22037'
+SINGLETON_SQL_JSON_ITEM_REQUIRED = '22038'
+SQL_JSON_ARRAY_NOT_FOUND = '22039'
+SQL_JSON_MEMBER_NOT_FOUND = '2203A'
+SQL_JSON_NUMBER_NOT_FOUND = '2203B'
+SQL_JSON_OBJECT_NOT_FOUND = '2203C'
+TOO_MANY_JSON_ARRAY_ELEMENTS = '2203D'
+TOO_MANY_JSON_OBJECT_MEMBERS = '2203E'
+SQL_JSON_SCALAR_REQUIRED = '2203F'
+FLOATING_POINT_EXCEPTION = '22P01'
+INVALID_TEXT_REPRESENTATION = '22P02'
+INVALID_BINARY_REPRESENTATION = '22P03'
+BAD_COPY_FILE_FORMAT = '22P04'
+UNTRANSLATABLE_CHARACTER = '22P05'
+NONSTANDARD_USE_OF_ESCAPE_CHARACTER = '22P06'
+
+# Class 23 - Integrity Constraint Violation
+INTEGRITY_CONSTRAINT_VIOLATION = '23000'
+RESTRICT_VIOLATION = '23001'
+NOT_NULL_VIOLATION = '23502'
+FOREIGN_KEY_VIOLATION = '23503'
+UNIQUE_VIOLATION = '23505'
+CHECK_VIOLATION = '23514'
+EXCLUSION_VIOLATION = '23P01'
+
+# Class 24 - Invalid Cursor State
+INVALID_CURSOR_STATE = '24000'
+
+# Class 25 - Invalid Transaction State
+INVALID_TRANSACTION_STATE = '25000'
+ACTIVE_SQL_TRANSACTION = '25001'
+BRANCH_TRANSACTION_ALREADY_ACTIVE = '25002'
+INAPPROPRIATE_ACCESS_MODE_FOR_BRANCH_TRANSACTION = '25003'
+INAPPROPRIATE_ISOLATION_LEVEL_FOR_BRANCH_TRANSACTION = '25004'
+NO_ACTIVE_SQL_TRANSACTION_FOR_BRANCH_TRANSACTION = '25005'
+READ_ONLY_SQL_TRANSACTION = '25006'
+SCHEMA_AND_DATA_STATEMENT_MIXING_NOT_SUPPORTED = '25007'
+HELD_CURSOR_REQUIRES_SAME_ISOLATION_LEVEL = '25008'
+NO_ACTIVE_SQL_TRANSACTION = '25P01'
+IN_FAILED_SQL_TRANSACTION = '25P02'
+IDLE_IN_TRANSACTION_SESSION_TIMEOUT = '25P03'
+
+# Class 26 - Invalid SQL Statement Name
+INVALID_SQL_STATEMENT_NAME = '26000'
+
+# Class 27 - Triggered Data Change Violation
+TRIGGERED_DATA_CHANGE_VIOLATION = '27000'
+
+# Class 28 - Invalid Authorization Specification
+INVALID_AUTHORIZATION_SPECIFICATION = '28000'
+INVALID_PASSWORD = '28P01'
+
+# Class 2B - Dependent Privilege Descriptors Still Exist
+DEPENDENT_PRIVILEGE_DESCRIPTORS_STILL_EXIST = '2B000'
+DEPENDENT_OBJECTS_STILL_EXIST = '2BP01'
+
+# Class 2D - Invalid Transaction Termination
+INVALID_TRANSACTION_TERMINATION = '2D000'
+
+# Class 2F - SQL Routine Exception
+SQL_ROUTINE_EXCEPTION = '2F000'
+MODIFYING_SQL_DATA_NOT_PERMITTED_ = '2F002'
+PROHIBITED_SQL_STATEMENT_ATTEMPTED_ = '2F003'
+READING_SQL_DATA_NOT_PERMITTED_ = '2F004'
+FUNCTION_EXECUTED_NO_RETURN_STATEMENT = '2F005'
+
+# Class 34 - Invalid Cursor Name
+INVALID_CURSOR_NAME = '34000'
+
+# Class 38 - External Routine Exception
+EXTERNAL_ROUTINE_EXCEPTION = '38000'
+CONTAINING_SQL_NOT_PERMITTED = '38001'
+MODIFYING_SQL_DATA_NOT_PERMITTED = '38002'
+PROHIBITED_SQL_STATEMENT_ATTEMPTED = '38003'
+READING_SQL_DATA_NOT_PERMITTED = '38004'
+
+# Class 39 - External Routine Invocation Exception
+EXTERNAL_ROUTINE_INVOCATION_EXCEPTION = '39000'
+INVALID_SQLSTATE_RETURNED = '39001'
+NULL_VALUE_NOT_ALLOWED = '39004'
+TRIGGER_PROTOCOL_VIOLATED = '39P01'
+SRF_PROTOCOL_VIOLATED = '39P02'
+EVENT_TRIGGER_PROTOCOL_VIOLATED = '39P03'
+
+# Class 3B - Savepoint Exception
+SAVEPOINT_EXCEPTION = '3B000'
+INVALID_SAVEPOINT_SPECIFICATION = '3B001'
+
+# Class 3D - Invalid Catalog Name
+INVALID_CATALOG_NAME = '3D000'
+
+# Class 3F - Invalid Schema Name
+INVALID_SCHEMA_NAME = '3F000'
+
+# Class 40 - Transaction Rollback
+TRANSACTION_ROLLBACK = '40000'
+SERIALIZATION_FAILURE = '40001'
+TRANSACTION_INTEGRITY_CONSTRAINT_VIOLATION = '40002'
+STATEMENT_COMPLETION_UNKNOWN = '40003'
+DEADLOCK_DETECTED = '40P01'
+
+# Class 42 - Syntax Error or Access Rule Violation
+SYNTAX_ERROR_OR_ACCESS_RULE_VIOLATION = '42000'
+INSUFFICIENT_PRIVILEGE = '42501'
+SYNTAX_ERROR = '42601'
+INVALID_NAME = '42602'
+INVALID_COLUMN_DEFINITION = '42611'
+NAME_TOO_LONG = '42622'
+DUPLICATE_COLUMN = '42701'
+AMBIGUOUS_COLUMN = '42702'
+UNDEFINED_COLUMN = '42703'
+UNDEFINED_OBJECT = '42704'
+DUPLICATE_OBJECT = '42710'
+DUPLICATE_ALIAS = '42712'
+DUPLICATE_FUNCTION = '42723'
+AMBIGUOUS_FUNCTION = '42725'
+GROUPING_ERROR = '42803'
+DATATYPE_MISMATCH = '42804'
+WRONG_OBJECT_TYPE = '42809'
+INVALID_FOREIGN_KEY = '42830'
+CANNOT_COERCE = '42846'
+UNDEFINED_FUNCTION = '42883'
+GENERATED_ALWAYS = '428C9'
+RESERVED_NAME = '42939'
+UNDEFINED_TABLE = '42P01'
+UNDEFINED_PARAMETER = '42P02'
+DUPLICATE_CURSOR = '42P03'
+DUPLICATE_DATABASE = '42P04'
+DUPLICATE_PREPARED_STATEMENT = '42P05'
+DUPLICATE_SCHEMA = '42P06'
+DUPLICATE_TABLE = '42P07'
+AMBIGUOUS_PARAMETER = '42P08'
+AMBIGUOUS_ALIAS = '42P09'
+INVALID_COLUMN_REFERENCE = '42P10'
+INVALID_CURSOR_DEFINITION = '42P11'
+INVALID_DATABASE_DEFINITION = '42P12'
+INVALID_FUNCTION_DEFINITION = '42P13'
+INVALID_PREPARED_STATEMENT_DEFINITION = '42P14'
+INVALID_SCHEMA_DEFINITION = '42P15'
+INVALID_TABLE_DEFINITION = '42P16'
+INVALID_OBJECT_DEFINITION = '42P17'
+INDETERMINATE_DATATYPE = '42P18'
+INVALID_RECURSION = '42P19'
+WINDOWING_ERROR = '42P20'
+COLLATION_MISMATCH = '42P21'
+INDETERMINATE_COLLATION = '42P22'
+
+# Class 44 - WITH CHECK OPTION Violation
+WITH_CHECK_OPTION_VIOLATION = '44000'
+
+# Class 53 - Insufficient Resources
+INSUFFICIENT_RESOURCES = '53000'
+DISK_FULL = '53100'
+OUT_OF_MEMORY = '53200'
+TOO_MANY_CONNECTIONS = '53300'
+CONFIGURATION_LIMIT_EXCEEDED = '53400'
+
+# Class 54 - Program Limit Exceeded
+PROGRAM_LIMIT_EXCEEDED = '54000'
+STATEMENT_TOO_COMPLEX = '54001'
+TOO_MANY_COLUMNS = '54011'
+TOO_MANY_ARGUMENTS = '54023'
+
+# Class 55 - Object Not In Prerequisite State
+OBJECT_NOT_IN_PREREQUISITE_STATE = '55000'
+OBJECT_IN_USE = '55006'
+CANT_CHANGE_RUNTIME_PARAM = '55P02'
+LOCK_NOT_AVAILABLE = '55P03'
+UNSAFE_NEW_ENUM_VALUE_USAGE = '55P04'
+
+# Class 57 - Operator Intervention
+OPERATOR_INTERVENTION = '57000'
+QUERY_CANCELED = '57014'
+ADMIN_SHUTDOWN = '57P01'
+CRASH_SHUTDOWN = '57P02'
+CANNOT_CONNECT_NOW = '57P03'
+DATABASE_DROPPED = '57P04'
+
+# Class 58 - System Error (errors external to PostgreSQL itself)
+SYSTEM_ERROR = '58000'
+IO_ERROR = '58030'
+UNDEFINED_FILE = '58P01'
+DUPLICATE_FILE = '58P02'
+
+# Class 72 - Snapshot Failure
+SNAPSHOT_TOO_OLD = '72000'
+
+# Class F0 - Configuration File Error
+CONFIG_FILE_ERROR = 'F0000'
+LOCK_FILE_EXISTS = 'F0001'
+
+# Class HV - Foreign Data Wrapper Error (SQL/MED)
+FDW_ERROR = 'HV000'
+FDW_OUT_OF_MEMORY = 'HV001'
+FDW_DYNAMIC_PARAMETER_VALUE_NEEDED = 'HV002'
+FDW_INVALID_DATA_TYPE = 'HV004'
+FDW_COLUMN_NAME_NOT_FOUND = 'HV005'
+FDW_INVALID_DATA_TYPE_DESCRIPTORS = 'HV006'
+FDW_INVALID_COLUMN_NAME = 'HV007'
+FDW_INVALID_COLUMN_NUMBER = 'HV008'
+FDW_INVALID_USE_OF_NULL_POINTER = 'HV009'
+FDW_INVALID_STRING_FORMAT = 'HV00A'
+FDW_INVALID_HANDLE = 'HV00B'
+FDW_INVALID_OPTION_INDEX = 'HV00C'
+FDW_INVALID_OPTION_NAME = 'HV00D'
+FDW_OPTION_NAME_NOT_FOUND = 'HV00J'
+FDW_REPLY_HANDLE = 'HV00K'
+FDW_UNABLE_TO_CREATE_EXECUTION = 'HV00L'
+FDW_UNABLE_TO_CREATE_REPLY = 'HV00M'
+FDW_UNABLE_TO_ESTABLISH_CONNECTION = 'HV00N'
+FDW_NO_SCHEMAS = 'HV00P'
+FDW_SCHEMA_NOT_FOUND = 'HV00Q'
+FDW_TABLE_NOT_FOUND = 'HV00R'
+FDW_FUNCTION_SEQUENCE_ERROR = 'HV010'
+FDW_TOO_MANY_HANDLES = 'HV014'
+FDW_INCONSISTENT_DESCRIPTOR_INFORMATION = 'HV021'
+FDW_INVALID_ATTRIBUTE_VALUE = 'HV024'
+FDW_INVALID_STRING_LENGTH_OR_BUFFER_LENGTH = 'HV090'
+FDW_INVALID_DESCRIPTOR_FIELD_IDENTIFIER = 'HV091'
+
+# Class P0 - PL/pgSQL Error
+PLPGSQL_ERROR = 'P0000'
+RAISE_EXCEPTION = 'P0001'
+NO_DATA_FOUND = 'P0002'
+TOO_MANY_ROWS = 'P0003'
+ASSERT_FAILURE = 'P0004'
+
+# Class XX - Internal Error
+INTERNAL_ERROR = 'XX000'
+DATA_CORRUPTED = 'XX001'
+INDEX_CORRUPTED = 'XX002'
Index: venv/Lib/site-packages/psycopg2/errors.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/psycopg2/errors.py b/venv/Lib/site-packages/psycopg2/errors.py
new file mode 100644
--- /dev/null	(date 1630065394289)
+++ b/venv/Lib/site-packages/psycopg2/errors.py	(date 1630065394289)
@@ -0,0 +1,38 @@
+"""Error classes for PostgreSQL error codes
+"""
+
+# psycopg/errors.py - SQLSTATE and DB-API exceptions
+#
+# Copyright (C) 2018-2019 Daniele Varrazzo  <daniele.varrazzo@gmail.com>
+# Copyright (C) 2020-2021 The Psycopg Team
+#
+# psycopg2 is free software: you can redistribute it and/or modify it
+# under the terms of the GNU Lesser General Public License as published
+# by the Free Software Foundation, either version 3 of the License, or
+# (at your option) any later version.
+#
+# In addition, as a special exception, the copyright holders give
+# permission to link this program with the OpenSSL library (or with
+# modified versions of OpenSSL that use the same license as OpenSSL),
+# and distribute linked combinations including the two.
+#
+# You must obey the GNU Lesser General Public License in all respects for
+# all of the code used other than OpenSSL.
+#
+# psycopg2 is distributed in the hope that it will be useful, but WITHOUT
+# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU Lesser General Public
+# License for more details.
+
+#
+# NOTE: the exceptions are injected into this module by the C extention.
+#
+
+
+def lookup(code):
+    """Lookup an error code and return its exception class.
+
+    Raise `!KeyError` if the code is not found.
+    """
+    from psycopg2._psycopg import sqlstate_errors   # avoid circular import
+    return sqlstate_errors[code]
Index: venv/Lib/site-packages/gunicorn/errors.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/errors.py b/venv/Lib/site-packages/gunicorn/errors.py
new file mode 100644
--- /dev/null	(date 1630065626331)
+++ b/venv/Lib/site-packages/gunicorn/errors.py	(date 1630065626331)
@@ -0,0 +1,29 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+# We don't need to call super() in __init__ methods of our
+# BaseException and Exception classes because we also define
+# our own __str__ methods so there is no need to pass 'message'
+# to the base class to get a meaningful output from 'str(exc)'.
+# pylint: disable=super-init-not-called
+
+
+# we inherit from BaseException here to make sure to not be caught
+# at application level
+class HaltServer(BaseException):
+    def __init__(self, reason, exit_status=1):
+        self.reason = reason
+        self.exit_status = exit_status
+
+    def __str__(self):
+        return "<HaltServer %r %d>" % (self.reason, self.exit_status)
+
+
+class ConfigError(Exception):
+    """ Exception raised on config error """
+
+
+class AppImportError(Exception):
+    """ Exception raised when loading an application """
Index: venv/Lib/site-packages/gunicorn/glogging.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/glogging.py b/venv/Lib/site-packages/gunicorn/glogging.py
new file mode 100644
--- /dev/null	(date 1630065626335)
+++ b/venv/Lib/site-packages/gunicorn/glogging.py	(date 1630065626335)
@@ -0,0 +1,464 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+import base64
+import binascii
+import time
+import logging
+logging.Logger.manager.emittedNoHandlerWarning = 1  # noqa
+from logging.config import dictConfig
+from logging.config import fileConfig
+import os
+import socket
+import sys
+import threading
+import traceback
+
+from gunicorn import util
+
+
+# syslog facility codes
+SYSLOG_FACILITIES = {
+    "auth": 4,
+    "authpriv": 10,
+    "cron": 9,
+    "daemon": 3,
+    "ftp": 11,
+    "kern": 0,
+    "lpr": 6,
+    "mail": 2,
+    "news": 7,
+    "security": 4,  # DEPRECATED
+    "syslog": 5,
+    "user": 1,
+    "uucp": 8,
+    "local0": 16,
+    "local1": 17,
+    "local2": 18,
+    "local3": 19,
+    "local4": 20,
+    "local5": 21,
+    "local6": 22,
+    "local7": 23
+}
+
+
+CONFIG_DEFAULTS = dict(
+        version=1,
+        disable_existing_loggers=False,
+
+        root={"level": "INFO", "handlers": ["console"]},
+        loggers={
+            "gunicorn.error": {
+                "level": "INFO",
+                "handlers": ["error_console"],
+                "propagate": True,
+                "qualname": "gunicorn.error"
+            },
+
+            "gunicorn.access": {
+                "level": "INFO",
+                "handlers": ["console"],
+                "propagate": True,
+                "qualname": "gunicorn.access"
+            }
+        },
+        handlers={
+            "console": {
+                "class": "logging.StreamHandler",
+                "formatter": "generic",
+                "stream": "ext://sys.stdout"
+            },
+            "error_console": {
+                "class": "logging.StreamHandler",
+                "formatter": "generic",
+                "stream": "ext://sys.stderr"
+            },
+        },
+        formatters={
+            "generic": {
+                "format": "%(asctime)s [%(process)d] [%(levelname)s] %(message)s",
+                "datefmt": "[%Y-%m-%d %H:%M:%S %z]",
+                "class": "logging.Formatter"
+            }
+        }
+)
+
+
+def loggers():
+    """ get list of all loggers """
+    root = logging.root
+    existing = root.manager.loggerDict.keys()
+    return [logging.getLogger(name) for name in existing]
+
+
+class SafeAtoms(dict):
+
+    def __init__(self, atoms):
+        dict.__init__(self)
+        for key, value in atoms.items():
+            if isinstance(value, str):
+                self[key] = value.replace('"', '\\"')
+            else:
+                self[key] = value
+
+    def __getitem__(self, k):
+        if k.startswith("{"):
+            kl = k.lower()
+            if kl in self:
+                return super().__getitem__(kl)
+            else:
+                return "-"
+        if k in self:
+            return super().__getitem__(k)
+        else:
+            return '-'
+
+
+def parse_syslog_address(addr):
+
+    # unix domain socket type depends on backend
+    # SysLogHandler will try both when given None
+    if addr.startswith("unix://"):
+        sock_type = None
+
+        # set socket type only if explicitly requested
+        parts = addr.split("#", 1)
+        if len(parts) == 2:
+            addr = parts[0]
+            if parts[1] == "dgram":
+                sock_type = socket.SOCK_DGRAM
+
+        return (sock_type, addr.split("unix://")[1])
+
+    if addr.startswith("udp://"):
+        addr = addr.split("udp://")[1]
+        socktype = socket.SOCK_DGRAM
+    elif addr.startswith("tcp://"):
+        addr = addr.split("tcp://")[1]
+        socktype = socket.SOCK_STREAM
+    else:
+        raise RuntimeError("invalid syslog address")
+
+    if '[' in addr and ']' in addr:
+        host = addr.split(']')[0][1:].lower()
+    elif ':' in addr:
+        host = addr.split(':')[0].lower()
+    elif addr == "":
+        host = "localhost"
+    else:
+        host = addr.lower()
+
+    addr = addr.split(']')[-1]
+    if ":" in addr:
+        port = addr.split(':', 1)[1]
+        if not port.isdigit():
+            raise RuntimeError("%r is not a valid port number." % port)
+        port = int(port)
+    else:
+        port = 514
+
+    return (socktype, (host, port))
+
+
+class Logger(object):
+
+    LOG_LEVELS = {
+        "critical": logging.CRITICAL,
+        "error": logging.ERROR,
+        "warning": logging.WARNING,
+        "info": logging.INFO,
+        "debug": logging.DEBUG
+    }
+    loglevel = logging.INFO
+
+    error_fmt = r"%(asctime)s [%(process)d] [%(levelname)s] %(message)s"
+    datefmt = r"[%Y-%m-%d %H:%M:%S %z]"
+
+    access_fmt = "%(message)s"
+    syslog_fmt = "[%(process)d] %(message)s"
+
+    atoms_wrapper_class = SafeAtoms
+
+    def __init__(self, cfg):
+        self.error_log = logging.getLogger("gunicorn.error")
+        self.error_log.propagate = False
+        self.access_log = logging.getLogger("gunicorn.access")
+        self.access_log.propagate = False
+        self.error_handlers = []
+        self.access_handlers = []
+        self.logfile = None
+        self.lock = threading.Lock()
+        self.cfg = cfg
+        self.setup(cfg)
+
+    def setup(self, cfg):
+        self.loglevel = self.LOG_LEVELS.get(cfg.loglevel.lower(), logging.INFO)
+        self.error_log.setLevel(self.loglevel)
+        self.access_log.setLevel(logging.INFO)
+
+        # set gunicorn.error handler
+        if self.cfg.capture_output and cfg.errorlog != "-":
+            for stream in sys.stdout, sys.stderr:
+                stream.flush()
+
+            self.logfile = open(cfg.errorlog, 'a+')
+            os.dup2(self.logfile.fileno(), sys.stdout.fileno())
+            os.dup2(self.logfile.fileno(), sys.stderr.fileno())
+
+        self._set_handler(self.error_log, cfg.errorlog,
+                          logging.Formatter(self.error_fmt, self.datefmt))
+
+        # set gunicorn.access handler
+        if cfg.accesslog is not None:
+            self._set_handler(
+                self.access_log, cfg.accesslog,
+                fmt=logging.Formatter(self.access_fmt), stream=sys.stdout
+            )
+
+        # set syslog handler
+        if cfg.syslog:
+            self._set_syslog_handler(
+                self.error_log, cfg, self.syslog_fmt, "error"
+            )
+            if not cfg.disable_redirect_access_to_syslog:
+                self._set_syslog_handler(
+                    self.access_log, cfg, self.syslog_fmt, "access"
+                )
+
+        if cfg.logconfig_dict:
+            config = CONFIG_DEFAULTS.copy()
+            config.update(cfg.logconfig_dict)
+            try:
+                dictConfig(config)
+            except (
+                    AttributeError,
+                    ImportError,
+                    ValueError,
+                    TypeError
+            ) as exc:
+                raise RuntimeError(str(exc))
+        elif cfg.logconfig:
+            if os.path.exists(cfg.logconfig):
+                defaults = CONFIG_DEFAULTS.copy()
+                defaults['__file__'] = cfg.logconfig
+                defaults['here'] = os.path.dirname(cfg.logconfig)
+                fileConfig(cfg.logconfig, defaults=defaults,
+                           disable_existing_loggers=False)
+            else:
+                msg = "Error: log config '%s' not found"
+                raise RuntimeError(msg % cfg.logconfig)
+
+    def critical(self, msg, *args, **kwargs):
+        self.error_log.critical(msg, *args, **kwargs)
+
+    def error(self, msg, *args, **kwargs):
+        self.error_log.error(msg, *args, **kwargs)
+
+    def warning(self, msg, *args, **kwargs):
+        self.error_log.warning(msg, *args, **kwargs)
+
+    def info(self, msg, *args, **kwargs):
+        self.error_log.info(msg, *args, **kwargs)
+
+    def debug(self, msg, *args, **kwargs):
+        self.error_log.debug(msg, *args, **kwargs)
+
+    def exception(self, msg, *args, **kwargs):
+        self.error_log.exception(msg, *args, **kwargs)
+
+    def log(self, lvl, msg, *args, **kwargs):
+        if isinstance(lvl, str):
+            lvl = self.LOG_LEVELS.get(lvl.lower(), logging.INFO)
+        self.error_log.log(lvl, msg, *args, **kwargs)
+
+    def atoms(self, resp, req, environ, request_time):
+        """ Gets atoms for log formating.
+        """
+        status = resp.status
+        if isinstance(status, str):
+            status = status.split(None, 1)[0]
+        atoms = {
+            'h': environ.get('REMOTE_ADDR', '-'),
+            'l': '-',
+            'u': self._get_user(environ) or '-',
+            't': self.now(),
+            'r': "%s %s %s" % (environ['REQUEST_METHOD'],
+                               environ['RAW_URI'],
+                               environ["SERVER_PROTOCOL"]),
+            's': status,
+            'm': environ.get('REQUEST_METHOD'),
+            'U': environ.get('PATH_INFO'),
+            'q': environ.get('QUERY_STRING'),
+            'H': environ.get('SERVER_PROTOCOL'),
+            'b': getattr(resp, 'sent', None) is not None and str(resp.sent) or '-',
+            'B': getattr(resp, 'sent', None),
+            'f': environ.get('HTTP_REFERER', '-'),
+            'a': environ.get('HTTP_USER_AGENT', '-'),
+            'T': request_time.seconds,
+            'D': (request_time.seconds * 1000000) + request_time.microseconds,
+            'M': (request_time.seconds * 1000) + int(request_time.microseconds/1000),
+            'L': "%d.%06d" % (request_time.seconds, request_time.microseconds),
+            'p': "<%s>" % os.getpid()
+        }
+
+        # add request headers
+        if hasattr(req, 'headers'):
+            req_headers = req.headers
+        else:
+            req_headers = req
+
+        if hasattr(req_headers, "items"):
+            req_headers = req_headers.items()
+
+        atoms.update({"{%s}i" % k.lower(): v for k, v in req_headers})
+
+        resp_headers = resp.headers
+        if hasattr(resp_headers, "items"):
+            resp_headers = resp_headers.items()
+
+        # add response headers
+        atoms.update({"{%s}o" % k.lower(): v for k, v in resp_headers})
+
+        # add environ variables
+        environ_variables = environ.items()
+        atoms.update({"{%s}e" % k.lower(): v for k, v in environ_variables})
+
+        return atoms
+
+    def access(self, resp, req, environ, request_time):
+        """ See http://httpd.apache.org/docs/2.0/logs.html#combined
+        for format details
+        """
+
+        if not (self.cfg.accesslog or self.cfg.logconfig or
+           self.cfg.logconfig_dict or
+           (self.cfg.syslog and not self.cfg.disable_redirect_access_to_syslog)):
+            return
+
+        # wrap atoms:
+        # - make sure atoms will be test case insensitively
+        # - if atom doesn't exist replace it by '-'
+        safe_atoms = self.atoms_wrapper_class(
+            self.atoms(resp, req, environ, request_time)
+        )
+
+        try:
+            self.access_log.info(self.cfg.access_log_format, safe_atoms)
+        except Exception:
+            self.error(traceback.format_exc())
+
+    def now(self):
+        """ return date in Apache Common Log Format """
+        return time.strftime('[%d/%b/%Y:%H:%M:%S %z]')
+
+    def reopen_files(self):
+        if self.cfg.capture_output and self.cfg.errorlog != "-":
+            for stream in sys.stdout, sys.stderr:
+                stream.flush()
+
+            with self.lock:
+                if self.logfile is not None:
+                    self.logfile.close()
+                self.logfile = open(self.cfg.errorlog, 'a+')
+                os.dup2(self.logfile.fileno(), sys.stdout.fileno())
+                os.dup2(self.logfile.fileno(), sys.stderr.fileno())
+
+        for log in loggers():
+            for handler in log.handlers:
+                if isinstance(handler, logging.FileHandler):
+                    handler.acquire()
+                    try:
+                        if handler.stream:
+                            handler.close()
+                            handler.stream = handler._open()
+                    finally:
+                        handler.release()
+
+    def close_on_exec(self):
+        for log in loggers():
+            for handler in log.handlers:
+                if isinstance(handler, logging.FileHandler):
+                    handler.acquire()
+                    try:
+                        if handler.stream:
+                            util.close_on_exec(handler.stream.fileno())
+                    finally:
+                        handler.release()
+
+    def _get_gunicorn_handler(self, log):
+        for h in log.handlers:
+            if getattr(h, "_gunicorn", False):
+                return h
+
+    def _set_handler(self, log, output, fmt, stream=None):
+        # remove previous gunicorn log handler
+        h = self._get_gunicorn_handler(log)
+        if h:
+            log.handlers.remove(h)
+
+        if output is not None:
+            if output == "-":
+                h = logging.StreamHandler(stream)
+            else:
+                util.check_is_writeable(output)
+                h = logging.FileHandler(output)
+                # make sure the user can reopen the file
+                try:
+                    os.chown(h.baseFilename, self.cfg.user, self.cfg.group)
+                except OSError:
+                    # it's probably OK there, we assume the user has given
+                    # /dev/null as a parameter.
+                    pass
+
+            h.setFormatter(fmt)
+            h._gunicorn = True
+            log.addHandler(h)
+
+    def _set_syslog_handler(self, log, cfg, fmt, name):
+        # setup format
+        prefix = cfg.syslog_prefix or cfg.proc_name.replace(":", ".")
+
+        prefix = "gunicorn.%s.%s" % (prefix, name)
+
+        # set format
+        fmt = logging.Formatter(r"%s: %s" % (prefix, fmt))
+
+        # syslog facility
+        try:
+            facility = SYSLOG_FACILITIES[cfg.syslog_facility.lower()]
+        except KeyError:
+            raise RuntimeError("unknown facility name")
+
+        # parse syslog address
+        socktype, addr = parse_syslog_address(cfg.syslog_addr)
+
+        # finally setup the syslog handler
+        h = logging.handlers.SysLogHandler(address=addr,
+                facility=facility, socktype=socktype)
+
+        h.setFormatter(fmt)
+        h._gunicorn = True
+        log.addHandler(h)
+
+    def _get_user(self, environ):
+        user = None
+        http_auth = environ.get("HTTP_AUTHORIZATION")
+        if http_auth and http_auth.lower().startswith('basic'):
+            auth = http_auth.split(" ", 1)
+            if len(auth) == 2:
+                try:
+                    # b64decode doesn't accept unicode in Python < 3.3
+                    # so we need to convert it to a byte string
+                    auth = base64.b64decode(auth[1].strip().encode('utf-8'))
+                    # b64decode returns a byte string
+                    auth = auth.decode('utf-8')
+                    auth = auth.split(":", 1)
+                except (TypeError, binascii.Error, UnicodeDecodeError) as exc:
+                    self.debug("Couldn't get username: %s", exc)
+                    return user
+                if len(auth) == 2:
+                    user = auth[0]
+        return user
Index: venv/Lib/site-packages/gunicorn/config.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/config.py b/venv/Lib/site-packages/gunicorn/config.py
new file mode 100644
--- /dev/null	(date 1630065626324)
+++ b/venv/Lib/site-packages/gunicorn/config.py	(date 1630065626324)
@@ -0,0 +1,2190 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+# Please remember to run "make -C docs html" after update "desc" attributes.
+
+import argparse
+import copy
+import grp
+import inspect
+import os
+import pwd
+import re
+import shlex
+import ssl
+import sys
+import textwrap
+
+from gunicorn import __version__, util
+from gunicorn.errors import ConfigError
+from gunicorn.reloader import reloader_engines
+
+KNOWN_SETTINGS = []
+PLATFORM = sys.platform
+
+
+def make_settings(ignore=None):
+    settings = {}
+    ignore = ignore or ()
+    for s in KNOWN_SETTINGS:
+        setting = s()
+        if setting.name in ignore:
+            continue
+        settings[setting.name] = setting.copy()
+    return settings
+
+
+def auto_int(_, x):
+    # for compatible with octal numbers in python3
+    if re.match(r'0(\d)', x, re.IGNORECASE):
+        x = x.replace('0', '0o', 1)
+    return int(x, 0)
+
+
+class Config(object):
+
+    def __init__(self, usage=None, prog=None):
+        self.settings = make_settings()
+        self.usage = usage
+        self.prog = prog or os.path.basename(sys.argv[0])
+        self.env_orig = os.environ.copy()
+
+    def __str__(self):
+        lines = []
+        kmax = max(len(k) for k in self.settings)
+        for k in sorted(self.settings):
+            v = self.settings[k].value
+            if callable(v):
+                v = "<{}()>".format(v.__qualname__)
+            lines.append("{k:{kmax}} = {v}".format(k=k, v=v, kmax=kmax))
+        return "\n".join(lines)
+
+    def __getattr__(self, name):
+        if name not in self.settings:
+            raise AttributeError("No configuration setting for: %s" % name)
+        return self.settings[name].get()
+
+    def __setattr__(self, name, value):
+        if name != "settings" and name in self.settings:
+            raise AttributeError("Invalid access!")
+        super().__setattr__(name, value)
+
+    def set(self, name, value):
+        if name not in self.settings:
+            raise AttributeError("No configuration setting for: %s" % name)
+        self.settings[name].set(value)
+
+    def get_cmd_args_from_env(self):
+        if 'GUNICORN_CMD_ARGS' in self.env_orig:
+            return shlex.split(self.env_orig['GUNICORN_CMD_ARGS'])
+        return []
+
+    def parser(self):
+        kwargs = {
+            "usage": self.usage,
+            "prog": self.prog
+        }
+        parser = argparse.ArgumentParser(**kwargs)
+        parser.add_argument("-v", "--version",
+                            action="version", default=argparse.SUPPRESS,
+                            version="%(prog)s (version " + __version__ + ")\n",
+                            help="show program's version number and exit")
+        parser.add_argument("args", nargs="*", help=argparse.SUPPRESS)
+
+        keys = sorted(self.settings, key=self.settings.__getitem__)
+        for k in keys:
+            self.settings[k].add_option(parser)
+
+        return parser
+
+    @property
+    def worker_class_str(self):
+        uri = self.settings['worker_class'].get()
+
+        # are we using a threaded worker?
+        is_sync = uri.endswith('SyncWorker') or uri == 'sync'
+        if is_sync and self.threads > 1:
+            return "gthread"
+        return uri
+
+    @property
+    def worker_class(self):
+        uri = self.settings['worker_class'].get()
+
+        # are we using a threaded worker?
+        is_sync = uri.endswith('SyncWorker') or uri == 'sync'
+        if is_sync and self.threads > 1:
+            uri = "gunicorn.workers.gthread.ThreadWorker"
+
+        worker_class = util.load_class(uri)
+        if hasattr(worker_class, "setup"):
+            worker_class.setup()
+        return worker_class
+
+    @property
+    def address(self):
+        s = self.settings['bind'].get()
+        return [util.parse_address(util.bytes_to_str(bind)) for bind in s]
+
+    @property
+    def uid(self):
+        return self.settings['user'].get()
+
+    @property
+    def gid(self):
+        return self.settings['group'].get()
+
+    @property
+    def proc_name(self):
+        pn = self.settings['proc_name'].get()
+        if pn is not None:
+            return pn
+        else:
+            return self.settings['default_proc_name'].get()
+
+    @property
+    def logger_class(self):
+        uri = self.settings['logger_class'].get()
+        if uri == "simple":
+            # support the default
+            uri = LoggerClass.default
+
+        # if default logger is in use, and statsd is on, automagically switch
+        # to the statsd logger
+        if uri == LoggerClass.default:
+            if 'statsd_host' in self.settings and self.settings['statsd_host'].value is not None:
+                uri = "gunicorn.instrument.statsd.Statsd"
+
+        logger_class = util.load_class(
+            uri,
+            default="gunicorn.glogging.Logger",
+            section="gunicorn.loggers")
+
+        if hasattr(logger_class, "install"):
+            logger_class.install()
+        return logger_class
+
+    @property
+    def is_ssl(self):
+        return self.certfile or self.keyfile
+
+    @property
+    def ssl_options(self):
+        opts = {}
+        for name, value in self.settings.items():
+            if value.section == 'SSL':
+                opts[name] = value.get()
+        return opts
+
+    @property
+    def env(self):
+        raw_env = self.settings['raw_env'].get()
+        env = {}
+
+        if not raw_env:
+            return env
+
+        for e in raw_env:
+            s = util.bytes_to_str(e)
+            try:
+                k, v = s.split('=', 1)
+            except ValueError:
+                raise RuntimeError("environment setting %r invalid" % s)
+
+            env[k] = v
+
+        return env
+
+    @property
+    def sendfile(self):
+        if self.settings['sendfile'].get() is not None:
+            return False
+
+        if 'SENDFILE' in os.environ:
+            sendfile = os.environ['SENDFILE'].lower()
+            return sendfile in ['y', '1', 'yes', 'true']
+
+        return True
+
+    @property
+    def reuse_port(self):
+        return self.settings['reuse_port'].get()
+
+    @property
+    def paste_global_conf(self):
+        raw_global_conf = self.settings['raw_paste_global_conf'].get()
+        if raw_global_conf is None:
+            return None
+
+        global_conf = {}
+        for e in raw_global_conf:
+            s = util.bytes_to_str(e)
+            try:
+                k, v = re.split(r'(?<!\\)=', s, 1)
+            except ValueError:
+                raise RuntimeError("environment setting %r invalid" % s)
+            k = k.replace('\\=', '=')
+            v = v.replace('\\=', '=')
+            global_conf[k] = v
+
+        return global_conf
+
+
+class SettingMeta(type):
+    def __new__(cls, name, bases, attrs):
+        super_new = super().__new__
+        parents = [b for b in bases if isinstance(b, SettingMeta)]
+        if not parents:
+            return super_new(cls, name, bases, attrs)
+
+        attrs["order"] = len(KNOWN_SETTINGS)
+        attrs["validator"] = staticmethod(attrs["validator"])
+
+        new_class = super_new(cls, name, bases, attrs)
+        new_class.fmt_desc(attrs.get("desc", ""))
+        KNOWN_SETTINGS.append(new_class)
+        return new_class
+
+    def fmt_desc(cls, desc):
+        desc = textwrap.dedent(desc).strip()
+        setattr(cls, "desc", desc)
+        setattr(cls, "short", desc.splitlines()[0])
+
+
+class Setting(object):
+    name = None
+    value = None
+    section = None
+    cli = None
+    validator = None
+    type = None
+    meta = None
+    action = None
+    default = None
+    short = None
+    desc = None
+    nargs = None
+    const = None
+
+    def __init__(self):
+        if self.default is not None:
+            self.set(self.default)
+
+    def add_option(self, parser):
+        if not self.cli:
+            return
+        args = tuple(self.cli)
+
+        help_txt = "%s [%s]" % (self.short, self.default)
+        help_txt = help_txt.replace("%", "%%")
+
+        kwargs = {
+            "dest": self.name,
+            "action": self.action or "store",
+            "type": self.type or str,
+            "default": None,
+            "help": help_txt
+        }
+
+        if self.meta is not None:
+            kwargs['metavar'] = self.meta
+
+        if kwargs["action"] != "store":
+            kwargs.pop("type")
+
+        if self.nargs is not None:
+            kwargs["nargs"] = self.nargs
+
+        if self.const is not None:
+            kwargs["const"] = self.const
+
+        parser.add_argument(*args, **kwargs)
+
+    def copy(self):
+        return copy.copy(self)
+
+    def get(self):
+        return self.value
+
+    def set(self, val):
+        if not callable(self.validator):
+            raise TypeError('Invalid validator: %s' % self.name)
+        self.value = self.validator(val)
+
+    def __lt__(self, other):
+        return (self.section == other.section and
+                self.order < other.order)
+    __cmp__ = __lt__
+
+    def __repr__(self):
+        return "<%s.%s object at %x with value %r>" % (
+            self.__class__.__module__,
+            self.__class__.__name__,
+            id(self),
+            self.value,
+        )
+
+
+Setting = SettingMeta('Setting', (Setting,), {})
+
+
+def validate_bool(val):
+    if val is None:
+        return
+
+    if isinstance(val, bool):
+        return val
+    if not isinstance(val, str):
+        raise TypeError("Invalid type for casting: %s" % val)
+    if val.lower().strip() == "true":
+        return True
+    elif val.lower().strip() == "false":
+        return False
+    else:
+        raise ValueError("Invalid boolean: %s" % val)
+
+
+def validate_dict(val):
+    if not isinstance(val, dict):
+        raise TypeError("Value is not a dictionary: %s " % val)
+    return val
+
+
+def validate_pos_int(val):
+    if not isinstance(val, int):
+        val = int(val, 0)
+    else:
+        # Booleans are ints!
+        val = int(val)
+    if val < 0:
+        raise ValueError("Value must be positive: %s" % val)
+    return val
+
+
+def validate_ssl_version(val):
+    ssl_versions = {}
+    for protocol in [p for p in dir(ssl) if p.startswith("PROTOCOL_")]:
+        ssl_versions[protocol[9:]] = getattr(ssl, protocol)
+    if val in ssl_versions:
+        # string matching PROTOCOL_...
+        return ssl_versions[val]
+
+    try:
+        intval = validate_pos_int(val)
+        if intval in ssl_versions.values():
+            # positive int matching a protocol int constant
+            return intval
+    except (ValueError, TypeError):
+        # negative integer or not an integer
+        # drop this in favour of the more descriptive ValueError below
+        pass
+
+    raise ValueError("Invalid ssl_version: %s. Valid options: %s"
+                     % (val, ', '.join(ssl_versions)))
+
+
+def validate_string(val):
+    if val is None:
+        return None
+    if not isinstance(val, str):
+        raise TypeError("Not a string: %s" % val)
+    return val.strip()
+
+
+def validate_file_exists(val):
+    if val is None:
+        return None
+    if not os.path.exists(val):
+        raise ValueError("File %s does not exists." % val)
+    return val
+
+
+def validate_list_string(val):
+    if not val:
+        return []
+
+    # legacy syntax
+    if isinstance(val, str):
+        val = [val]
+
+    return [validate_string(v) for v in val]
+
+
+def validate_list_of_existing_files(val):
+    return [validate_file_exists(v) for v in validate_list_string(val)]
+
+
+def validate_string_to_list(val):
+    val = validate_string(val)
+
+    if not val:
+        return []
+
+    return [v.strip() for v in val.split(",") if v]
+
+
+def validate_class(val):
+    if inspect.isfunction(val) or inspect.ismethod(val):
+        val = val()
+    if inspect.isclass(val):
+        return val
+    return validate_string(val)
+
+
+def validate_callable(arity):
+    def _validate_callable(val):
+        if isinstance(val, str):
+            try:
+                mod_name, obj_name = val.rsplit(".", 1)
+            except ValueError:
+                raise TypeError("Value '%s' is not import string. "
+                                "Format: module[.submodules...].object" % val)
+            try:
+                mod = __import__(mod_name, fromlist=[obj_name])
+                val = getattr(mod, obj_name)
+            except ImportError as e:
+                raise TypeError(str(e))
+            except AttributeError:
+                raise TypeError("Can not load '%s' from '%s'"
+                    "" % (obj_name, mod_name))
+        if not callable(val):
+            raise TypeError("Value is not callable: %s" % val)
+        if arity != -1 and arity != util.get_arity(val):
+            raise TypeError("Value must have an arity of: %s" % arity)
+        return val
+    return _validate_callable
+
+
+def validate_user(val):
+    if val is None:
+        return os.geteuid()
+    if isinstance(val, int):
+        return val
+    elif val.isdigit():
+        return int(val)
+    else:
+        try:
+            return pwd.getpwnam(val).pw_uid
+        except KeyError:
+            raise ConfigError("No such user: '%s'" % val)
+
+
+def validate_group(val):
+    if val is None:
+        return os.getegid()
+
+    if isinstance(val, int):
+        return val
+    elif val.isdigit():
+        return int(val)
+    else:
+        try:
+            return grp.getgrnam(val).gr_gid
+        except KeyError:
+            raise ConfigError("No such group: '%s'" % val)
+
+
+def validate_post_request(val):
+    val = validate_callable(-1)(val)
+
+    largs = util.get_arity(val)
+    if largs == 4:
+        return val
+    elif largs == 3:
+        return lambda worker, req, env, _r: val(worker, req, env)
+    elif largs == 2:
+        return lambda worker, req, _e, _r: val(worker, req)
+    else:
+        raise TypeError("Value must have an arity of: 4")
+
+
+def validate_chdir(val):
+    # valid if the value is a string
+    val = validate_string(val)
+
+    # transform relative paths
+    path = os.path.abspath(os.path.normpath(os.path.join(util.getcwd(), val)))
+
+    # test if the path exists
+    if not os.path.exists(path):
+        raise ConfigError("can't chdir to %r" % val)
+
+    return path
+
+
+def validate_hostport(val):
+    val = validate_string(val)
+    if val is None:
+        return None
+    elements = val.split(":")
+    if len(elements) == 2:
+        return (elements[0], int(elements[1]))
+    else:
+        raise TypeError("Value must consist of: hostname:port")
+
+
+def validate_reload_engine(val):
+    if val not in reloader_engines:
+        raise ConfigError("Invalid reload_engine: %r" % val)
+
+    return val
+
+
+def get_default_config_file():
+    config_path = os.path.join(os.path.abspath(os.getcwd()),
+                               'gunicorn.conf.py')
+    if os.path.exists(config_path):
+        return config_path
+    return None
+
+
+class ConfigFile(Setting):
+    name = "config"
+    section = "Config File"
+    cli = ["-c", "--config"]
+    meta = "CONFIG"
+    validator = validate_string
+    default = "./gunicorn.conf.py"
+    desc = """\
+        The Gunicorn config file.
+
+        A string of the form ``PATH``, ``file:PATH``, or ``python:MODULE_NAME``.
+
+        Only has an effect when specified on the command line or as part of an
+        application specific configuration.
+
+        By default, a file named ``gunicorn.conf.py`` will be read from the same
+        directory where gunicorn is being run.
+
+        .. versionchanged:: 19.4
+           Loading the config from a Python module requires the ``python:``
+           prefix.
+        """
+
+class WSGIApp(Setting):
+    name = "wsgi_app"
+    section = "Config File"
+    meta = "STRING"
+    validator = validate_string
+    default = None
+    desc = """\
+        A WSGI application path in pattern ``$(MODULE_NAME):$(VARIABLE_NAME)``.
+
+        .. versionadded:: 20.1.0
+        """
+
+class Bind(Setting):
+    name = "bind"
+    action = "append"
+    section = "Server Socket"
+    cli = ["-b", "--bind"]
+    meta = "ADDRESS"
+    validator = validate_list_string
+
+    if 'PORT' in os.environ:
+        default = ['0.0.0.0:{0}'.format(os.environ.get('PORT'))]
+    else:
+        default = ['127.0.0.1:8000']
+
+    desc = """\
+        The socket to bind.
+
+        A string of the form: ``HOST``, ``HOST:PORT``, ``unix:PATH``,
+        ``fd://FD``. An IP is a valid ``HOST``.
+
+        .. versionchanged:: 20.0
+           Support for ``fd://FD`` got added.
+
+        Multiple addresses can be bound. ex.::
+
+            $ gunicorn -b 127.0.0.1:8000 -b [::1]:8000 test:app
+
+        will bind the `test:app` application on localhost both on ipv6
+        and ipv4 interfaces.
+
+        If the ``PORT`` environment variable is defined, the default
+        is ``['0.0.0.0:$PORT']``. If it is not defined, the default
+        is ``['127.0.0.1:8000']``.
+        """
+
+
+class Backlog(Setting):
+    name = "backlog"
+    section = "Server Socket"
+    cli = ["--backlog"]
+    meta = "INT"
+    validator = validate_pos_int
+    type = int
+    default = 2048
+    desc = """\
+        The maximum number of pending connections.
+
+        This refers to the number of clients that can be waiting to be served.
+        Exceeding this number results in the client getting an error when
+        attempting to connect. It should only affect servers under significant
+        load.
+
+        Must be a positive integer. Generally set in the 64-2048 range.
+        """
+
+
+class Workers(Setting):
+    name = "workers"
+    section = "Worker Processes"
+    cli = ["-w", "--workers"]
+    meta = "INT"
+    validator = validate_pos_int
+    type = int
+    default = int(os.environ.get("WEB_CONCURRENCY", 1))
+    desc = """\
+        The number of worker processes for handling requests.
+
+        A positive integer generally in the ``2-4 x $(NUM_CORES)`` range.
+        You'll want to vary this a bit to find the best for your particular
+        application's work load.
+
+        By default, the value of the ``WEB_CONCURRENCY`` environment variable,
+        which is set by some Platform-as-a-Service providers such as Heroku. If
+        it is not defined, the default is ``1``.
+        """
+
+
+class WorkerClass(Setting):
+    name = "worker_class"
+    section = "Worker Processes"
+    cli = ["-k", "--worker-class"]
+    meta = "STRING"
+    validator = validate_class
+    default = "sync"
+    desc = """\
+        The type of workers to use.
+
+        The default class (``sync``) should handle most "normal" types of
+        workloads. You'll want to read :doc:`design` for information on when
+        you might want to choose one of the other worker classes. Required
+        libraries may be installed using setuptools' ``extras_require`` feature.
+
+        A string referring to one of the following bundled classes:
+
+        * ``sync``
+        * ``eventlet`` - Requires eventlet >= 0.24.1 (or install it via
+          ``pip install gunicorn[eventlet]``)
+        * ``gevent``   - Requires gevent >= 1.4 (or install it via
+          ``pip install gunicorn[gevent]``)
+        * ``tornado``  - Requires tornado >= 0.2 (or install it via
+          ``pip install gunicorn[tornado]``)
+        * ``gthread``  - Python 2 requires the futures package to be installed
+          (or install it via ``pip install gunicorn[gthread]``)
+
+        Optionally, you can provide your own worker by giving Gunicorn a
+        Python path to a subclass of ``gunicorn.workers.base.Worker``.
+        This alternative syntax will load the gevent class:
+        ``gunicorn.workers.ggevent.GeventWorker``.
+        """
+
+
+class WorkerThreads(Setting):
+    name = "threads"
+    section = "Worker Processes"
+    cli = ["--threads"]
+    meta = "INT"
+    validator = validate_pos_int
+    type = int
+    default = 1
+    desc = """\
+        The number of worker threads for handling requests.
+
+        Run each worker with the specified number of threads.
+
+        A positive integer generally in the ``2-4 x $(NUM_CORES)`` range.
+        You'll want to vary this a bit to find the best for your particular
+        application's work load.
+
+        If it is not defined, the default is ``1``.
+
+        This setting only affects the Gthread worker type.
+
+        .. note::
+           If you try to use the ``sync`` worker type and set the ``threads``
+           setting to more than 1, the ``gthread`` worker type will be used
+           instead.
+        """
+
+
+class WorkerConnections(Setting):
+    name = "worker_connections"
+    section = "Worker Processes"
+    cli = ["--worker-connections"]
+    meta = "INT"
+    validator = validate_pos_int
+    type = int
+    default = 1000
+    desc = """\
+        The maximum number of simultaneous clients.
+
+        This setting only affects the Eventlet and Gevent worker types.
+        """
+
+
+class MaxRequests(Setting):
+    name = "max_requests"
+    section = "Worker Processes"
+    cli = ["--max-requests"]
+    meta = "INT"
+    validator = validate_pos_int
+    type = int
+    default = 0
+    desc = """\
+        The maximum number of requests a worker will process before restarting.
+
+        Any value greater than zero will limit the number of requests a worker
+        will process before automatically restarting. This is a simple method
+        to help limit the damage of memory leaks.
+
+        If this is set to zero (the default) then the automatic worker
+        restarts are disabled.
+        """
+
+
+class MaxRequestsJitter(Setting):
+    name = "max_requests_jitter"
+    section = "Worker Processes"
+    cli = ["--max-requests-jitter"]
+    meta = "INT"
+    validator = validate_pos_int
+    type = int
+    default = 0
+    desc = """\
+        The maximum jitter to add to the *max_requests* setting.
+
+        The jitter causes the restart per worker to be randomized by
+        ``randint(0, max_requests_jitter)``. This is intended to stagger worker
+        restarts to avoid all workers restarting at the same time.
+
+        .. versionadded:: 19.2
+        """
+
+
+class Timeout(Setting):
+    name = "timeout"
+    section = "Worker Processes"
+    cli = ["-t", "--timeout"]
+    meta = "INT"
+    validator = validate_pos_int
+    type = int
+    default = 30
+    desc = """\
+        Workers silent for more than this many seconds are killed and restarted.
+
+        Value is a positive number or 0. Setting it to 0 has the effect of
+        infinite timeouts by disabling timeouts for all workers entirely.
+
+        Generally, the default of thirty seconds should suffice. Only set this
+        noticeably higher if you're sure of the repercussions for sync workers.
+        For the non sync workers it just means that the worker process is still
+        communicating and is not tied to the length of time required to handle a
+        single request.
+        """
+
+
+class GracefulTimeout(Setting):
+    name = "graceful_timeout"
+    section = "Worker Processes"
+    cli = ["--graceful-timeout"]
+    meta = "INT"
+    validator = validate_pos_int
+    type = int
+    default = 30
+    desc = """\
+        Timeout for graceful workers restart.
+
+        After receiving a restart signal, workers have this much time to finish
+        serving requests. Workers still alive after the timeout (starting from
+        the receipt of the restart signal) are force killed.
+        """
+
+
+class Keepalive(Setting):
+    name = "keepalive"
+    section = "Worker Processes"
+    cli = ["--keep-alive"]
+    meta = "INT"
+    validator = validate_pos_int
+    type = int
+    default = 2
+    desc = """\
+        The number of seconds to wait for requests on a Keep-Alive connection.
+
+        Generally set in the 1-5 seconds range for servers with direct connection
+        to the client (e.g. when you don't have separate load balancer). When
+        Gunicorn is deployed behind a load balancer, it often makes sense to
+        set this to a higher value.
+
+        .. note::
+           ``sync`` worker does not support persistent connections and will
+           ignore this option.
+        """
+
+
+class LimitRequestLine(Setting):
+    name = "limit_request_line"
+    section = "Security"
+    cli = ["--limit-request-line"]
+    meta = "INT"
+    validator = validate_pos_int
+    type = int
+    default = 4094
+    desc = """\
+        The maximum size of HTTP request line in bytes.
+
+        This parameter is used to limit the allowed size of a client's
+        HTTP request-line. Since the request-line consists of the HTTP
+        method, URI, and protocol version, this directive places a
+        restriction on the length of a request-URI allowed for a request
+        on the server. A server needs this value to be large enough to
+        hold any of its resource names, including any information that
+        might be passed in the query part of a GET request. Value is a number
+        from 0 (unlimited) to 8190.
+
+        This parameter can be used to prevent any DDOS attack.
+        """
+
+
+class LimitRequestFields(Setting):
+    name = "limit_request_fields"
+    section = "Security"
+    cli = ["--limit-request-fields"]
+    meta = "INT"
+    validator = validate_pos_int
+    type = int
+    default = 100
+    desc = """\
+        Limit the number of HTTP headers fields in a request.
+
+        This parameter is used to limit the number of headers in a request to
+        prevent DDOS attack. Used with the *limit_request_field_size* it allows
+        more safety. By default this value is 100 and can't be larger than
+        32768.
+        """
+
+
+class LimitRequestFieldSize(Setting):
+    name = "limit_request_field_size"
+    section = "Security"
+    cli = ["--limit-request-field_size"]
+    meta = "INT"
+    validator = validate_pos_int
+    type = int
+    default = 8190
+    desc = """\
+        Limit the allowed size of an HTTP request header field.
+
+        Value is a positive number or 0. Setting it to 0 will allow unlimited
+        header field sizes.
+
+        .. warning::
+           Setting this parameter to a very high or unlimited value can open
+           up for DDOS attacks.
+        """
+
+
+class Reload(Setting):
+    name = "reload"
+    section = 'Debugging'
+    cli = ['--reload']
+    validator = validate_bool
+    action = 'store_true'
+    default = False
+
+    desc = '''\
+        Restart workers when code changes.
+
+        This setting is intended for development. It will cause workers to be
+        restarted whenever application code changes.
+
+        The reloader is incompatible with application preloading. When using a
+        paste configuration be sure that the server block does not import any
+        application code or the reload will not work as designed.
+
+        The default behavior is to attempt inotify with a fallback to file
+        system polling. Generally, inotify should be preferred if available
+        because it consumes less system resources.
+
+        .. note::
+           In order to use the inotify reloader, you must have the ``inotify``
+           package installed.
+        '''
+
+
+class ReloadEngine(Setting):
+    name = "reload_engine"
+    section = "Debugging"
+    cli = ["--reload-engine"]
+    meta = "STRING"
+    validator = validate_reload_engine
+    default = "auto"
+    desc = """\
+        The implementation that should be used to power :ref:`reload`.
+
+        Valid engines are:
+
+        * ``'auto'``
+        * ``'poll'``
+        * ``'inotify'`` (requires inotify)
+
+        .. versionadded:: 19.7
+        """
+
+
+class ReloadExtraFiles(Setting):
+    name = "reload_extra_files"
+    action = "append"
+    section = "Debugging"
+    cli = ["--reload-extra-file"]
+    meta = "FILES"
+    validator = validate_list_of_existing_files
+    default = []
+    desc = """\
+        Extends :ref:`reload` option to also watch and reload on additional files
+        (e.g., templates, configurations, specifications, etc.).
+
+        .. versionadded:: 19.8
+        """
+
+
+class Spew(Setting):
+    name = "spew"
+    section = "Debugging"
+    cli = ["--spew"]
+    validator = validate_bool
+    action = "store_true"
+    default = False
+    desc = """\
+        Install a trace function that spews every line executed by the server.
+
+        This is the nuclear option.
+        """
+
+
+class ConfigCheck(Setting):
+    name = "check_config"
+    section = "Debugging"
+    cli = ["--check-config"]
+    validator = validate_bool
+    action = "store_true"
+    default = False
+    desc = """\
+        Check the configuration and exit. The exit status is 0 if the
+        configuration is correct, and 1 if the configuration is incorrect.
+        """
+
+
+class PrintConfig(Setting):
+    name = "print_config"
+    section = "Debugging"
+    cli = ["--print-config"]
+    validator = validate_bool
+    action = "store_true"
+    default = False
+    desc = """\
+        Print the configuration settings as fully resolved. Implies :ref:`check-config`.
+        """
+
+
+class PreloadApp(Setting):
+    name = "preload_app"
+    section = "Server Mechanics"
+    cli = ["--preload"]
+    validator = validate_bool
+    action = "store_true"
+    default = False
+    desc = """\
+        Load application code before the worker processes are forked.
+
+        By preloading an application you can save some RAM resources as well as
+        speed up server boot times. Although, if you defer application loading
+        to each worker process, you can reload your application code easily by
+        restarting workers.
+        """
+
+
+class Sendfile(Setting):
+    name = "sendfile"
+    section = "Server Mechanics"
+    cli = ["--no-sendfile"]
+    validator = validate_bool
+    action = "store_const"
+    const = False
+
+    desc = """\
+        Disables the use of ``sendfile()``.
+
+        If not set, the value of the ``SENDFILE`` environment variable is used
+        to enable or disable its usage.
+
+        .. versionadded:: 19.2
+        .. versionchanged:: 19.4
+           Swapped ``--sendfile`` with ``--no-sendfile`` to actually allow
+           disabling.
+        .. versionchanged:: 19.6
+           added support for the ``SENDFILE`` environment variable
+        """
+
+
+class ReusePort(Setting):
+    name = "reuse_port"
+    section = "Server Mechanics"
+    cli = ["--reuse-port"]
+    validator = validate_bool
+    action = "store_true"
+    default = False
+
+    desc = """\
+        Set the ``SO_REUSEPORT`` flag on the listening socket.
+
+        .. versionadded:: 19.8
+        """
+
+
+class Chdir(Setting):
+    name = "chdir"
+    section = "Server Mechanics"
+    cli = ["--chdir"]
+    validator = validate_chdir
+    default = util.getcwd()
+    desc = """\
+        Change directory to specified directory before loading apps.
+        """
+
+
+class Daemon(Setting):
+    name = "daemon"
+    section = "Server Mechanics"
+    cli = ["-D", "--daemon"]
+    validator = validate_bool
+    action = "store_true"
+    default = False
+    desc = """\
+        Daemonize the Gunicorn process.
+
+        Detaches the server from the controlling terminal and enters the
+        background.
+        """
+
+
+class Env(Setting):
+    name = "raw_env"
+    action = "append"
+    section = "Server Mechanics"
+    cli = ["-e", "--env"]
+    meta = "ENV"
+    validator = validate_list_string
+    default = []
+
+    desc = """\
+        Set environment variables in the execution environment.
+
+        Should be a list of strings in the ``key=value`` format.
+
+        For example on the command line:
+
+        .. code-block:: console
+
+            $ gunicorn -b 127.0.0.1:8000 --env FOO=1 test:app
+
+        Or in the configuration file:
+
+        .. code-block:: python
+
+            raw_env = ["FOO=1"]
+        """
+
+
+class Pidfile(Setting):
+    name = "pidfile"
+    section = "Server Mechanics"
+    cli = ["-p", "--pid"]
+    meta = "FILE"
+    validator = validate_string
+    default = None
+    desc = """\
+        A filename to use for the PID file.
+
+        If not set, no PID file will be written.
+        """
+
+
+class WorkerTmpDir(Setting):
+    name = "worker_tmp_dir"
+    section = "Server Mechanics"
+    cli = ["--worker-tmp-dir"]
+    meta = "DIR"
+    validator = validate_string
+    default = None
+    desc = """\
+        A directory to use for the worker heartbeat temporary file.
+
+        If not set, the default temporary directory will be used.
+
+        .. note::
+           The current heartbeat system involves calling ``os.fchmod`` on
+           temporary file handlers and may block a worker for arbitrary time
+           if the directory is on a disk-backed filesystem.
+
+           See :ref:`blocking-os-fchmod` for more detailed information
+           and a solution for avoiding this problem.
+        """
+
+
+class User(Setting):
+    name = "user"
+    section = "Server Mechanics"
+    cli = ["-u", "--user"]
+    meta = "USER"
+    validator = validate_user
+    default = os.geteuid()
+    desc = """\
+        Switch worker processes to run as this user.
+
+        A valid user id (as an integer) or the name of a user that can be
+        retrieved with a call to ``pwd.getpwnam(value)`` or ``None`` to not
+        change the worker process user.
+        """
+
+
+class Group(Setting):
+    name = "group"
+    section = "Server Mechanics"
+    cli = ["-g", "--group"]
+    meta = "GROUP"
+    validator = validate_group
+    default = os.getegid()
+    desc = """\
+        Switch worker process to run as this group.
+
+        A valid group id (as an integer) or the name of a user that can be
+        retrieved with a call to ``pwd.getgrnam(value)`` or ``None`` to not
+        change the worker processes group.
+        """
+
+
+class Umask(Setting):
+    name = "umask"
+    section = "Server Mechanics"
+    cli = ["-m", "--umask"]
+    meta = "INT"
+    validator = validate_pos_int
+    type = auto_int
+    default = 0
+    desc = """\
+        A bit mask for the file mode on files written by Gunicorn.
+
+        Note that this affects unix socket permissions.
+
+        A valid value for the ``os.umask(mode)`` call or a string compatible
+        with ``int(value, 0)`` (``0`` means Python guesses the base, so values
+        like ``0``, ``0xFF``, ``0022`` are valid for decimal, hex, and octal
+        representations)
+        """
+
+
+class Initgroups(Setting):
+    name = "initgroups"
+    section = "Server Mechanics"
+    cli = ["--initgroups"]
+    validator = validate_bool
+    action = 'store_true'
+    default = False
+
+    desc = """\
+        If true, set the worker process's group access list with all of the
+        groups of which the specified username is a member, plus the specified
+        group id.
+
+        .. versionadded:: 19.7
+        """
+
+
+class TmpUploadDir(Setting):
+    name = "tmp_upload_dir"
+    section = "Server Mechanics"
+    meta = "DIR"
+    validator = validate_string
+    default = None
+    desc = """\
+        Directory to store temporary request data as they are read.
+
+        This may disappear in the near future.
+
+        This path should be writable by the process permissions set for Gunicorn
+        workers. If not specified, Gunicorn will choose a system generated
+        temporary directory.
+        """
+
+
+class SecureSchemeHeader(Setting):
+    name = "secure_scheme_headers"
+    section = "Server Mechanics"
+    validator = validate_dict
+    default = {
+        "X-FORWARDED-PROTOCOL": "ssl",
+        "X-FORWARDED-PROTO": "https",
+        "X-FORWARDED-SSL": "on"
+    }
+    desc = """\
+
+        A dictionary containing headers and values that the front-end proxy
+        uses to indicate HTTPS requests. If the source IP is permitted by
+        ``forwarded-allow-ips`` (below), *and* at least one request header matches
+        a key-value pair listed in this dictionary, then Gunicorn will set
+        ``wsgi.url_scheme`` to ``https``, so your application can tell that the
+        request is secure.
+
+        If the other headers listed in this dictionary are not present in the request, they will be ignored,
+        but if the other headers are present and do not match the provided values, then
+        the request will fail to parse. See the note below for more detailed examples of this behaviour.
+
+        The dictionary should map upper-case header names to exact string
+        values. The value comparisons are case-sensitive, unlike the header
+        names, so make sure they're exactly what your front-end proxy sends
+        when handling HTTPS requests.
+
+        It is important that your front-end proxy configuration ensures that
+        the headers defined here can not be passed directly from the client.
+        """
+
+
+class ForwardedAllowIPS(Setting):
+    name = "forwarded_allow_ips"
+    section = "Server Mechanics"
+    cli = ["--forwarded-allow-ips"]
+    meta = "STRING"
+    validator = validate_string_to_list
+    default = os.environ.get("FORWARDED_ALLOW_IPS", "127.0.0.1")
+    desc = """\
+        Front-end's IPs from which allowed to handle set secure headers.
+        (comma separate).
+
+        Set to ``*`` to disable checking of Front-end IPs (useful for setups
+        where you don't know in advance the IP address of Front-end, but
+        you still trust the environment).
+
+        By default, the value of the ``FORWARDED_ALLOW_IPS`` environment
+        variable. If it is not defined, the default is ``"127.0.0.1"``.
+        
+        .. note::
+        
+            The interplay between the request headers, the value of ``forwarded_allow_ips``, and the value of
+            ``secure_scheme_headers`` is complex. Various scenarios are documented below to further elaborate. In each case, we 
+            have a request from the remote address 134.213.44.18, and the default value of ``secure_scheme_headers``:
+            
+            .. code::
+            
+                secure_scheme_headers = {
+                    'X-FORWARDED-PROTOCOL': 'ssl',
+                    'X-FORWARDED-PROTO': 'https',
+                    'X-FORWARDED-SSL': 'on'
+                }
+            
+        
+            .. list-table:: 
+                :header-rows: 1
+                :align: center
+                :widths: auto
+                
+                * - ``forwarded-allow-ips``
+                  - Secure Request Headers
+                  - Result
+                  - Explanation
+                * - .. code:: 
+                    
+                        ["127.0.0.1"]
+                  - .. code::
+                  
+                        X-Forwarded-Proto: https
+                  - .. code:: 
+                    
+                        wsgi.url_scheme = "http"
+                  - IP address was not allowed
+                * - .. code:: 
+                    
+                        "*"
+                  - <none>
+                  - .. code:: 
+                    
+                        wsgi.url_scheme = "http"
+                  - IP address allowed, but no secure headers provided
+                * - .. code:: 
+                    
+                        "*"
+                  - .. code::
+                  
+                        X-Forwarded-Proto: https
+                  - .. code:: 
+                    
+                        wsgi.url_scheme = "https"
+                  - IP address allowed, one request header matched
+                * - .. code:: 
+                    
+                        ["134.213.44.18"]
+                  - .. code::
+                  
+                        X-Forwarded-Ssl: on
+                        X-Forwarded-Proto: http
+                  - ``InvalidSchemeHeaders()`` raised
+                  - IP address allowed, but the two secure headers disagreed on if HTTPS was used
+                
+
+        """
+
+
+class AccessLog(Setting):
+    name = "accesslog"
+    section = "Logging"
+    cli = ["--access-logfile"]
+    meta = "FILE"
+    validator = validate_string
+    default = None
+    desc = """\
+        The Access log file to write to.
+
+        ``'-'`` means log to stdout.
+        """
+
+
+class DisableRedirectAccessToSyslog(Setting):
+    name = "disable_redirect_access_to_syslog"
+    section = "Logging"
+    cli = ["--disable-redirect-access-to-syslog"]
+    validator = validate_bool
+    action = 'store_true'
+    default = False
+    desc = """\
+    Disable redirect access logs to syslog.
+
+    .. versionadded:: 19.8
+    """
+
+
+class AccessLogFormat(Setting):
+    name = "access_log_format"
+    section = "Logging"
+    cli = ["--access-logformat"]
+    meta = "STRING"
+    validator = validate_string
+    default = '%(h)s %(l)s %(u)s %(t)s "%(r)s" %(s)s %(b)s "%(f)s" "%(a)s"'
+    desc = """\
+        The access log format.
+
+        ===========  ===========
+        Identifier   Description
+        ===========  ===========
+        h            remote address
+        l            ``'-'``
+        u            user name
+        t            date of the request
+        r            status line (e.g. ``GET / HTTP/1.1``)
+        m            request method
+        U            URL path without query string
+        q            query string
+        H            protocol
+        s            status
+        B            response length
+        b            response length or ``'-'`` (CLF format)
+        f            referer
+        a            user agent
+        T            request time in seconds
+        M            request time in milliseconds
+        D            request time in microseconds
+        L            request time in decimal seconds
+        p            process ID
+        {header}i    request header
+        {header}o    response header
+        {variable}e  environment variable
+        ===========  ===========
+
+        Use lowercase for header and environment variable names, and put
+        ``{...}x`` names inside ``%(...)s``. For example::
+
+            %({x-forwarded-for}i)s
+        """
+
+
+class ErrorLog(Setting):
+    name = "errorlog"
+    section = "Logging"
+    cli = ["--error-logfile", "--log-file"]
+    meta = "FILE"
+    validator = validate_string
+    default = '-'
+    desc = """\
+        The Error log file to write to.
+
+        Using ``'-'`` for FILE makes gunicorn log to stderr.
+
+        .. versionchanged:: 19.2
+           Log to stderr by default.
+
+        """
+
+
+class Loglevel(Setting):
+    name = "loglevel"
+    section = "Logging"
+    cli = ["--log-level"]
+    meta = "LEVEL"
+    validator = validate_string
+    default = "info"
+    desc = """\
+        The granularity of Error log outputs.
+
+        Valid level names are:
+
+        * ``'debug'``
+        * ``'info'``
+        * ``'warning'``
+        * ``'error'``
+        * ``'critical'``
+        """
+
+
+class CaptureOutput(Setting):
+    name = "capture_output"
+    section = "Logging"
+    cli = ["--capture-output"]
+    validator = validate_bool
+    action = 'store_true'
+    default = False
+    desc = """\
+        Redirect stdout/stderr to specified file in :ref:`errorlog`.
+
+        .. versionadded:: 19.6
+        """
+
+
+class LoggerClass(Setting):
+    name = "logger_class"
+    section = "Logging"
+    cli = ["--logger-class"]
+    meta = "STRING"
+    validator = validate_class
+    default = "gunicorn.glogging.Logger"
+    desc = """\
+        The logger you want to use to log events in Gunicorn.
+
+        The default class (``gunicorn.glogging.Logger``) handles most
+        normal usages in logging. It provides error and access logging.
+
+        You can provide your own logger by giving Gunicorn a Python path to a
+        class that quacks like ``gunicorn.glogging.Logger``.
+        """
+
+
+class LogConfig(Setting):
+    name = "logconfig"
+    section = "Logging"
+    cli = ["--log-config"]
+    meta = "FILE"
+    validator = validate_string
+    default = None
+    desc = """\
+    The log config file to use.
+    Gunicorn uses the standard Python logging module's Configuration
+    file format.
+    """
+
+
+class LogConfigDict(Setting):
+    name = "logconfig_dict"
+    section = "Logging"
+    validator = validate_dict
+    default = {}
+    desc = """\
+    The log config dictionary to use, using the standard Python
+    logging module's dictionary configuration format. This option
+    takes precedence over the :ref:`logconfig` option, which uses the
+    older file configuration format.
+
+    Format: https://docs.python.org/3/library/logging.config.html#logging.config.dictConfig
+
+    .. versionadded:: 19.8
+    """
+
+
+class SyslogTo(Setting):
+    name = "syslog_addr"
+    section = "Logging"
+    cli = ["--log-syslog-to"]
+    meta = "SYSLOG_ADDR"
+    validator = validate_string
+
+    if PLATFORM == "darwin":
+        default = "unix:///var/run/syslog"
+    elif PLATFORM in ('freebsd', 'dragonfly', ):
+        default = "unix:///var/run/log"
+    elif PLATFORM == "openbsd":
+        default = "unix:///dev/log"
+    else:
+        default = "udp://localhost:514"
+
+    desc = """\
+    Address to send syslog messages.
+
+    Address is a string of the form:
+
+    * ``unix://PATH#TYPE`` : for unix domain socket. ``TYPE`` can be ``stream``
+      for the stream driver or ``dgram`` for the dgram driver.
+      ``stream`` is the default.
+    * ``udp://HOST:PORT`` : for UDP sockets
+    * ``tcp://HOST:PORT`` : for TCP sockets
+
+    """
+
+
+class Syslog(Setting):
+    name = "syslog"
+    section = "Logging"
+    cli = ["--log-syslog"]
+    validator = validate_bool
+    action = 'store_true'
+    default = False
+    desc = """\
+    Send *Gunicorn* logs to syslog.
+
+    .. versionchanged:: 19.8
+       You can now disable sending access logs by using the
+       :ref:`disable-redirect-access-to-syslog` setting.
+    """
+
+
+class SyslogPrefix(Setting):
+    name = "syslog_prefix"
+    section = "Logging"
+    cli = ["--log-syslog-prefix"]
+    meta = "SYSLOG_PREFIX"
+    validator = validate_string
+    default = None
+    desc = """\
+    Makes Gunicorn use the parameter as program-name in the syslog entries.
+
+    All entries will be prefixed by ``gunicorn.<prefix>``. By default the
+    program name is the name of the process.
+    """
+
+
+class SyslogFacility(Setting):
+    name = "syslog_facility"
+    section = "Logging"
+    cli = ["--log-syslog-facility"]
+    meta = "SYSLOG_FACILITY"
+    validator = validate_string
+    default = "user"
+    desc = """\
+    Syslog facility name
+    """
+
+
+class EnableStdioInheritance(Setting):
+    name = "enable_stdio_inheritance"
+    section = "Logging"
+    cli = ["-R", "--enable-stdio-inheritance"]
+    validator = validate_bool
+    default = False
+    action = "store_true"
+    desc = """\
+    Enable stdio inheritance.
+
+    Enable inheritance for stdio file descriptors in daemon mode.
+
+    Note: To disable the Python stdout buffering, you can to set the user
+    environment variable ``PYTHONUNBUFFERED`` .
+    """
+
+
+# statsD monitoring
+class StatsdHost(Setting):
+    name = "statsd_host"
+    section = "Logging"
+    cli = ["--statsd-host"]
+    meta = "STATSD_ADDR"
+    default = None
+    validator = validate_hostport
+    desc = """\
+    ``host:port`` of the statsd server to log to.
+
+    .. versionadded:: 19.1
+    """
+
+# Datadog Statsd (dogstatsd) tags. https://docs.datadoghq.com/developers/dogstatsd/
+class DogstatsdTags(Setting):
+    name = "dogstatsd_tags"
+    section = "Logging"
+    cli = ["--dogstatsd-tags"]
+    meta = "DOGSTATSD_TAGS"
+    default = ""
+    validator = validate_string
+    desc = """\
+    A comma-delimited list of datadog statsd (dogstatsd) tags to append to
+    statsd metrics.
+
+    .. versionadded:: 20
+    """
+
+class StatsdPrefix(Setting):
+    name = "statsd_prefix"
+    section = "Logging"
+    cli = ["--statsd-prefix"]
+    meta = "STATSD_PREFIX"
+    default = ""
+    validator = validate_string
+    desc = """\
+    Prefix to use when emitting statsd metrics (a trailing ``.`` is added,
+    if not provided).
+
+    .. versionadded:: 19.2
+    """
+
+
+class Procname(Setting):
+    name = "proc_name"
+    section = "Process Naming"
+    cli = ["-n", "--name"]
+    meta = "STRING"
+    validator = validate_string
+    default = None
+    desc = """\
+        A base to use with setproctitle for process naming.
+
+        This affects things like ``ps`` and ``top``. If you're going to be
+        running more than one instance of Gunicorn you'll probably want to set a
+        name to tell them apart. This requires that you install the setproctitle
+        module.
+
+        If not set, the *default_proc_name* setting will be used.
+        """
+
+
+class DefaultProcName(Setting):
+    name = "default_proc_name"
+    section = "Process Naming"
+    validator = validate_string
+    default = "gunicorn"
+    desc = """\
+        Internal setting that is adjusted for each type of application.
+        """
+
+
+class PythonPath(Setting):
+    name = "pythonpath"
+    section = "Server Mechanics"
+    cli = ["--pythonpath"]
+    meta = "STRING"
+    validator = validate_string
+    default = None
+    desc = """\
+        A comma-separated list of directories to add to the Python path.
+
+        e.g.
+        ``'/home/djangoprojects/myproject,/home/python/mylibrary'``.
+        """
+
+
+class Paste(Setting):
+    name = "paste"
+    section = "Server Mechanics"
+    cli = ["--paste", "--paster"]
+    meta = "STRING"
+    validator = validate_string
+    default = None
+    desc = """\
+        Load a PasteDeploy config file. The argument may contain a ``#``
+        symbol followed by the name of an app section from the config file,
+        e.g. ``production.ini#admin``.
+
+        At this time, using alternate server blocks is not supported. Use the
+        command line arguments to control server configuration instead.
+        """
+
+
+class OnStarting(Setting):
+    name = "on_starting"
+    section = "Server Hooks"
+    validator = validate_callable(1)
+    type = callable
+
+    def on_starting(server):
+        pass
+    default = staticmethod(on_starting)
+    desc = """\
+        Called just before the master process is initialized.
+
+        The callable needs to accept a single instance variable for the Arbiter.
+        """
+
+
+class OnReload(Setting):
+    name = "on_reload"
+    section = "Server Hooks"
+    validator = validate_callable(1)
+    type = callable
+
+    def on_reload(server):
+        pass
+    default = staticmethod(on_reload)
+    desc = """\
+        Called to recycle workers during a reload via SIGHUP.
+
+        The callable needs to accept a single instance variable for the Arbiter.
+        """
+
+
+class WhenReady(Setting):
+    name = "when_ready"
+    section = "Server Hooks"
+    validator = validate_callable(1)
+    type = callable
+
+    def when_ready(server):
+        pass
+    default = staticmethod(when_ready)
+    desc = """\
+        Called just after the server is started.
+
+        The callable needs to accept a single instance variable for the Arbiter.
+        """
+
+
+class Prefork(Setting):
+    name = "pre_fork"
+    section = "Server Hooks"
+    validator = validate_callable(2)
+    type = callable
+
+    def pre_fork(server, worker):
+        pass
+    default = staticmethod(pre_fork)
+    desc = """\
+        Called just before a worker is forked.
+
+        The callable needs to accept two instance variables for the Arbiter and
+        new Worker.
+        """
+
+
+class Postfork(Setting):
+    name = "post_fork"
+    section = "Server Hooks"
+    validator = validate_callable(2)
+    type = callable
+
+    def post_fork(server, worker):
+        pass
+    default = staticmethod(post_fork)
+    desc = """\
+        Called just after a worker has been forked.
+
+        The callable needs to accept two instance variables for the Arbiter and
+        new Worker.
+        """
+
+
+class PostWorkerInit(Setting):
+    name = "post_worker_init"
+    section = "Server Hooks"
+    validator = validate_callable(1)
+    type = callable
+
+    def post_worker_init(worker):
+        pass
+
+    default = staticmethod(post_worker_init)
+    desc = """\
+        Called just after a worker has initialized the application.
+
+        The callable needs to accept one instance variable for the initialized
+        Worker.
+        """
+
+
+class WorkerInt(Setting):
+    name = "worker_int"
+    section = "Server Hooks"
+    validator = validate_callable(1)
+    type = callable
+
+    def worker_int(worker):
+        pass
+
+    default = staticmethod(worker_int)
+    desc = """\
+        Called just after a worker exited on SIGINT or SIGQUIT.
+
+        The callable needs to accept one instance variable for the initialized
+        Worker.
+        """
+
+
+class WorkerAbort(Setting):
+    name = "worker_abort"
+    section = "Server Hooks"
+    validator = validate_callable(1)
+    type = callable
+
+    def worker_abort(worker):
+        pass
+
+    default = staticmethod(worker_abort)
+    desc = """\
+        Called when a worker received the SIGABRT signal.
+
+        This call generally happens on timeout.
+
+        The callable needs to accept one instance variable for the initialized
+        Worker.
+        """
+
+
+class PreExec(Setting):
+    name = "pre_exec"
+    section = "Server Hooks"
+    validator = validate_callable(1)
+    type = callable
+
+    def pre_exec(server):
+        pass
+    default = staticmethod(pre_exec)
+    desc = """\
+        Called just before a new master process is forked.
+
+        The callable needs to accept a single instance variable for the Arbiter.
+        """
+
+
+class PreRequest(Setting):
+    name = "pre_request"
+    section = "Server Hooks"
+    validator = validate_callable(2)
+    type = callable
+
+    def pre_request(worker, req):
+        worker.log.debug("%s %s" % (req.method, req.path))
+    default = staticmethod(pre_request)
+    desc = """\
+        Called just before a worker processes the request.
+
+        The callable needs to accept two instance variables for the Worker and
+        the Request.
+        """
+
+
+class PostRequest(Setting):
+    name = "post_request"
+    section = "Server Hooks"
+    validator = validate_post_request
+    type = callable
+
+    def post_request(worker, req, environ, resp):
+        pass
+    default = staticmethod(post_request)
+    desc = """\
+        Called after a worker processes the request.
+
+        The callable needs to accept two instance variables for the Worker and
+        the Request.
+        """
+
+
+class ChildExit(Setting):
+    name = "child_exit"
+    section = "Server Hooks"
+    validator = validate_callable(2)
+    type = callable
+
+    def child_exit(server, worker):
+        pass
+    default = staticmethod(child_exit)
+    desc = """\
+        Called just after a worker has been exited, in the master process.
+
+        The callable needs to accept two instance variables for the Arbiter and
+        the just-exited Worker.
+
+        .. versionadded:: 19.7
+        """
+
+
+class WorkerExit(Setting):
+    name = "worker_exit"
+    section = "Server Hooks"
+    validator = validate_callable(2)
+    type = callable
+
+    def worker_exit(server, worker):
+        pass
+    default = staticmethod(worker_exit)
+    desc = """\
+        Called just after a worker has been exited, in the worker process.
+
+        The callable needs to accept two instance variables for the Arbiter and
+        the just-exited Worker.
+        """
+
+
+class NumWorkersChanged(Setting):
+    name = "nworkers_changed"
+    section = "Server Hooks"
+    validator = validate_callable(3)
+    type = callable
+
+    def nworkers_changed(server, new_value, old_value):
+        pass
+    default = staticmethod(nworkers_changed)
+    desc = """\
+        Called just after *num_workers* has been changed.
+
+        The callable needs to accept an instance variable of the Arbiter and
+        two integers of number of workers after and before change.
+
+        If the number of workers is set for the first time, *old_value* would
+        be ``None``.
+        """
+
+
+class OnExit(Setting):
+    name = "on_exit"
+    section = "Server Hooks"
+    validator = validate_callable(1)
+
+    def on_exit(server):
+        pass
+
+    default = staticmethod(on_exit)
+    desc = """\
+        Called just before exiting Gunicorn.
+
+        The callable needs to accept a single instance variable for the Arbiter.
+        """
+
+
+class ProxyProtocol(Setting):
+    name = "proxy_protocol"
+    section = "Server Mechanics"
+    cli = ["--proxy-protocol"]
+    validator = validate_bool
+    default = False
+    action = "store_true"
+    desc = """\
+        Enable detect PROXY protocol (PROXY mode).
+
+        Allow using HTTP and Proxy together. It may be useful for work with
+        stunnel as HTTPS frontend and Gunicorn as HTTP server.
+
+        PROXY protocol: http://haproxy.1wt.eu/download/1.5/doc/proxy-protocol.txt
+
+        Example for stunnel config::
+
+            [https]
+            protocol = proxy
+            accept  = 443
+            connect = 80
+            cert = /etc/ssl/certs/stunnel.pem
+            key = /etc/ssl/certs/stunnel.key
+        """
+
+
+class ProxyAllowFrom(Setting):
+    name = "proxy_allow_ips"
+    section = "Server Mechanics"
+    cli = ["--proxy-allow-from"]
+    validator = validate_string_to_list
+    default = "127.0.0.1"
+    desc = """\
+        Front-end's IPs from which allowed accept proxy requests (comma separate).
+
+        Set to ``*`` to disable checking of Front-end IPs (useful for setups
+        where you don't know in advance the IP address of Front-end, but
+        you still trust the environment)
+        """
+
+
+class KeyFile(Setting):
+    name = "keyfile"
+    section = "SSL"
+    cli = ["--keyfile"]
+    meta = "FILE"
+    validator = validate_string
+    default = None
+    desc = """\
+    SSL key file
+    """
+
+
+class CertFile(Setting):
+    name = "certfile"
+    section = "SSL"
+    cli = ["--certfile"]
+    meta = "FILE"
+    validator = validate_string
+    default = None
+    desc = """\
+    SSL certificate file
+    """
+
+
+class SSLVersion(Setting):
+    name = "ssl_version"
+    section = "SSL"
+    cli = ["--ssl-version"]
+    validator = validate_ssl_version
+
+    if hasattr(ssl, "PROTOCOL_TLS"):
+        default = ssl.PROTOCOL_TLS
+    else:
+        default = ssl.PROTOCOL_SSLv23
+
+    desc = """\
+    SSL version to use (see stdlib ssl module's)
+
+    .. versionchanged:: 20.0.1
+       The default value has been changed from ``ssl.PROTOCOL_SSLv23`` to
+       ``ssl.PROTOCOL_TLS`` when Python >= 3.6 .
+
+    """
+    default = ssl.PROTOCOL_SSLv23
+    desc = """\
+    SSL version to use.
+
+    ============= ============
+    --ssl-version Description
+    ============= ============
+    SSLv3         SSLv3 is not-secure and is strongly discouraged.
+    SSLv23        Alias for TLS. Deprecated in Python 3.6, use TLS.
+    TLS           Negotiate highest possible version between client/server.
+                  Can yield SSL. (Python 3.6+)
+    TLSv1         TLS 1.0
+    TLSv1_1       TLS 1.1 (Python 3.4+)
+    TLSv1_2       TLS 1.2 (Python 3.4+)
+    TLS_SERVER    Auto-negotiate the highest protocol version like TLS,
+                  but only support server-side SSLSocket connections.
+                  (Python 3.6+)
+    ============= ============
+
+    .. versionchanged:: 19.7
+       The default value has been changed from ``ssl.PROTOCOL_TLSv1`` to
+       ``ssl.PROTOCOL_SSLv23``.
+    .. versionchanged:: 20.0
+       This setting now accepts string names based on ``ssl.PROTOCOL_``
+       constants.
+    """
+
+
+class CertReqs(Setting):
+    name = "cert_reqs"
+    section = "SSL"
+    cli = ["--cert-reqs"]
+    validator = validate_pos_int
+    default = ssl.CERT_NONE
+    desc = """\
+    Whether client certificate is required (see stdlib ssl module's)
+    """
+
+
+class CACerts(Setting):
+    name = "ca_certs"
+    section = "SSL"
+    cli = ["--ca-certs"]
+    meta = "FILE"
+    validator = validate_string
+    default = None
+    desc = """\
+    CA certificates file
+    """
+
+
+class SuppressRaggedEOFs(Setting):
+    name = "suppress_ragged_eofs"
+    section = "SSL"
+    cli = ["--suppress-ragged-eofs"]
+    action = "store_true"
+    default = True
+    validator = validate_bool
+    desc = """\
+    Suppress ragged EOFs (see stdlib ssl module's)
+    """
+
+
+class DoHandshakeOnConnect(Setting):
+    name = "do_handshake_on_connect"
+    section = "SSL"
+    cli = ["--do-handshake-on-connect"]
+    validator = validate_bool
+    action = "store_true"
+    default = False
+    desc = """\
+    Whether to perform SSL handshake on socket connect (see stdlib ssl module's)
+    """
+
+
+class Ciphers(Setting):
+    name = "ciphers"
+    section = "SSL"
+    cli = ["--ciphers"]
+    validator = validate_string
+    default = None
+    desc = """\
+    SSL Cipher suite to use, in the format of an OpenSSL cipher list.
+
+    By default we use the default cipher list from Python's ``ssl`` module,
+    which contains ciphers considered strong at the time of each Python
+    release.
+
+    As a recommended alternative, the Open Web App Security Project (OWASP)
+    offers `a vetted set of strong cipher strings rated A+ to C-
+    <https://www.owasp.org/index.php/TLS_Cipher_String_Cheat_Sheet>`_.
+    OWASP provides details on user-agent compatibility at each security level.
+
+    See the `OpenSSL Cipher List Format Documentation
+    <https://www.openssl.org/docs/manmaster/man1/ciphers.html#CIPHER-LIST-FORMAT>`_
+    for details on the format of an OpenSSL cipher list.
+    """
+
+
+class PasteGlobalConf(Setting):
+    name = "raw_paste_global_conf"
+    action = "append"
+    section = "Server Mechanics"
+    cli = ["--paste-global"]
+    meta = "CONF"
+    validator = validate_list_string
+    default = []
+
+    desc = """\
+        Set a PasteDeploy global config variable in ``key=value`` form.
+
+        The option can be specified multiple times.
+
+        The variables are passed to the the PasteDeploy entrypoint. Example::
+
+            $ gunicorn -b 127.0.0.1:8000 --paste development.ini --paste-global FOO=1 --paste-global BAR=2
+
+        .. versionadded:: 19.7
+        """
+
+
+class StripHeaderSpaces(Setting):
+    name = "strip_header_spaces"
+    section = "Server Mechanics"
+    cli = ["--strip-header-spaces"]
+    validator = validate_bool
+    action = "store_true"
+    default = False
+    desc = """\
+        Strip spaces present between the header name and the the ``:``.
+
+        This is known to induce vulnerabilities and is not compliant with the HTTP/1.1 standard.
+        See https://portswigger.net/research/http-desync-attacks-request-smuggling-reborn.
+
+        Use with care and only if necessary.
+        """
Index: venv/Lib/site-packages/django_heroku/__version__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/django_heroku/__version__.py b/venv/Lib/site-packages/django_heroku/__version__.py
new file mode 100644
--- /dev/null	(date 1630065394627)
+++ b/venv/Lib/site-packages/django_heroku/__version__.py	(date 1630065394627)
@@ -0,0 +1,1 @@
+__version__ = '0.3.1'
\ No newline at end of file
Index: venv/Lib/site-packages/gunicorn/debug.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/debug.py b/venv/Lib/site-packages/gunicorn/debug.py
new file mode 100644
--- /dev/null	(date 1630065626327)
+++ b/venv/Lib/site-packages/gunicorn/debug.py	(date 1630065626327)
@@ -0,0 +1,69 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+
+"""The debug module contains utilities and functions for better
+debugging Gunicorn."""
+
+import sys
+import linecache
+import re
+import inspect
+
+__all__ = ['spew', 'unspew']
+
+_token_spliter = re.compile(r'\W+')
+
+
+class Spew(object):
+
+    def __init__(self, trace_names=None, show_values=True):
+        self.trace_names = trace_names
+        self.show_values = show_values
+
+    def __call__(self, frame, event, arg):
+        if event == 'line':
+            lineno = frame.f_lineno
+            if '__file__' in frame.f_globals:
+                filename = frame.f_globals['__file__']
+                if (filename.endswith('.pyc') or
+                    filename.endswith('.pyo')):
+                    filename = filename[:-1]
+                name = frame.f_globals['__name__']
+                line = linecache.getline(filename, lineno)
+            else:
+                name = '[unknown]'
+                try:
+                    src = inspect.getsourcelines(frame)
+                    line = src[lineno]
+                except IOError:
+                    line = 'Unknown code named [%s].  VM instruction #%d' % (
+                        frame.f_code.co_name, frame.f_lasti)
+            if self.trace_names is None or name in self.trace_names:
+                print('%s:%s: %s' % (name, lineno, line.rstrip()))
+                if not self.show_values:
+                    return self
+                details = []
+                tokens = _token_spliter.split(line)
+                for tok in tokens:
+                    if tok in frame.f_globals:
+                        details.append('%s=%r' % (tok, frame.f_globals[tok]))
+                    if tok in frame.f_locals:
+                        details.append('%s=%r' % (tok, frame.f_locals[tok]))
+                if details:
+                    print("\t%s" % ' '.join(details))
+        return self
+
+
+def spew(trace_names=None, show_values=False):
+    """Install a trace hook which writes incredibly detailed logs
+    about what code is being executed to stdout.
+    """
+    sys.settrace(Spew(trace_names, show_values))
+
+
+def unspew():
+    """Remove the trace hook installed by spew.
+    """
+    sys.settrace(None)
Index: venv/Lib/site-packages/django_heroku/__init__.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/django_heroku/__init__.py b/venv/Lib/site-packages/django_heroku/__init__.py
new file mode 100644
--- /dev/null	(date 1630065394625)
+++ b/venv/Lib/site-packages/django_heroku/__init__.py	(date 1630065394625)
@@ -0,0 +1,1 @@
+from .core import *
\ No newline at end of file
Index: venv/Lib/site-packages/gunicorn/arbiter.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/gunicorn/arbiter.py b/venv/Lib/site-packages/gunicorn/arbiter.py
new file mode 100644
--- /dev/null	(date 1630065626319)
+++ b/venv/Lib/site-packages/gunicorn/arbiter.py	(date 1630065626319)
@@ -0,0 +1,652 @@
+# -*- coding: utf-8 -
+#
+# This file is part of gunicorn released under the MIT license.
+# See the NOTICE for more information.
+import errno
+import os
+import random
+import select
+import signal
+import sys
+import time
+import traceback
+
+from gunicorn.errors import HaltServer, AppImportError
+from gunicorn.pidfile import Pidfile
+from gunicorn import sock, systemd, util
+
+from gunicorn import __version__, SERVER_SOFTWARE
+
+
+class Arbiter(object):
+    """
+    Arbiter maintain the workers processes alive. It launches or
+    kills them if needed. It also manages application reloading
+    via SIGHUP/USR2.
+    """
+
+    # A flag indicating if a worker failed to
+    # to boot. If a worker process exist with
+    # this error code, the arbiter will terminate.
+    WORKER_BOOT_ERROR = 3
+
+    # A flag indicating if an application failed to be loaded
+    APP_LOAD_ERROR = 4
+
+    START_CTX = {}
+
+    LISTENERS = []
+    WORKERS = {}
+    PIPE = []
+
+    # I love dynamic languages
+    SIG_QUEUE = []
+    SIGNALS = [getattr(signal, "SIG%s" % x)
+               for x in "HUP QUIT INT TERM TTIN TTOU USR1 USR2 WINCH".split()]
+    SIG_NAMES = dict(
+        (getattr(signal, name), name[3:].lower()) for name in dir(signal)
+        if name[:3] == "SIG" and name[3] != "_"
+    )
+
+    def __init__(self, app):
+        os.environ["SERVER_SOFTWARE"] = SERVER_SOFTWARE
+
+        self._num_workers = None
+        self._last_logged_active_worker_count = None
+        self.log = None
+
+        self.setup(app)
+
+        self.pidfile = None
+        self.systemd = False
+        self.worker_age = 0
+        self.reexec_pid = 0
+        self.master_pid = 0
+        self.master_name = "Master"
+
+        cwd = util.getcwd()
+
+        args = sys.argv[:]
+        args.insert(0, sys.executable)
+
+        # init start context
+        self.START_CTX = {
+            "args": args,
+            "cwd": cwd,
+            0: sys.executable
+        }
+
+    def _get_num_workers(self):
+        return self._num_workers
+
+    def _set_num_workers(self, value):
+        old_value = self._num_workers
+        self._num_workers = value
+        self.cfg.nworkers_changed(self, value, old_value)
+    num_workers = property(_get_num_workers, _set_num_workers)
+
+    def setup(self, app):
+        self.app = app
+        self.cfg = app.cfg
+
+        if self.log is None:
+            self.log = self.cfg.logger_class(app.cfg)
+
+        # reopen files
+        if 'GUNICORN_FD' in os.environ:
+            self.log.reopen_files()
+
+        self.worker_class = self.cfg.worker_class
+        self.address = self.cfg.address
+        self.num_workers = self.cfg.workers
+        self.timeout = self.cfg.timeout
+        self.proc_name = self.cfg.proc_name
+
+        self.log.debug('Current configuration:\n{0}'.format(
+            '\n'.join(
+                '  {0}: {1}'.format(config, value.value)
+                for config, value
+                in sorted(self.cfg.settings.items(),
+                          key=lambda setting: setting[1]))))
+
+        # set enviroment' variables
+        if self.cfg.env:
+            for k, v in self.cfg.env.items():
+                os.environ[k] = v
+
+        if self.cfg.preload_app:
+            self.app.wsgi()
+
+    def start(self):
+        """\
+        Initialize the arbiter. Start listening and set pidfile if needed.
+        """
+        self.log.info("Starting gunicorn %s", __version__)
+
+        if 'GUNICORN_PID' in os.environ:
+            self.master_pid = int(os.environ.get('GUNICORN_PID'))
+            self.proc_name = self.proc_name + ".2"
+            self.master_name = "Master.2"
+
+        self.pid = os.getpid()
+        if self.cfg.pidfile is not None:
+            pidname = self.cfg.pidfile
+            if self.master_pid != 0:
+                pidname += ".2"
+            self.pidfile = Pidfile(pidname)
+            self.pidfile.create(self.pid)
+        self.cfg.on_starting(self)
+
+        self.init_signals()
+
+        if not self.LISTENERS:
+            fds = None
+            listen_fds = systemd.listen_fds()
+            if listen_fds:
+                self.systemd = True
+                fds = range(systemd.SD_LISTEN_FDS_START,
+                            systemd.SD_LISTEN_FDS_START + listen_fds)
+
+            elif self.master_pid:
+                fds = []
+                for fd in os.environ.pop('GUNICORN_FD').split(','):
+                    fds.append(int(fd))
+
+            self.LISTENERS = sock.create_sockets(self.cfg, self.log, fds)
+
+        listeners_str = ",".join([str(l) for l in self.LISTENERS])
+        self.log.debug("Arbiter booted")
+        self.log.info("Listening at: %s (%s)", listeners_str, self.pid)
+        self.log.info("Using worker: %s", self.cfg.worker_class_str)
+        systemd.sd_notify("READY=1\nSTATUS=Gunicorn arbiter booted", self.log)
+
+        # check worker class requirements
+        if hasattr(self.worker_class, "check_config"):
+            self.worker_class.check_config(self.cfg, self.log)
+
+        self.cfg.when_ready(self)
+
+    def init_signals(self):
+        """\
+        Initialize master signal handling. Most of the signals
+        are queued. Child signals only wake up the master.
+        """
+        # close old PIPE
+        for p in self.PIPE:
+            os.close(p)
+
+        # initialize the pipe
+        self.PIPE = pair = os.pipe()
+        for p in pair:
+            util.set_non_blocking(p)
+            util.close_on_exec(p)
+
+        self.log.close_on_exec()
+
+        # initialize all signals
+        for s in self.SIGNALS:
+            signal.signal(s, self.signal)
+        signal.signal(signal.SIGCHLD, self.handle_chld)
+
+    def signal(self, sig, frame):
+        if len(self.SIG_QUEUE) < 5:
+            self.SIG_QUEUE.append(sig)
+            self.wakeup()
+
+    def run(self):
+        "Main master loop."
+        self.start()
+        util._setproctitle("master [%s]" % self.proc_name)
+
+        try:
+            self.manage_workers()
+
+            while True:
+                self.maybe_promote_master()
+
+                sig = self.SIG_QUEUE.pop(0) if self.SIG_QUEUE else None
+                if sig is None:
+                    self.sleep()
+                    self.murder_workers()
+                    self.manage_workers()
+                    continue
+
+                if sig not in self.SIG_NAMES:
+                    self.log.info("Ignoring unknown signal: %s", sig)
+                    continue
+
+                signame = self.SIG_NAMES.get(sig)
+                handler = getattr(self, "handle_%s" % signame, None)
+                if not handler:
+                    self.log.error("Unhandled signal: %s", signame)
+                    continue
+                self.log.info("Handling signal: %s", signame)
+                handler()
+                self.wakeup()
+        except (StopIteration, KeyboardInterrupt):
+            self.halt()
+        except HaltServer as inst:
+            self.halt(reason=inst.reason, exit_status=inst.exit_status)
+        except SystemExit:
+            raise
+        except Exception:
+            self.log.info("Unhandled exception in main loop",
+                          exc_info=True)
+            self.stop(False)
+            if self.pidfile is not None:
+                self.pidfile.unlink()
+            sys.exit(-1)
+
+    def handle_chld(self, sig, frame):
+        "SIGCHLD handling"
+        self.reap_workers()
+        self.wakeup()
+
+    def handle_hup(self):
+        """\
+        HUP handling.
+        - Reload configuration
+        - Start the new worker processes with a new configuration
+        - Gracefully shutdown the old worker processes
+        """
+        self.log.info("Hang up: %s", self.master_name)
+        self.reload()
+
+    def handle_term(self):
+        "SIGTERM handling"
+        raise StopIteration
+
+    def handle_int(self):
+        "SIGINT handling"
+        self.stop(False)
+        raise StopIteration
+
+    def handle_quit(self):
+        "SIGQUIT handling"
+        self.stop(False)
+        raise StopIteration
+
+    def handle_ttin(self):
+        """\
+        SIGTTIN handling.
+        Increases the number of workers by one.
+        """
+        self.num_workers += 1
+        self.manage_workers()
+
+    def handle_ttou(self):
+        """\
+        SIGTTOU handling.
+        Decreases the number of workers by one.
+        """
+        if self.num_workers <= 1:
+            return
+        self.num_workers -= 1
+        self.manage_workers()
+
+    def handle_usr1(self):
+        """\
+        SIGUSR1 handling.
+        Kill all workers by sending them a SIGUSR1
+        """
+        self.log.reopen_files()
+        self.kill_workers(signal.SIGUSR1)
+
+    def handle_usr2(self):
+        """\
+        SIGUSR2 handling.
+        Creates a new arbiter/worker set as a fork of the current
+        arbiter without affecting old workers. Use this to do live
+        deployment with the ability to backout a change.
+        """
+        self.reexec()
+
+    def handle_winch(self):
+        """SIGWINCH handling"""
+        if self.cfg.daemon:
+            self.log.info("graceful stop of workers")
+            self.num_workers = 0
+            self.kill_workers(signal.SIGTERM)
+        else:
+            self.log.debug("SIGWINCH ignored. Not daemonized")
+
+    def maybe_promote_master(self):
+        if self.master_pid == 0:
+            return
+
+        if self.master_pid != os.getppid():
+            self.log.info("Master has been promoted.")
+            # reset master infos
+            self.master_name = "Master"
+            self.master_pid = 0
+            self.proc_name = self.cfg.proc_name
+            del os.environ['GUNICORN_PID']
+            # rename the pidfile
+            if self.pidfile is not None:
+                self.pidfile.rename(self.cfg.pidfile)
+            # reset proctitle
+            util._setproctitle("master [%s]" % self.proc_name)
+
+    def wakeup(self):
+        """\
+        Wake up the arbiter by writing to the PIPE
+        """
+        try:
+            os.write(self.PIPE[1], b'.')
+        except IOError as e:
+            if e.errno not in [errno.EAGAIN, errno.EINTR]:
+                raise
+
+    def halt(self, reason=None, exit_status=0):
+        """ halt arbiter """
+        self.stop()
+        self.log.info("Shutting down: %s", self.master_name)
+        if reason is not None:
+            self.log.info("Reason: %s", reason)
+        if self.pidfile is not None:
+            self.pidfile.unlink()
+        self.cfg.on_exit(self)
+        sys.exit(exit_status)
+
+    def sleep(self):
+        """\
+        Sleep until PIPE is readable or we timeout.
+        A readable PIPE means a signal occurred.
+        """
+        try:
+            ready = select.select([self.PIPE[0]], [], [], 1.0)
+            if not ready[0]:
+                return
+            while os.read(self.PIPE[0], 1):
+                pass
+        except (select.error, OSError) as e:
+            # TODO: select.error is a subclass of OSError since Python 3.3.
+            error_number = getattr(e, 'errno', e.args[0])
+            if error_number not in [errno.EAGAIN, errno.EINTR]:
+                raise
+        except KeyboardInterrupt:
+            sys.exit()
+
+    def stop(self, graceful=True):
+        """\
+        Stop workers
+
+        :attr graceful: boolean, If True (the default) workers will be
+        killed gracefully  (ie. trying to wait for the current connection)
+        """
+        unlink = (
+            self.reexec_pid == self.master_pid == 0
+            and not self.systemd
+            and not self.cfg.reuse_port
+        )
+        sock.close_sockets(self.LISTENERS, unlink)
+
+        self.LISTENERS = []
+        sig = signal.SIGTERM
+        if not graceful:
+            sig = signal.SIGQUIT
+        limit = time.time() + self.cfg.graceful_timeout
+        # instruct the workers to exit
+        self.kill_workers(sig)
+        # wait until the graceful timeout
+        while self.WORKERS and time.time() < limit:
+            time.sleep(0.1)
+
+        self.kill_workers(signal.SIGKILL)
+
+    def reexec(self):
+        """\
+        Relaunch the master and workers.
+        """
+        if self.reexec_pid != 0:
+            self.log.warning("USR2 signal ignored. Child exists.")
+            return
+
+        if self.master_pid != 0:
+            self.log.warning("USR2 signal ignored. Parent exists.")
+            return
+
+        master_pid = os.getpid()
+        self.reexec_pid = os.fork()
+        if self.reexec_pid != 0:
+            return
+
+        self.cfg.pre_exec(self)
+
+        environ = self.cfg.env_orig.copy()
+        environ['GUNICORN_PID'] = str(master_pid)
+
+        if self.systemd:
+            environ['LISTEN_PID'] = str(os.getpid())
+            environ['LISTEN_FDS'] = str(len(self.LISTENERS))
+        else:
+            environ['GUNICORN_FD'] = ','.join(
+                str(l.fileno()) for l in self.LISTENERS)
+
+        os.chdir(self.START_CTX['cwd'])
+
+        # exec the process using the original environment
+        os.execvpe(self.START_CTX[0], self.START_CTX['args'], environ)
+
+    def reload(self):
+        old_address = self.cfg.address
+
+        # reset old environment
+        for k in self.cfg.env:
+            if k in self.cfg.env_orig:
+                # reset the key to the value it had before
+                # we launched gunicorn
+                os.environ[k] = self.cfg.env_orig[k]
+            else:
+                # delete the value set by gunicorn
+                try:
+                    del os.environ[k]
+                except KeyError:
+                    pass
+
+        # reload conf
+        self.app.reload()
+        self.setup(self.app)
+
+        # reopen log files
+        self.log.reopen_files()
+
+        # do we need to change listener ?
+        if old_address != self.cfg.address:
+            # close all listeners
+            for l in self.LISTENERS:
+                l.close()
+            # init new listeners
+            self.LISTENERS = sock.create_sockets(self.cfg, self.log)
+            listeners_str = ",".join([str(l) for l in self.LISTENERS])
+            self.log.info("Listening at: %s", listeners_str)
+
+        # do some actions on reload
+        self.cfg.on_reload(self)
+
+        # unlink pidfile
+        if self.pidfile is not None:
+            self.pidfile.unlink()
+
+        # create new pidfile
+        if self.cfg.pidfile is not None:
+            self.pidfile = Pidfile(self.cfg.pidfile)
+            self.pidfile.create(self.pid)
+
+        # set new proc_name
+        util._setproctitle("master [%s]" % self.proc_name)
+
+        # spawn new workers
+        for _ in range(self.cfg.workers):
+            self.spawn_worker()
+
+        # manage workers
+        self.manage_workers()
+
+    def murder_workers(self):
+        """\
+        Kill unused/idle workers
+        """
+        if not self.timeout:
+            return
+        workers = list(self.WORKERS.items())
+        for (pid, worker) in workers:
+            try:
+                if time.time() - worker.tmp.last_update() <= self.timeout:
+                    continue
+            except (OSError, ValueError):
+                continue
+
+            if not worker.aborted:
+                self.log.critical("WORKER TIMEOUT (pid:%s)", pid)
+                worker.aborted = True
+                self.kill_worker(pid, signal.SIGABRT)
+            else:
+                self.kill_worker(pid, signal.SIGKILL)
+
+    def reap_workers(self):
+        """\
+        Reap workers to avoid zombie processes
+        """
+        try:
+            while True:
+                wpid, status = os.waitpid(-1, os.WNOHANG)
+                if not wpid:
+                    break
+                if self.reexec_pid == wpid:
+                    self.reexec_pid = 0
+                else:
+                    # A worker was terminated. If the termination reason was
+                    # that it could not boot, we'll shut it down to avoid
+                    # infinite start/stop cycles.
+                    exitcode = status >> 8
+                    if exitcode == self.WORKER_BOOT_ERROR:
+                        reason = "Worker failed to boot."
+                        raise HaltServer(reason, self.WORKER_BOOT_ERROR)
+                    if exitcode == self.APP_LOAD_ERROR:
+                        reason = "App failed to load."
+                        raise HaltServer(reason, self.APP_LOAD_ERROR)
+                    if os.WIFSIGNALED(status):
+                        self.log.warning(
+                            "Worker with pid %s was terminated due to signal %s",
+                            wpid,
+                            os.WTERMSIG(status)
+                        )
+
+                    worker = self.WORKERS.pop(wpid, None)
+                    if not worker:
+                        continue
+                    worker.tmp.close()
+                    self.cfg.child_exit(self, worker)
+        except OSError as e:
+            if e.errno != errno.ECHILD:
+                raise
+
+    def manage_workers(self):
+        """\
+        Maintain the number of workers by spawning or killing
+        as required.
+        """
+        if len(self.WORKERS) < self.num_workers:
+            self.spawn_workers()
+
+        workers = self.WORKERS.items()
+        workers = sorted(workers, key=lambda w: w[1].age)
+        while len(workers) > self.num_workers:
+            (pid, _) = workers.pop(0)
+            self.kill_worker(pid, signal.SIGTERM)
+
+        active_worker_count = len(workers)
+        if self._last_logged_active_worker_count != active_worker_count:
+            self._last_logged_active_worker_count = active_worker_count
+            self.log.debug("{0} workers".format(active_worker_count),
+                           extra={"metric": "gunicorn.workers",
+                                  "value": active_worker_count,
+                                  "mtype": "gauge"})
+
+    def spawn_worker(self):
+        self.worker_age += 1
+        worker = self.worker_class(self.worker_age, self.pid, self.LISTENERS,
+                                   self.app, self.timeout / 2.0,
+                                   self.cfg, self.log)
+        self.cfg.pre_fork(self, worker)
+        pid = os.fork()
+        if pid != 0:
+            worker.pid = pid
+            self.WORKERS[pid] = worker
+            return pid
+
+        # Do not inherit the temporary files of other workers
+        for sibling in self.WORKERS.values():
+            sibling.tmp.close()
+
+        # Process Child
+        worker.pid = os.getpid()
+        try:
+            util._setproctitle("worker [%s]" % self.proc_name)
+            self.log.info("Booting worker with pid: %s", worker.pid)
+            self.cfg.post_fork(self, worker)
+            worker.init_process()
+            sys.exit(0)
+        except SystemExit:
+            raise
+        except AppImportError as e:
+            self.log.debug("Exception while loading the application",
+                           exc_info=True)
+            print("%s" % e, file=sys.stderr)
+            sys.stderr.flush()
+            sys.exit(self.APP_LOAD_ERROR)
+        except Exception:
+            self.log.exception("Exception in worker process")
+            if not worker.booted:
+                sys.exit(self.WORKER_BOOT_ERROR)
+            sys.exit(-1)
+        finally:
+            self.log.info("Worker exiting (pid: %s)", worker.pid)
+            try:
+                worker.tmp.close()
+                self.cfg.worker_exit(self, worker)
+            except Exception:
+                self.log.warning("Exception during worker exit:\n%s",
+                                 traceback.format_exc())
+
+    def spawn_workers(self):
+        """\
+        Spawn new workers as needed.
+
+        This is where a worker process leaves the main loop
+        of the master process.
+        """
+
+        for _ in range(self.num_workers - len(self.WORKERS)):
+            self.spawn_worker()
+            time.sleep(0.1 * random.random())
+
+    def kill_workers(self, sig):
+        """\
+        Kill all workers with the signal `sig`
+        :attr sig: `signal.SIG*` value
+        """
+        worker_pids = list(self.WORKERS.keys())
+        for pid in worker_pids:
+            self.kill_worker(pid, sig)
+
+    def kill_worker(self, pid, sig):
+        """\
+        Kill a worker
+
+        :attr pid: int, worker pid
+        :attr sig: `signal.SIG*` value
+         """
+        try:
+            os.kill(pid, sig)
+        except OSError as e:
+            if e.errno == errno.ESRCH:
+                try:
+                    worker = self.WORKERS.pop(pid)
+                    worker.tmp.close()
+                    self.cfg.worker_exit(self, worker)
+                    return
+                except (KeyError, OSError):
+                    return
+            raise
Index: venv/Lib/site-packages/django_heroku/core.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/django_heroku/core.py b/venv/Lib/site-packages/django_heroku/core.py
new file mode 100644
--- /dev/null	(date 1630065394629)
+++ b/venv/Lib/site-packages/django_heroku/core.py	(date 1630065394629)
@@ -0,0 +1,150 @@
+import logging
+import os
+
+import dj_database_url
+from django.test.runner import DiscoverRunner
+
+MAX_CONN_AGE = 600
+
+logger = logging.getLogger(__name__)
+
+
+class HerokuDiscoverRunner(DiscoverRunner):
+    """Test Runner for Heroku CI, which provides a database for you.
+    This requires you to set the TEST database (done for you by settings().)"""
+
+    def setup_databases(self, **kwargs):
+        if not os.environ.get('CI'):
+            raise ValueError(
+                "The CI env variable must be set to enable this functionality.  WARNING:  "
+                "This test runner will wipe all tables in the database it targets!")
+        self.keepdb = True
+        return super(HerokuDiscoverRunner, self).setup_databases(**kwargs)
+
+    def _wipe_tables(self, connection):
+        with connection.cursor() as cursor:
+            cursor.execute(
+                """
+                    DROP SCHEMA public CASCADE;
+                    CREATE SCHEMA public;
+                    GRANT ALL ON SCHEMA public TO postgres;
+                    GRANT ALL ON SCHEMA public TO public;
+                    COMMENT ON SCHEMA public IS 'standard public schema';
+                """
+            )
+        pass
+
+    def teardown_databases(self, old_config, **kwargs):
+        self.keepdb = True
+        for connection, old_name, destroy in old_config:
+            if destroy:
+                self._wipe_tables(connection)
+        super(HerokuDiscoverRunner, self).teardown_databases(old_config, **kwargs)
+
+
+def settings(config, *, db_colors=False, databases=True, test_runner=True, staticfiles=True, allowed_hosts=True, logging=True, secret_key=True):
+
+    # Database configuration.
+    # TODO: support other database (e.g. TEAL, AMBER, etc, automatically.)
+    if databases:
+        # Integrity check.
+        if 'DATABASES' not in config:
+            config['DATABASES'] = {'default': None}
+
+        if db_colors:
+            # Support all Heroku databases.
+            # TODO: This appears to break TestRunner.
+            for (env, url) in os.environ.items():
+                if env.startswith('HEROKU_POSTGRESQL'):
+                    db_color = env[len('HEROKU_POSTGRESQL_'):].split('_')[0]
+
+                    logger.info('Adding ${} to DATABASES Django setting ({}).'.format(env, db_color))
+
+                    config['DATABASES'][db_color] = dj_database_url.parse(url, conn_max_age=MAX_CONN_AGE, ssl_require=True)
+
+        if 'DATABASE_URL' in os.environ:
+            logger.info('Adding $DATABASE_URL to default DATABASE Django setting.')
+
+            # Configure Django for DATABASE_URL environment variable.
+            config['DATABASES']['default'] = dj_database_url.config(conn_max_age=MAX_CONN_AGE, ssl_require=True)
+
+            logger.info('Adding $DATABASE_URL to TEST default DATABASE Django setting.')
+
+            # Enable test database if found in CI environment.
+            if 'CI' in os.environ:
+                config['DATABASES']['default']['TEST'] = config['DATABASES']['default']
+
+        else:
+            logger.info('$DATABASE_URL not found, falling back to previous settings!')
+
+    if test_runner:
+        # Enable test runner if found in CI environment.
+        if 'CI' in os.environ:
+            config['TEST_RUNNER'] = 'django_heroku.HerokuDiscoverRunner'
+
+    # Staticfiles configuration.
+    if staticfiles:
+        logger.info('Applying Heroku Staticfiles configuration to Django settings.')
+
+        config['STATIC_ROOT'] = os.path.join(config['BASE_DIR'], 'staticfiles')
+        config['STATIC_URL'] = '/static/'
+
+        # Ensure STATIC_ROOT exists.
+        os.makedirs(config['STATIC_ROOT'], exist_ok=True)
+
+        # Insert Whitenoise Middleware.
+        try:
+            config['MIDDLEWARE_CLASSES'] = tuple(['whitenoise.middleware.WhiteNoiseMiddleware'] + list(config['MIDDLEWARE_CLASSES']))
+        except KeyError:
+            config['MIDDLEWARE'] = tuple(['whitenoise.middleware.WhiteNoiseMiddleware'] + list(config['MIDDLEWARE']))
+
+        # Enable GZip.
+        config['STATICFILES_STORAGE'] = 'whitenoise.storage.CompressedManifestStaticFilesStorage'
+
+    if allowed_hosts:
+        logger.info('Applying Heroku ALLOWED_HOSTS configuration to Django settings.')
+        config['ALLOWED_HOSTS'] = ['*']
+
+    if logging:
+        logger.info('Applying Heroku logging configuration to Django settings.')
+
+        config['LOGGING'] = {
+            'version': 1,
+            'disable_existing_loggers': False,
+            'formatters': {
+                'verbose': {
+                    'format': ('%(asctime)s [%(process)d] [%(levelname)s] ' +
+                               'pathname=%(pathname)s lineno=%(lineno)s ' +
+                               'funcname=%(funcName)s %(message)s'),
+                    'datefmt': '%Y-%m-%d %H:%M:%S'
+                },
+                'simple': {
+                    'format': '%(levelname)s %(message)s'
+                }
+            },
+            'handlers': {
+                'null': {
+                    'level': 'DEBUG',
+                    'class': 'logging.NullHandler',
+                },
+                'console': {
+                    'level': 'DEBUG',
+                    'class': 'logging.StreamHandler',
+                    'formatter': 'verbose'
+                }
+            },
+            'loggers': {
+                'testlogger': {
+                    'handlers': ['console'],
+                    'level': 'INFO',
+                }
+            }
+        }
+
+    # SECRET_KEY configuration.
+    if secret_key:
+        if 'SECRET_KEY' in os.environ:
+            logger.info('Adding $SECRET_KEY to SECRET_KEY Django setting.')
+            # Set the Django setting from the environment variable.
+            config['SECRET_KEY'] = os.environ['SECRET_KEY']
+
Index: venv/Lib/site-packages/dj_database_url.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/venv/Lib/site-packages/dj_database_url.py b/venv/Lib/site-packages/dj_database_url.py
new file mode 100644
--- /dev/null	(date 1630065394575)
+++ b/venv/Lib/site-packages/dj_database_url.py	(date 1630065394575)
@@ -0,0 +1,144 @@
+# -*- coding: utf-8 -*-
+
+import os
+
+try:
+    import urlparse
+except ImportError:
+    import urllib.parse as urlparse
+
+
+# Register database schemes in URLs.
+urlparse.uses_netloc.append('postgres')
+urlparse.uses_netloc.append('postgresql')
+urlparse.uses_netloc.append('pgsql')
+urlparse.uses_netloc.append('postgis')
+urlparse.uses_netloc.append('mysql')
+urlparse.uses_netloc.append('mysql2')
+urlparse.uses_netloc.append('mysqlgis')
+urlparse.uses_netloc.append('mysql-connector')
+urlparse.uses_netloc.append('mssql')
+urlparse.uses_netloc.append('spatialite')
+urlparse.uses_netloc.append('sqlite')
+urlparse.uses_netloc.append('oracle')
+urlparse.uses_netloc.append('oraclegis')
+urlparse.uses_netloc.append('redshift')
+
+DEFAULT_ENV = 'DATABASE_URL'
+
+SCHEMES = {
+    'postgres': 'django.db.backends.postgresql_psycopg2',
+    'postgresql': 'django.db.backends.postgresql_psycopg2',
+    'pgsql': 'django.db.backends.postgresql_psycopg2',
+    'postgis': 'django.contrib.gis.db.backends.postgis',
+    'mysql': 'django.db.backends.mysql',
+    'mysql2': 'django.db.backends.mysql',
+    'mysqlgis': 'django.contrib.gis.db.backends.mysql',
+    'mysql-connector': 'mysql.connector.django',
+    'mssql': 'sql_server.pyodbc',
+    'spatialite': 'django.contrib.gis.db.backends.spatialite',
+    'sqlite': 'django.db.backends.sqlite3',
+    'oracle': 'django.db.backends.oracle',
+    'oraclegis': 'django.contrib.gis.db.backends.oracle',
+    'redshift': 'django_redshift_backend',
+}
+
+
+def config(env=DEFAULT_ENV, default=None, engine=None, conn_max_age=0, ssl_require=False):
+    """Returns configured DATABASE dictionary from DATABASE_URL."""
+
+    config = {}
+
+    s = os.environ.get(env, default)
+
+    if s:
+        config = parse(s, engine, conn_max_age, ssl_require)
+
+    return config
+
+
+def parse(url, engine=None, conn_max_age=0, ssl_require=False):
+    """Parses a database URL."""
+
+    if url == 'sqlite://:memory:':
+        # this is a special case, because if we pass this URL into
+        # urlparse, urlparse will choke trying to interpret "memory"
+        # as a port number
+        return {
+            'ENGINE': SCHEMES['sqlite'],
+            'NAME': ':memory:'
+        }
+        # note: no other settings are required for sqlite
+
+    # otherwise parse the url as normal
+    config = {}
+
+    url = urlparse.urlparse(url)
+
+    # Split query strings from path.
+    path = url.path[1:]
+    if '?' in path and not url.query:
+        path, query = path.split('?', 2)
+    else:
+        path, query = path, url.query
+    query = urlparse.parse_qs(query)
+
+    # If we are using sqlite and we have no path, then assume we
+    # want an in-memory database (this is the behaviour of sqlalchemy)
+    if url.scheme == 'sqlite' and path == '':
+        path = ':memory:'
+
+    # Handle postgres percent-encoded paths.
+    hostname = url.hostname or ''
+    if '%2f' in hostname.lower():
+        # Switch to url.netloc to avoid lower cased paths
+        hostname = url.netloc
+        if "@" in hostname:
+            hostname = hostname.rsplit("@", 1)[1]
+        if ":" in hostname:
+            hostname = hostname.split(":", 1)[0]
+        hostname = hostname.replace('%2f', '/').replace('%2F', '/')
+
+    # Lookup specified engine.
+    engine = SCHEMES[url.scheme] if engine is None else engine
+
+    port = (str(url.port) if url.port and engine == SCHEMES['oracle']
+            else url.port)
+
+    # Update with environment configuration.
+    config.update({
+        'NAME': urlparse.unquote(path or ''),
+        'USER': urlparse.unquote(url.username or ''),
+        'PASSWORD': urlparse.unquote(url.password or ''),
+        'HOST': hostname,
+        'PORT': port or '',
+        'CONN_MAX_AGE': conn_max_age,
+    })
+
+    # Pass the query string into OPTIONS.
+    options = {}
+    for key, values in query.items():
+        if url.scheme == 'mysql' and key == 'ssl-ca':
+            options['ssl'] = {'ca': values[-1]}
+            continue
+
+        options[key] = values[-1]
+
+    if ssl_require:
+        options['sslmode'] = 'require'
+
+    # Support for Postgres Schema URLs
+    if 'currentSchema' in options and engine in (
+        'django.contrib.gis.db.backends.postgis',
+        'django.db.backends.postgresql_psycopg2',
+        'django_redshift_backend',
+    ):
+        options['options'] = '-c search_path={0}'.format(options.pop('currentSchema'))
+
+    if options:
+        config['OPTIONS'] = options
+
+    if engine:
+        config['ENGINE'] = engine
+
+    return config
diff --git a/venv/Lib/site-packages/whitenoise/runserver_nostatic/management/commands/__init__.py b/venv/Lib/site-packages/whitenoise/runserver_nostatic/management/commands/__init__.py
new file mode 100644
diff --git a/venv/Lib/site-packages/whitenoise/runserver_nostatic/management/__init__.py b/venv/Lib/site-packages/whitenoise/runserver_nostatic/management/__init__.py
new file mode 100644
diff --git a/venv/Lib/site-packages/whitenoise/runserver_nostatic/__init__.py b/venv/Lib/site-packages/whitenoise/runserver_nostatic/__init__.py
new file mode 100644
diff --git a/venv/Lib/site-packages/gunicorn/instrument/__init__.py b/venv/Lib/site-packages/gunicorn/instrument/__init__.py
new file mode 100644
